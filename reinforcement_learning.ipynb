{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## 1. Introduction to Reinforcement Learning (RL)\n",
    "\n",
    "Introduction to RL\n",
    "\n",
    "In this section, we introduce Reinforcement Learning (RL) by defining it as a method where an agent learns to map situations (states) to actions in order to maximize numerical rewards. Unlike supervised learning (with labeled examples) or unsupervised learning (finding structure in data), RL is centered around trial-and-error learning with delayed rewards.\n",
    "\n",
    "Demo Context (Business Application):\n",
    "\n",
    "Imagine an online advertising system that learns over time which ads generate the highest click-through rate. Even without knowing the best ad in advance, the system can improve its performance by trying different options.\n",
    "\n",
    "Before the Demo Question:\n",
    "\n",
    "- What does “trial-and-error” mean in the context of learning from feedback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Simulate a simple environment with two actions.\n",
    "def simple_environment(action):\n",
    "    # Action 0 gives a reward of 1 with 50% probability.\n",
    "    # Action 1 gives a reward of 1 with 80% probability.\n",
    "    if action == 0:\n",
    "        return 1 if random.random() < 0.5 else 0\n",
    "    elif action == 1:\n",
    "        return 1 if random.random() < 0.8 else 0\n",
    "\n",
    "# Simulate an agent that randomly explores the actions.\n",
    "def run_agent(steps=1000):\n",
    "    actions = [0, 1]\n",
    "    action_rewards = {0: 0, 1: 0}\n",
    "    action_counts = {0: 0, 1: 0}\n",
    "    for _ in range(steps):\n",
    "        action = random.choice(actions)  # Random (exploratory) action selection\n",
    "        reward = simple_environment(action)\n",
    "        action_rewards[action] += reward\n",
    "        action_counts[action] += 1\n",
    "    avg_rewards = {a: action_rewards[a] / action_counts[a] if action_counts[a] != 0 else 0 for a in actions}\n",
    "    return avg_rewards\n",
    "\n",
    "avg_rewards = run_agent(1000)\n",
    "print(\"Average rewards after 1000 steps:\", avg_rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection:\n",
    "\n",
    "The simulation shows that even with random actions, the agent gathers enough data to notice that one action yields a higher average reward.\n",
    "\n",
    "Discussion Questions:\n",
    "\n",
    "- What is the main idea behind trial-and-error learning?\n",
    "- How does the agent begin to identify which action is better?\n",
    "- How could you apply this concept to a business scenario like advertisement selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reinforcement Learning Problem Components\n",
    "\n",
    "**RL Problem Components**  \n",
    "\n",
    "In RL, we decompose the problem into several components:\n",
    "- **Environment:** Where the agent interacts.\n",
    "- **Reward Function:** \\( $(s, a)$ \\) that provides feedback.\n",
    "- **Value Function:** \\( $V(s)$ \\) estimates long-term rewards.\n",
    "- **Policy:** \\( $\\pi(s)$ \\) that guides the actions.\n",
    "- **Optimal Policy:** \\( $\\pi^*$ \\) that maximizes cumulative rewards.\n",
    "\n",
    "**Business Context:**  \n",
    "Consider modeling a customer journey where the “state” is the customer’s position in a sales funnel, and actions are marketing interventions.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- What role does the reward function play in guiding an agent’s behavior?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple MDP-like environment: a linear chain from state 0 to state 4.\n",
    "class SimpleMDP:\n",
    "    def __init__(self):\n",
    "        self.states = list(range(5))\n",
    "        self.current_state = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Action: 1 means move right; -1 means move left.\n",
    "        next_state = self.current_state + action\n",
    "        next_state = max(0, min(4, next_state))\n",
    "        # Reward is given only when reaching state 4.\n",
    "        reward = 1 if next_state == 4 else 0\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward\n",
    "\n",
    "env = SimpleMDP()\n",
    "state = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "next_state, reward = env.step(1)\n",
    "print(\"After taking action 1, next state:\", next_state, \"Reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "In this MDP, the environment’s structure, the transition dynamics, and the reward function are clearly defined. Notice how the reward encourages reaching a terminal state.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- What does the reward function encourage the agent to do?  \n",
    "- How is the environment structured in this example?  \n",
    "- How might such an environment be used to model a business process, such as a customer moving through a sales funnel?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inverse Reinforcement Learning (IRL)\n",
    "\n",
    "**Inverse Reinforcement Learning (IRL)**  \n",
    "\n",
    "IRL focuses on inferring the reward function by observing the behavior of an expert. Instead of defining the reward function explicitly, we learn what the expert values by examining their actions.\n",
    "\n",
    "**Business Context:**  \n",
    "For example, by analyzing successful sales strategies employed by top salespeople, a company can infer what factors contribute most to closing deals.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- How might observing an expert help us uncover the underlying rewards in a decision-making process?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate expert trajectories in our simple MDP using an expert policy.\n",
    "def generate_expert_trajectory(env, expert_policy, steps=10):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    for _ in range(steps):\n",
    "        action = expert_policy(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        trajectory.append((state, action, next_state, reward))\n",
    "        state = next_state\n",
    "    return trajectory\n",
    "\n",
    "# Define a simple expert policy: always move right (action = 1)\n",
    "def expert_policy(state):\n",
    "    return 1\n",
    "\n",
    "env = SimpleMDP()\n",
    "trajectory = generate_expert_trajectory(env, expert_policy, steps=10)\n",
    "print(\"Expert Trajectory:\")\n",
    "for t in trajectory:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "The expert trajectory shows a sequence of state transitions where the expert consistently takes the action that eventually leads to the goal. IRL would use such data to deduce the reward function.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- How does the expert policy compare to a random policy?  \n",
    "- What can you infer about the reward structure from the expert trajectories?  \n",
    "- How could you use expert behavior data to improve a business process?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Markov Decision Processes (MDPs)\n",
    "\n",
    "**Markov Decision Processes (MDPs)**  \n",
    "\n",
    "An MDP is a mathematical framework for modeling decision-making situations. It is defined by:\n",
    "- **States** $( S )$\n",
    "- **Actions** $( A )$\n",
    "- **Transition Dynamics**\n",
    "- **Rewards**\n",
    "\n",
    "A key property is the **Markov assumption**, where the next state depends only on the current state and action.\n",
    "\n",
    "**Business Context:**  \n",
    "MDPs can model customer behavior where the next state (e.g., customer status) depends only on the current state and the latest interaction.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- What is the Markov property and why might it be useful in modeling decision-making processes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomMDP:\n",
    "    def __init__(self):\n",
    "        self.states = ['A', 'B', 'C']\n",
    "        self.current_state = 'A'\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = 'A'\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # For demonstration, action does not affect transition; next state is random.\n",
    "        next_state = random.choice(self.states)\n",
    "        # Reward is given only if the next state is 'C'.\n",
    "        reward = 1 if next_state == 'C' else 0\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward\n",
    "\n",
    "mdp = RandomMDP()\n",
    "state = mdp.reset()\n",
    "print(\"Initial state:\", state)\n",
    "next_state, reward = mdp.step(action=None)\n",
    "print(\"Next state:\", next_state, \"Reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "This simple MDP illustrates that the next state depends solely on the current state and (if applicable) the action taken, thereby satisfying the Markov property.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- How does the random transition in this demo illustrate the Markov assumption?  \n",
    "- Why is the Markov property important when modeling real-world processes?  \n",
    "- Can you think of a business scenario where only the current state influences the next decision?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Value Function and Optimality\n",
    "\n",
    "**Value Function and Optimality**  \n",
    "\n",
    "The value function $ V(s) $ estimates the expected cumulative reward from a given state under a particular policy. Evaluating $ V(s) $ helps in understanding how “good” it is to be in a state.\n",
    "\n",
    "**Business Context:**  \n",
    "In strategic planning, a value function can help assess the long-term benefits of being in a particular market segment or customer segment.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- How does the discount factor $ \\gamma $ influence the value assigned to future rewards?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple policy evaluation for a 3-state MDP.\n",
    "states = [0, 1, 2]\n",
    "# Transition dynamics for a fixed policy:\n",
    "# - From state 0: moves to state 1 with probability 1.0.\n",
    "# - From state 1: 50% chance to go to state 0 (reward=0) and 50% chance to go to state 2 (reward=1).\n",
    "# - State 2 is terminal.\n",
    "P = {\n",
    "    0: [(1, 1.0, 0)],         # (next_state, probability, reward)\n",
    "    1: [(0, 0.5, 0), (2, 0.5, 1)],\n",
    "    2: []                     # Terminal state\n",
    "}\n",
    "\n",
    "gamma = 0.9  # Discount factor\n",
    "V = {s: 0 for s in states}  # Initialize value function\n",
    "\n",
    "def policy_evaluation(P, V, gamma, theta=0.0001):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s not in P or not P[s]:\n",
    "                continue\n",
    "            v = V[s]\n",
    "            V[s] = sum(prob * (reward + gamma * V[next_state]) for next_state, prob, reward in P[s])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "V = policy_evaluation(P, V, gamma)\n",
    "print(\"Computed Value Function:\", V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "The iterative policy evaluation computes the value function $ V(s) $ for each state under the current policy, highlighting the importance of long-term planning.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- In what way does the discount factor $ \\gamma $ affect the computed value function?  \n",
    "- Why is policy evaluation important when determining an optimal strategy?  \n",
    "- How might a value function be used to assess long-term investments in a business context?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Q-Function and Q-Learning\n",
    "\n",
    "**Q-Function and Q-Learning**  \n",
    "\n",
    "The Q-function $ Q(s, a) $ represents the expected cumulative reward of taking an action $ a $ in state $ s $ and thereafter following the optimal policy. Q-learning is an iterative algorithm that updates these values based on experiences.\n",
    "\n",
    "**Business Context:**  \n",
    "Q-learning can be applied to optimize dynamic pricing strategies or supply chain decisions by learning which actions yield the best long-term profit.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- What is the purpose of maintaining a Q-table in Q-learning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple Q-learning environment similar to SimpleMDP.\n",
    "class SimpleQLearningEnv:\n",
    "    def __init__(self):\n",
    "        self.states = list(range(5))\n",
    "        self.current_state = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_state = self.current_state + action\n",
    "        next_state = max(0, min(4, next_state))\n",
    "        reward = 1 if next_state == 4 else 0\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward\n",
    "\n",
    "env = SimpleQLearningEnv()\n",
    "\n",
    "# Q-learning parameters.\n",
    "num_states = 5\n",
    "actions = [-1, 1]  # Represent left and right moves.\n",
    "Q_table = np.zeros((num_states, len(actions)))\n",
    "alpha = 0.1      # Learning rate.\n",
    "gamma = 0.9      # Discount factor.\n",
    "epsilon = 0.2    # Exploration probability.\n",
    "episodes = 1000\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection.\n",
    "        if np.random.rand() < epsilon:\n",
    "            action_index = np.random.choice(len(actions))\n",
    "        else:\n",
    "            action_index = np.argmax(Q_table[state])\n",
    "        action = actions[action_index]\n",
    "        next_state, reward = env.step(action)\n",
    "        # Q-learning update.\n",
    "        best_next_action = np.argmax(Q_table[next_state])\n",
    "        Q_table[state, action_index] += alpha * (reward + gamma * Q_table[next_state, best_next_action] - Q_table[state, action_index])\n",
    "        state = next_state\n",
    "        if state == 4:\n",
    "            done = True\n",
    "\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "The Q-learning algorithm iteratively updates the Q-table, using both immediate rewards and estimates of future rewards to improve decision making.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- What is the role of the exploration rate $ \\epsilon $ in this algorithm?  \n",
    "- How does Q-learning update the Q-values based on new experiences?  \n",
    "- Can you think of a business scenario where such iterative learning from feedback would be beneficial?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploration vs. Exploitation in RL\n",
    "\n",
    "**Exploration vs. Exploitation**  \n",
    "\n",
    "A major challenge in RL is balancing:\n",
    "- **Exploration:** Trying new actions to discover their rewards.\n",
    "- **Exploitation:** Using known information to maximize reward.\n",
    "\n",
    "In this demo, we simulate an epsilon-greedy strategy in a multi-armed bandit scenario.\n",
    "\n",
    "**Business Context:**  \n",
    "This is analogous to testing multiple marketing campaigns (exploration) while investing more in the best-performing ones (exploitation).\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- What might be the risks of exploring too much or exploiting too soon?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate a multi-armed bandit with 3 arms, each with a different reward probability.\n",
    "true_probabilities = [0.2, 0.5, 0.7]\n",
    "num_arms = len(true_probabilities)\n",
    "num_steps = 1000\n",
    "epsilon = 0.1  # Exploration probability.\n",
    "\n",
    "# Initialize Q-value estimates and selection counts.\n",
    "Q_estimates = np.zeros(num_arms)\n",
    "counts = np.zeros(num_arms)\n",
    "\n",
    "def get_reward(arm):\n",
    "    return 1 if np.random.rand() < true_probabilities[arm] else 0\n",
    "\n",
    "rewards = []\n",
    "for step in range(num_steps):\n",
    "    if np.random.rand() < epsilon:\n",
    "        arm = np.random.choice(num_arms)\n",
    "    else:\n",
    "        arm = np.argmax(Q_estimates)\n",
    "    reward = get_reward(arm)\n",
    "    counts[arm] += 1\n",
    "    # Update estimate using incremental average.\n",
    "    Q_estimates[arm] += (reward - Q_estimates[arm]) / counts[arm]\n",
    "    rewards.append(reward)\n",
    "\n",
    "print(\"Estimated Q-values for each arm:\", Q_estimates)\n",
    "print(\"Counts for each arm:\", counts)\n",
    "print(\"Average reward over all steps:\", np.mean(rewards))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "This multi-armed bandit simulation illustrates the balance between exploration (trying different arms) and exploitation (choosing the best arm based on current estimates).\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- How might setting $ \\epsilon $ too high or too low affect the learning process?  \n",
    "- Why is balancing exploration and exploitation crucial in sequential decision-making?  \n",
    "- How could this concept be applied to optimize marketing strategies, such as A/B testing of advertisements?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Neural Networks as Function Approximators in RL\n",
    "\n",
    "**Neural Networks as Function Approximators**  \n",
    "\n",
    "In complex or high-dimensional environments, it is impractical to store Q-values in a table. Instead, neural networks can approximate the Q-function (or policy) from raw inputs.\n",
    "\n",
    "**Business Context:**  \n",
    "For example, a financial forecasting model might use deep neural networks to evaluate a wide range of market conditions and recommend investment strategies.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- Why might a neural network be a better choice than a Q-table when dealing with complex environments?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network to approximate Q-values.\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# For demonstration, assume the state is represented by a single scalar.\n",
    "input_dim = 1\n",
    "output_dim = 2  # Two possible actions.\n",
    "q_net = QNetwork(input_dim, output_dim)\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Simulated training data: states and corresponding target Q-values.\n",
    "states = torch.tensor([[0.0], [1.0], [2.0], [3.0]], dtype=torch.float32)\n",
    "target_Q = torch.tensor([[0.5, 0.7],\n",
    "                         [0.6, 0.8],\n",
    "                         [0.7, 0.9],\n",
    "                         [0.8, 1.0]], dtype=torch.float32)\n",
    "\n",
    "# Train the network over multiple epochs.\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = q_net(states)\n",
    "    loss = loss_fn(outputs, target_Q)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Trained Q-network outputs:\")\n",
    "print(q_net(states).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "The neural network approximates the Q-values for each state, making it possible to work with high-dimensional or continuous state spaces. This is critical in many business applications where simple tabular methods fall short.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- What advantages does a neural network offer over a table-based approach?  \n",
    "- How does the network learn to approximate the Q-values?  \n",
    "- Can you think of a business scenario where function approximation is crucial for decision-making?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Marketing Campaign Optimization using Reinforcement Learning\n",
    "\n",
    "In this assignment, you will use a simple reinforcement learning algorithm (the epsilon-greedy multi-armed bandit) to optimize the selection of marketing campaigns. Imagine you run three different online marketing campaigns with the following conversion rates (i.e., the probability that a user clicks on the ad):\n",
    "\n",
    "- **Campaign 1:** 20%\n",
    "- **Campaign 2:** 50%\n",
    "- **Campaign 3:** 70%\n",
    "\n",
    "Your goal is to write a simulation that learns, over many trials, which campaign performs best. You will implement an epsilon-greedy algorithm that balances exploring all campaigns with exploiting the one that seems best based on your estimates.\n",
    "\n",
    "### What You'll Do:\n",
    "1. **Implement the simulation:** Complete the provided code to update the estimated conversion rates using the incremental update rule.\n",
    "2. **Experiment with different values of epsilon:** Observe how changing the exploration rate (epsilon) affects performance.\n",
    "3. **Meet the performance threshold:** Your solution must achieve an average reward (conversion rate) above a specified threshold (binary pass/fail metric).\n",
    "\n",
    "*Estimated Completion Time: 1–2 hours*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "A **multi-armed bandit** problem is a classic RL scenario. Each “arm” of the bandit represents a marketing campaign. When you choose an arm, you get a reward (a click, represented as 1) or no reward (0). The goal is to maximize your total reward (i.e., achieve the highest overall conversion rate).\n",
    "\n",
    "In an **epsilon-greedy** strategy, with probability *epsilon* you choose a random campaign (exploration), and with probability *(1 - epsilon)* you choose the campaign with the highest estimated conversion rate (exploitation).\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Implement the Incremental Update Rule:**\n",
    "\n",
    "   In the code cell below, complete the function `simulate_bandit(epsilon, steps)` by filling in the incremental update rule for the estimated conversion rate. The correct update rule is:\n",
    "   \n",
    "   \\[\n",
    "   $Q_{\\text{new}} = Q_{\\text{old}} + \\frac{(\\text{reward} - Q_{\\text{old}})}{\\text{count}}$\n",
    "   \\]\n",
    "   \n",
    "   (In the code, you will see a comment `# YOUR CODE HERE` where you should add the update.)\n",
    "\n",
    "2. **Experiment with Epsilon:**\n",
    "\n",
    "   Run the simulation with different values of epsilon (e.g., 0.0, 0.1, 0.5) and note the differences in performance. Answer the following in a Markdown cell:\n",
    "   \n",
    "   - How does increasing epsilon affect the balance between exploration and exploitation?\n",
    "   - What value of epsilon seems to work best in this scenario, and why might that be the case?\n",
    "\n",
    "3. **Pass/Fail Metric:**\n",
    "\n",
    "   The final part of this assignment is automated: your implementation must achieve an average reward of at least **0.65** when running the simulation with `epsilon = 0.1` and 1000 steps. If your simulation achieves this, you pass the assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_bandit(epsilon, steps=1000):\n",
    "    # For reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # True conversion probabilities for each campaign\n",
    "    true_probs = [0.2, 0.5, 0.7]\n",
    "    num_arms = len(true_probs)\n",
    "    \n",
    "    # Initialize estimated conversion rates (Q-values) and counts for each campaign\n",
    "    Q_estimates = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Exploration: choose a random campaign\n",
    "            chosen_arm = np.random.choice(num_arms)\n",
    "        else:\n",
    "            # Exploitation: choose the campaign with the highest estimated conversion rate\n",
    "            chosen_arm = np.argmax(Q_estimates)\n",
    "        \n",
    "        # Simulate the reward (click): 1 with probability true_probs[chosen_arm], else 0\n",
    "        reward = 1 if np.random.rand() < true_probs[chosen_arm] else 0\n",
    "        \n",
    "        # Update the count for the chosen campaign\n",
    "        counts[chosen_arm] += 1\n",
    "        \n",
    "        # Update the Q-value (estimated conversion rate) for the chosen campaign\n",
    "        # TODO: Implement the incremental update rule:\n",
    "        # Q_estimates[chosen_arm] = Q_estimates[chosen_arm] + (reward - Q_estimates[chosen_arm]) / counts[chosen_arm]\n",
    "        #\n",
    "        # Remove the comment from the line below and complete it:\n",
    "        Q_estimates[chosen_arm] = Q_estimates[chosen_arm] + (reward - Q_estimates[chosen_arm]) / counts[chosen_arm]\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    average_reward = np.mean(rewards)\n",
    "    return average_reward, Q_estimates, counts\n",
    "\n",
    "# Run an example simulation with epsilon = 0.1\n",
    "avg_reward, Q_estimates, counts = simulate_bandit(epsilon=0.1, steps=1000)\n",
    "print(\"Average Reward:\", avg_reward)\n",
    "print(\"Estimated Q-values:\", Q_estimates)\n",
    "print(\"Counts (number of times each campaign was chosen):\", counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Epsilon Values\n",
    "\n",
    "Now that your simulation is working, try running the `simulate_bandit` function with different epsilon values (for example, 0.0, 0.1, and 0.5). In a new code cell, experiment with these values and answer the following questions in a Markdown cell:\n",
    "\n",
    "1. **How does increasing epsilon affect the balance between exploration and exploitation?**\n",
    "\n",
    "2. **Which epsilon value gives you the best average reward in this simulation, and why do you think that is the case?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different epsilon values\n",
    "epsilons = [0.0, 0.1, 0.5]\n",
    "results = {}\n",
    "\n",
    "for eps in epsilons:\n",
    "    avg_reward, Q_estimates, counts = simulate_bandit(epsilon=eps, steps=1000)\n",
    "    results[eps] = avg_reward\n",
    "    print(f\"Epsilon: {eps} --> Average Reward: {avg_reward:.3f}\")\n",
    "\n",
    "print(\"\\nExperiment Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Assessment\n",
    "\n",
    "For your assignment to pass, your implementation must achieve an average reward of **at least 0.65** when running the simulation with `epsilon = 0.1` and 1000 steps. The cell below will automatically check this condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final automated check: Pass/Fail based on average reward threshold.\n",
    "# We use epsilon = 0.1 and 1000 simulation steps.\n",
    "threshold = 0.65\n",
    "avg_reward, _, _ = simulate_bandit(epsilon=0.1, steps=1000)\n",
    "\n",
    "if avg_reward >= threshold:\n",
    "    print(\"PASS: Your algorithm achieved an average reward of {:.3f} (>= {:.2f}).\".format(avg_reward, threshold))\n",
    "else:\n",
    "    print(\"FAIL: Your algorithm achieved an average reward of {:.3f} (< {:.2f}). Please review your implementation.\".format(avg_reward, threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "1. **Complete the code:** Ensure you have filled in any missing code (if applicable) and that your simulation runs correctly.\n",
    "2. **Answer the questions:** Provide your answers to the experiment questions in a separate Markdown cell.\n",
    "3. **Run the final check:** Make sure the final pass/fail cell prints \"PASS\" before submission.\n",
    "4. **Submit the notebook:** Once you have met the performance threshold and answered the questions, submit your completed notebook.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
