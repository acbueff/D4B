{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assessment: Customer Churn Prediction\n",
    "\n",
    "In this notebook, you'll apply deep learning concepts to predict customer churn - a critical business challenge. You'll learn how neural networks can identify patterns in customer behavior data to predict which customers are likely to leave.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand how to structure data for deep learning\n",
    "2. Build and train a neural network using PyTorch\n",
    "3. Evaluate model performance on business metrics\n",
    "4. Implement improvements to a baseline model\n",
    "\n",
    "## Assessment Criteria\n",
    "To pass this assessment, your model must achieve:\n",
    "- Test accuracy > 85% on the holdout set\n",
    "- F1 score > 0.80 for the churn class\n",
    "\n",
    "These metrics ensure your model is both accurate overall and good at identifying customers likely to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "Before you start coding, consider these questions:\n",
    "- What factors do you think contribute most to customer churn?\n",
    "- How would you quantify the business cost of false negatives vs false positives?\n",
    "- Why might a high accuracy model still perform poorly in a business context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation and Feature Engineering\n",
    "\n",
    "Real-world businesses collect numerous data points about their customers. For this assessment, we'll simulate customer data with features that typically influence churn:\n",
    "\n",
    "- **Tenure**: How long the customer has been with the company\n",
    "- **Monthly Charges**: How much the customer pays each month\n",
    "- **Total Charges**: Cumulative amount paid over the customer's lifetime\n",
    "- **Support Calls**: Number of calls to customer support\n",
    "- **Usage Score**: How actively the customer uses the service/product\n",
    "\n",
    "The code below generates synthetic data with relationships between these features and churn probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_customer_data(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generates synthetic customer data with features that influence churn probability.\n",
    "    \n",
    "    Parameters:\n",
    "        n_samples: Number of customer records to generate\n",
    "        \n",
    "    Returns:\n",
    "        X: Feature matrix with shape (n_samples, 5)\n",
    "        y: Target vector (1 = churned, 0 = stayed)\n",
    "    \"\"\"\n",
    "    # Generate features\n",
    "    tenure = np.random.randint(1, 72, n_samples)  # 1-72 months\n",
    "    monthly_charges = np.random.uniform(30, 150, n_samples)  # $30-$150\n",
    "    total_charges = monthly_charges * tenure + np.random.normal(0, 100, n_samples)\n",
    "    support_calls = np.random.poisson(2, n_samples)  # Average 2 calls\n",
    "    usage_score = np.random.uniform(0, 100, n_samples)  # 0-100 usage score\n",
    "    \n",
    "    # Combine features\n",
    "    X = np.column_stack([tenure, monthly_charges, total_charges, support_calls, usage_score])\n",
    "    \n",
    "    # Generate churn labels based on a rule\n",
    "    churn_score = (\n",
    "        -0.1 * tenure +  # Longer tenure = less likely to churn\n",
    "        0.3 * (monthly_charges / 50) +  # Higher charges = more likely to churn\n",
    "        0.2 * (support_calls / 2) +  # More support calls = more likely to churn\n",
    "        -0.4 * (usage_score / 50)  # Higher usage = less likely to churn\n",
    "    )\n",
    "    \n",
    "    churn_score += np.random.normal(0, 0.1, n_samples)\n",
    "    y = (churn_score > 0).astype(np.int64)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data and split into train/test sets\n",
    "X, y = generate_customer_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Churn rate in training set: {y_train.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization and Pattern Recognition\n",
    "\n",
    "Before building a model, it's essential to explore the data to understand patterns and relationships. The visualization below shows how features differ between customers who churned and those who stayed.\n",
    "\n",
    "#### Reflection Questions:\n",
    "- What patterns do you observe in the visualizations?\n",
    "- How might these patterns inform business strategies?\n",
    "- Why is data exploration an important step before modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_relationships(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Visualizes the distribution of features for churned vs retained customers.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    feature_names = ['Tenure', 'Monthly Charges', 'Support Calls', 'Usage Score']\n",
    "    feature_indices = [0, 1, 3, 4]  # Skip total charges as it's derived from tenure\n",
    "    \n",
    "    for i, (name, idx) in enumerate(zip(feature_names, feature_indices)):\n",
    "        churned = X_train[y_train == 1, idx]\n",
    "        stayed = X_train[y_train == 0, idx]\n",
    "        \n",
    "        axes[i].hist([stayed, churned], bins=20, label=['Stayed', 'Churned'], alpha=0.6)\n",
    "        axes[i].set_title(f'{name} Distribution by Churn Status')\n",
    "        axes[i].set_xlabel(name)\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature relationships\n",
    "plot_feature_relationships(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network Architecture\n",
    "\n",
    "Now we'll build a neural network to predict customer churn. This model will:\n",
    "1. Take the 5 customer features as input\n",
    "2. Process them through hidden layers with non-linear activation functions\n",
    "3. Output a probability of churn\n",
    "\n",
    "This is a feed-forward neural network with a sigmoid activation in the output layer to produce probabilities between 0 and 1.\n",
    "\n",
    "#### Reflection Questions:\n",
    "- Why do we need non-linear activation functions?\n",
    "- How does the network architecture relate to the complexity of patterns it can learn?\n",
    "- What would happen if we used a linear model instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnPredictor(nn.Module):\n",
    "    def __init__(self, input_size=5):\n",
    "        super(ChurnPredictor, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 16)\n",
    "        self.layer2 = nn.Linear(16, 8)\n",
    "        self.layer3 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))  # ReLU helps with vanishing gradient\n",
    "        x = self.relu(self.layer2(x))  # Multiple layers capture complex patterns\n",
    "        x = self.sigmoid(self.layer3(x))  # Sigmoid squashes output to [0,1]\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ChurnPredictor()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "During training, the model learns to minimize the loss function through backpropagation and gradient descent. We'll track both training loss and validation metrics to monitor progress.\n",
    "\n",
    "#### Reflection Questions:\n",
    "- Why do we use both training and test sets?\n",
    "- What patterns in the learning curves might suggest overfitting?\n",
    "- How does backpropagation work to update the model's weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, epochs=100):\n",
    "    \"\"\"\n",
    "    Trains the neural network and tracks performance metrics.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses: List of loss values during training\n",
    "        test_metrics: List of tuples containing (epoch, accuracy, f1)\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    test_metrics = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train.view(-1, 1))\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Gradient descent\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Evaluation\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test)\n",
    "                test_preds = (test_outputs >= 0.5).float()\n",
    "                accuracy = accuracy_score(y_test, test_preds)\n",
    "                f1 = f1_score(y_test, test_preds)\n",
    "                test_metrics.append((epoch, accuracy, f1))\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, '\n",
    "                      f'Test Accuracy: {accuracy:.4f}, F1: {f1:.4f}')\n",
    "    \n",
    "    return train_losses, test_metrics\n",
    "\n",
    "# Train the model\n",
    "print(\"Training baseline model...\")\n",
    "train_losses, test_metrics = train_model(model, X_train_tensor, y_train_tensor, \n",
    "                                        X_test_tensor, y_test_tensor,\n",
    "                                        criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Training Progress\n",
    "\n",
    "Visualizing the training process helps us understand how the model learns over time. We'll plot both the training loss and test metrics.\n",
    "\n",
    "#### Reflection Questions:\n",
    "- What does the convergence of the loss curve tell us?\n",
    "- How do we know if we've trained for enough epochs?\n",
    "- Why might accuracy and F1 score evolve differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, test_metrics):\n",
    "    \"\"\"\n",
    "    Visualizes training loss and test metrics over epochs.\n",
    "    \"\"\"\n",
    "    epochs, accuracies, f1_scores = zip(*test_metrics)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    # Plot test metrics\n",
    "    ax2.plot(epochs, accuracies, label='Accuracy')\n",
    "    ax2.plot(epochs, f1_scores, label='F1 Score')\n",
    "    ax2.set_title('Test Metrics')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training progress\n",
    "plot_training_progress(train_losses, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Now we'll evaluate the model's performance on the test set using several metrics:\n",
    "- Accuracy: Overall percentage of correct predictions\n",
    "- F1 Score: Harmonic mean of precision and recall\n",
    "- Classification Report: Detailed breakdown of precision, recall, and F1 by class\n",
    "- Confusion Matrix: Visualization of true vs. predicted labels\n",
    "\n",
    "#### Reflection Questions:\n",
    "- Which metric is most important for our business case?\n",
    "- How do these metrics translate to business value?\n",
    "- Why might high accuracy be misleading for imbalanced datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates model performance and checks if it passes assessment criteria.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_preds = (test_outputs >= 0.5).float()\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, test_preds)\n",
    "        f1 = f1_score(y_test, test_preds)\n",
    "        \n",
    "        print(\"\\nFinal Model Performance:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"\\nDetailed Classification Report:\")\n",
    "        print(classification_report(y_test, test_preds))\n",
    "        \n",
    "        # Check if model passes assessment criteria\n",
    "        passes_accuracy = accuracy > 0.85\n",
    "        passes_f1 = f1 > 0.80\n",
    "        \n",
    "        print(\"\\nAssessment Results:\")\n",
    "        print(f\"Accuracy > 85%: {'✓' if passes_accuracy else '✗'}\")\n",
    "        print(f\"F1 Score > 0.80: {'✓' if passes_f1 else '✗'}\")\n",
    "        print(f\"Overall: {'PASS' if (passes_accuracy and passes_f1) else 'FAIL'}\")\n",
    "        \n",
    "        return passes_accuracy and passes_f1\n",
    "\n",
    "# Evaluate the baseline model\n",
    "base_passed = evaluate_model(model, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Visualizes confusion matrix and calculates business metrics.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate business metrics\n",
    "    retention_rate = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "    detection_rate = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "    print(f\"\\nBusiness Metrics:\")\n",
    "    print(f\"Customer Retention Rate: {retention_rate:.2%}\")\n",
    "    print(f\"Churn Detection Rate: {detection_rate:.2%}\")\n",
    "\n",
    "# Plot confusion matrix for baseline model\n",
    "with torch.no_grad():\n",
    "    base_preds = (model(X_test_tensor) >= 0.5).float()\n",
    "plot_confusion_matrix(y_test_tensor, base_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Your Assessment Task: Improve the Model\n",
    "\n",
    "If your model didn't pass both criteria (accuracy > 85% and F1 > 0.80), your task is to improve it.\n",
    "\n",
    "You can:\n",
    "1. Add more layers or neurons\n",
    "2. Implement regularization techniques like dropout\n",
    "3. Add batch normalization\n",
    "4. Adjust the learning rate\n",
    "5. Use a different optimizer\n",
    "6. Train for more epochs\n",
    "\n",
    "Complete the `ImprovedChurnPredictor` class below with your enhanced architecture, then train and evaluate it.\n",
    "\n",
    "#### Reflection Questions:\n",
    "- How does dropout help prevent overfitting?\n",
    "- Why use different layer sizes in the architecture?\n",
    "- How do batch normalization and regularization affect model training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedChurnPredictor(nn.Module):\n",
    "    def __init__(self, input_size=5):\n",
    "        super(ImprovedChurnPredictor, self).__init__()\n",
    "        # TODO: Implement your improved architecture\n",
    "        # Add layers, dropout, batch normalization, etc.\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        pass\n",
    "\n",
    "# TODO: Initialize and train your improved model\n",
    "# improved_model = ImprovedChurnPredictor()\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(improved_model.parameters(), lr=0.005)\n",
    "# train_losses, test_metrics = train_model(improved_model, X_train_tensor, y_train_tensor,\n",
    "#                                        X_test_tensor, y_test_tensor,\n",
    "#                                        criterion, optimizer, epochs=150)\n",
    "\n",
    "# TODO: Evaluate your improved model\n",
    "# plot_training_progress(train_losses, test_metrics)\n",
    "# improved_passed = evaluate_model(improved_model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# TODO: Generate and plot confusion matrix\n",
    "# with torch.no_grad():\n",
    "#     improved_preds = (improved_model(X_test_tensor) >= 0.5).float()\n",
    "# plot_confusion_matrix(y_test_tensor, improved_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Business Analysis\n",
    "\n",
    "After completing the assessment, consider these final reflection questions:\n",
    "\n",
    "1. What improvements in your model led to better performance?\n",
    "2. How would you implement this model in a production environment?\n",
    "3. What additional data would make the model more effective?\n",
    "4. How would you quantify the ROI of this churn prediction system?\n",
    "5. What ethical considerations should be addressed when deploying this model?\n",
    "\n",
    "In your answer, analyze both the technical improvements and their business implications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 