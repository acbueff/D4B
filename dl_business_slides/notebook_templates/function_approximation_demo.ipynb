{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Function Approximation: Business Applications\n",
    "\n",
    "This notebook demonstrates how neural networks can learn to model complex, non-linear relationships - a fundamental capability that powers many business applications. We'll explore how even a simple neural network can approximate complicated functions, and how this ability translates to real-world business problems.\n",
    "\n",
    "## Business Context\n",
    "\n",
    "Many business relationships are inherently non-linear:\n",
    "- Marketing spend vs. return on investment\n",
    "- Price vs. demand elasticity\n",
    "- Product features vs. customer satisfaction\n",
    "- Resource allocation vs. productivity\n",
    "\n",
    "Traditional approaches often use oversimplified linear models or require manual feature engineering to capture these non-linearities. Neural networks can automatically learn these complex relationships directly from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries for our deep learning exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure plots\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Data\n",
    "\n",
    "Let's create synthetic data that represents a non-linear business relationship. We'll use a quadratic function with some noise, which could represent a real-world scenario like:\n",
    "\n",
    "- Price optimization curve (price vs. profit)\n",
    "- Marketing spend efficiency curve (spend vs. conversions)\n",
    "- Production volume vs. unit cost (economies of scale)\n",
    "\n",
    "Our function will be $y = x^2 + \\text{noise}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the underlying function: y = x^2 with a bit of noise\n",
    "def generate_data(x):\n",
    "    noise = 0.05 * np.random.randn(*x.shape)\n",
    "    return x**2 + noise\n",
    "\n",
    "# Create the training data: 20 samples from a uniform distribution in [-1, 1]\n",
    "n_train = 20\n",
    "X_train = np.random.uniform(-1, 1, (n_train, 1)).astype(np.float32)\n",
    "y_train = generate_data(X_train)\n",
    "\n",
    "# Create validation data:\n",
    "# 30 samples (about 30%) similar to training: uniform(-1, 1)\n",
    "n_val_similar = 30\n",
    "X_val_similar = np.random.uniform(-1, 1, (n_val_similar, 1)).astype(np.float32)\n",
    "y_val_similar = generate_data(X_val_similar)\n",
    "\n",
    "# 70 samples (about 70%) out-of-distribution: uniform(2, 3)\n",
    "n_val_out = 70\n",
    "X_val_out = np.random.uniform(2, 3, (n_val_out, 1)).astype(np.float32)\n",
    "y_val_out = generate_data(X_val_out)\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors with explicit float32 type\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_similar_tensor = torch.tensor(X_val_similar, dtype=torch.float32)\n",
    "y_val_similar_tensor = torch.tensor(y_val_similar, dtype=torch.float32)\n",
    "X_val_out_tensor = torch.tensor(X_val_out, dtype=torch.float32)\n",
    "y_val_out_tensor = torch.tensor(y_val_out, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our training data and the true underlying function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine grid of x values to plot the true function\n",
    "x_grid = np.linspace(-1.5, 3.5, 500).reshape(-1, 1)\n",
    "y_true = x_grid**2  # True function without noise\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(x_grid, y_true, 'g-', linewidth=2, label='True Function (y = x²)')\n",
    "plt.scatter(X_train, y_train, color='blue', s=80, alpha=0.7, label='Training Data')\n",
    "plt.scatter(X_val_similar, y_val_similar, color='orange', s=40, alpha=0.7, label='Validation Data (Similar Distribution)')\n",
    "plt.scatter(X_val_out, y_val_out, color='red', s=40, alpha=0.7, label='Validation Data (Out-of-Distribution)')\n",
    "\n",
    "plt.axvspan(-1, 1, alpha=0.1, color='blue', label='Training Distribution Range')\n",
    "plt.axvspan(2, 3, alpha=0.1, color='red', label='Out-of-Distribution Range')\n",
    "\n",
    "plt.title('Training and Validation Data with True Function', fontsize=16)\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Neural Network Models\n",
    "\n",
    "We'll create two neural network architectures to demonstrate different approaches to function approximation:\n",
    "\n",
    "1. **Small Network:** A simple architecture with one hidden layer\n",
    "2. **Large Network:** A more complex architecture with multiple hidden layers\n",
    "\n",
    "This will help us understand the trade-offs between model complexity, generalization, and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 5)  # Input dimension is 1, hidden layer has 5 neurons\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5, 1)  # Output dimension is 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class LargeNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 64),  # Input layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),  # Hidden layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),  # Hidden layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)    # Output layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function\n",
    "\n",
    "Now let's define a function to train our neural networks and track their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val_similar, y_val_similar, X_val_out, y_val_out, \n",
    "                epochs=1000, learning_rate=0.01, weight_decay=0):\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Lists to store metrics for plotting\n",
    "    train_losses = []\n",
    "    val_similar_losses = []\n",
    "    val_out_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Record training and validation losses\n",
    "        with torch.no_grad():\n",
    "            train_loss = criterion(model(X_train), y_train).item()\n",
    "            val_similar_loss = criterion(model(X_val_similar), y_val_similar).item()\n",
    "            val_out_loss = criterion(model(X_val_out), y_val_out).item()\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_similar_losses.append(val_similar_loss)\n",
    "            val_out_losses.append(val_out_loss)\n",
    "        \n",
    "        # Print progress every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Similar Loss: {val_similar_loss:.4f}, Val Out Loss: {val_out_loss:.4f}')\n",
    "    \n",
    "    return model, {'train': train_losses, 'val_similar': val_similar_losses, 'val_out': val_out_losses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate the Small Network\n",
    "\n",
    "Now let's train our small network and see how well it can approximate the quadratic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = SmallNetwork()\n",
    "trained_small_model, small_model_history = train_model(\n",
    "    small_model, X_train_tensor, y_train_tensor, \n",
    "    X_val_similar_tensor, y_val_similar_tensor,\n",
    "    X_val_out_tensor, y_val_out_tensor,\n",
    "    epochs=1000, learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the learning curves to see how our model trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, title=\"Learning Curves\"):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    epochs = range(1, len(history['train']) + 1)\n",
    "    \n",
    "    plt.plot(epochs, history['train'], 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.plot(epochs, history['val_similar'], 'g-', linewidth=2, label='Validation Loss (Similar Distribution)')\n",
    "    plt.plot(epochs, history['val_out'], 'r-', linewidth=2, label='Validation Loss (Out-of-Distribution)')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('Loss (MSE)', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(small_model_history, \"Small Network Learning Curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize how well our small model approximates the true function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, title=\"Model Predictions vs True Function\"):\n",
    "    # Convert grid to tensor for prediction\n",
    "    x_grid_tensor = torch.tensor(x_grid, dtype=torch.float32)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_pred = model(x_grid_tensor).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Plot true function\n",
    "    plt.plot(x_grid, y_true, 'g-', linewidth=2, label='True Function (y = x²)')\n",
    "    \n",
    "    # Plot model predictions\n",
    "    plt.plot(x_grid, y_pred, 'b--', linewidth=2, label='Model Predictions')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X_train, y_train, color='blue', s=80, alpha=0.7, label='Training Data')\n",
    "    \n",
    "    # Highlight distribution ranges\n",
    "    plt.axvspan(-1, 1, alpha=0.1, color='blue', label='Training Distribution Range')\n",
    "    plt.axvspan(2, 3, alpha=0.1, color='red', label='Out-of-Distribution Range')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('x', fontsize=14)\n",
    "    plt.ylabel('y', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-1, 10)\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(trained_small_model, \"Small Network Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train and Evaluate the Large Network\n",
    "\n",
    "Now let's train our larger, more complex network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model = LargeNetwork()\n",
    "trained_large_model, large_model_history = train_model(\n",
    "    large_model, X_train_tensor, y_train_tensor, \n",
    "    X_val_similar_tensor, y_val_similar_tensor,\n",
    "    X_val_out_tensor, y_val_out_tensor,\n",
    "    epochs=1000, learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the learning curves for the large network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(large_model_history, \"Large Network Learning Curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's see the predictions from our large model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(trained_large_model, \"Large Network Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Improving Generalization with Regularization\n",
    "\n",
    "Let's try to improve our model's ability to generalize to out-of-distribution data by adding regularization. We'll train the large network again with weight decay (L2 regularization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_model = LargeNetwork()\n",
    "trained_regularized_model, regularized_model_history = train_model(\n",
    "    regularized_model, X_train_tensor, y_train_tensor, \n",
    "    X_val_similar_tensor, y_val_similar_tensor,\n",
    "    X_val_out_tensor, y_val_out_tensor,\n",
    "    epochs=1000, learning_rate=0.01, weight_decay=0.01  # Added weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the learning curves for the regularized network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(regularized_model_history, \"Regularized Network Learning Curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's see if regularization helped with generalizing to out-of-distribution data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(trained_regularized_model, \"Regularized Network Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Model Performance\n",
    "\n",
    "Let's quantitatively compare the performance of our three models on the training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val_similar, y_val_similar, X_val_out, y_val_out):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Calculate losses\n",
    "        train_loss = criterion(model(X_train), y_train).item()\n",
    "        val_similar_loss = criterion(model(X_val_similar), y_val_similar).item()\n",
    "        val_out_loss = criterion(model(X_val_out), y_val_out).item()\n",
    "    \n",
    "    return {\n",
    "        'Train Loss (MSE)': train_loss,\n",
    "        'Validation Loss (Similar Distribution)': val_similar_loss,\n",
    "        'Validation Loss (Out-of-Distribution)': val_out_loss\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "small_model_perf = evaluate_model(trained_small_model, X_train_tensor, y_train_tensor, \n",
    "                                 X_val_similar_tensor, y_val_similar_tensor,\n",
    "                                 X_val_out_tensor, y_val_out_tensor)\n",
    "\n",
    "large_model_perf = evaluate_model(trained_large_model, X_train_tensor, y_train_tensor, \n",
    "                                 X_val_similar_tensor, y_val_similar_tensor,\n",
    "                                 X_val_out_tensor, y_val_out_tensor)\n",
    "\n",
    "regularized_model_perf = evaluate_model(trained_regularized_model, X_train_tensor, y_train_tensor, \n",
    "                                       X_val_similar_tensor, y_val_similar_tensor,\n",
    "                                       X_val_out_tensor, y_val_out_tensor)\n",
    "\n",
    "# Create a comparison dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Small Network': small_model_perf,\n",
    "    'Large Network': large_model_perf,\n",
    "    'Regularized Network': regularized_model_perf\n",
    "}).T\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart comparison\n",
    "results_melted = results.reset_index().melt(id_vars='index', var_name='Metric', value_name='Loss')\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "bar_plot = sns.barplot(x='Metric', y='Loss', hue='index', data=results_melted)\n",
    "\n",
    "plt.title('Model Performance Comparison', fontsize=16)\n",
    "plt.xlabel('', fontsize=14)\n",
    "plt.ylabel('Mean Squared Error (MSE)', fontsize=14)\n",
    "plt.yscale('log')  # Log scale for better visualization\n",
    "plt.legend(title='Model', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Business Implications\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **In-Distribution vs. Out-of-Distribution Performance:**\n",
    "   - All models perform well on data similar to the training distribution\n",
    "   - Performance degrades significantly on out-of-distribution data\n",
    "   - Regularization improves generalization to new regions\n",
    "\n",
    "2. **Model Complexity Trade-offs:**\n",
    "   - Small model: Limited capacity, but better generalization\n",
    "   - Large model: Better fit to training data, but overfits\n",
    "   - Regularized model: Better balance of fit and generalization\n",
    "\n",
    "3. **Learning Process:**\n",
    "   - Models learn the quadratic relationship from limited data\n",
    "   - Neural networks automatically capture non-linearity\n",
    "   - Out-of-distribution performance requires careful consideration\n",
    "\n",
    "### Business Applications\n",
    "\n",
    "1. **Price Optimization:**\n",
    "   - Learning the relationship between price and demand\n",
    "   - Critical to validate predictions in new price ranges\n",
    "   - Regularization helps when testing untested price points\n",
    "\n",
    "2. **Marketing Budget Allocation:**\n",
    "   - Modeling how marketing spend affects campaign performance\n",
    "   - Avoid overfitting to historical spending patterns\n",
    "   - Use regularization when exploring new spending levels\n",
    "\n",
    "3. **Resource Planning:**\n",
    "   - Predicting how resource allocation affects productivity\n",
    "   - Ensuring models generalize to new operating conditions\n",
    "   - Avoiding extrapolation errors in critical planning\n",
    "\n",
    "### Best Practices for Business Implementation\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Sample across the full range of expected values\n",
    "   - Include diverse operating conditions\n",
    "   - Test model on out-of-sample data before deployment\n",
    "\n",
    "2. **Model Selection:**\n",
    "   - Match model complexity to data availability\n",
    "   - Use regularization for better generalization\n",
    "   - Consider ensemble methods for critical applications\n",
    "\n",
    "3. **Operational Safeguards:**\n",
    "   - Monitor predictions for out-of-distribution inputs\n",
    "   - Implement confidence intervals around predictions\n",
    "   - Establish business rules for extreme predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Learning Challenge: Marketing ROI Curve\n",
    "\n",
    "**Scenario:** A marketing team has collected data on advertising spend vs. return on investment (ROI). The relationship is known to be non-linear, with diminishing returns at higher spending levels.\n",
    "\n",
    "**Exercise:** \n",
    "1. Generate synthetic data that represents a typical marketing ROI curve\n",
    "2. Train a neural network to learn this relationship\n",
    "3. Use the model to predict the optimal marketing spend level\n",
    "4. Implement regularization to improve generalization\n",
    "5. Visualize and interpret the results\n",
    "\n",
    "**Starter Code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic marketing ROI data\n",
    "# Typical ROI curve: initially increases, then plateaus, then decreases\n",
    "def marketing_roi(spend):\n",
    "    # ROI starts positive, peaks, then diminishes\n",
    "    # Return is in dollars, spend is in thousands\n",
    "    return 2 * spend - 0.5 * spend**2 + 0.1 * np.random.randn(*spend.shape)\n",
    "\n",
    "# Generate training data\n",
    "spend_train = np.random.uniform(0, 2, (30, 1)).astype(np.float32)  # Marketing spend from $0K to $2K\n",
    "roi_train = marketing_roi(spend_train)\n",
    "\n",
    "# Generate test data including higher spend levels\n",
    "spend_test = np.linspace(0, 4, 100).reshape(-1, 1).astype(np.float32)  # Testing from $0K to $4K\n",
    "roi_test = marketing_roi(spend_test)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(spend_train, roi_train, color='blue', s=80, alpha=0.7, label='Training Data')\n",
    "plt.plot(spend_test, roi_test, 'g-', linewidth=2, alpha=0.5, label='True ROI Curve')\n",
    "plt.title('Marketing Spend vs. ROI', fontsize=16)\n",
    "plt.xlabel('Marketing Spend ($K)', fontsize=14)\n",
    "plt.ylabel('Return on Investment ($K)', fontsize=14)\n",
    "plt.axvspan(0, 2, alpha=0.1, color='blue', label='Training Range')\n",
    "plt.axvspan(2, 4, alpha=0.1, color='red', label='Extrapolation Range')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Your task: Build and train a neural network to model this relationship\n",
    "# Then use it to find the optimal marketing spend level\n",
    "\n",
    "# 1. Convert data to PyTorch tensors\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 2. Define a neural network model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 3. Train the model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 4. Find the optimal spend level that maximizes ROI\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 5. Visualize the results\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how neural networks can learn to approximate complex non-linear functions from data - a fundamental capability that powers many business applications. Key takeaways include:\n",
    "\n",
    "1. Neural networks automatically learn appropriate representations without manual feature engineering\n",
    "2. Model complexity needs to be balanced with the amount of available training data\n",
    "3. Regularization techniques improve generalization to new, unseen data points\n",
    "4. Performance in out-of-distribution regions requires careful validation\n",
    "5. These concepts apply directly to business problems like pricing optimization, marketing spend efficiency, and resource planning\n",
    "\n",
    "The ability to model complex, non-linear relationships is at the heart of deep learning's value for business applications. By understanding these fundamental concepts, you can apply neural networks effectively to a wide range of business problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
