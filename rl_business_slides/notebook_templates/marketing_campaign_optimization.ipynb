{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Marketing Campaign Optimization using Reinforcement Learning\n",
       "\n",
       "This notebook demonstrates how to use a simple reinforcement learning algorithm (multi-armed bandit) to optimize the selection of marketing campaigns. We'll simulate a scenario where we need to choose between different marketing campaigns with unknown conversion rates, learning which performs best through trial and error.\n",
       "\n",
       "## Business Context\n",
       "\n",
       "Marketing teams often face the challenge of allocating limited budget across different campaign strategies. Traditionally, this might involve:\n",
       "- Running A/B tests for a fixed period\n",
       "- Analyzing results after the test period\n",
       "- Allocating the remaining budget to the winner\n",
       "\n",
       "With a multi-armed bandit approach, we can:\n",
       "- Continuously learn which campaigns perform best\n",
       "- Gradually shift budget toward high-performing campaigns\n",
       "- Continue exploring new options to adapt to changing conditions\n",
       "\n",
       "This approach maximizes overall campaign performance while still gathering valuable information about all options."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. The Multi-Armed Bandit Problem\n",
       "\n",
       "A multi-armed bandit problem is named after a casino slot machine (\"one-armed bandit\") but with multiple arms. Each arm, when pulled, provides a reward from a probability distribution specific to that arm. The objective is to maximize the total reward over a period of time.\n",
       "\n",
       "In our marketing context:\n",
       "- Each arm represents a different marketing campaign\n",
       "- Pulling an arm means showing a campaign to a customer\n",
       "- The reward is whether the customer converts (1) or not (0)\n",
       "- Each campaign has an unknown true conversion rate\n",
       "\n",
       "Let's set up our simulation with three different marketing campaigns with the following (hidden) conversion rates:\n",
       "- Campaign 1: 20% conversion rate\n",
       "- Campaign 2: 50% conversion rate\n",
       "- Campaign 3: 70% conversion rate\n",
       "\n",
       "In a real business scenario, we wouldn't know these rates in advance - we'd learn them through experimentation."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import pandas as pd\n",
       "import seaborn as sns\n",
       "import random\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "np.random.seed(42)\n",
       "random.seed(42)\n",
       "\n",
       "# True conversion rates for each campaign (unknown to the algorithm)\n",
       "true_conversion_rates = [0.2, 0.5, 0.7]\n",
       "num_campaigns = len(true_conversion_rates)\n",
       "\n",
       "# Function to simulate a campaign result\n",
       "def get_campaign_result(campaign_index):\n",
       "    \"\"\"Return 1 (conversion) or 0 (no conversion) based on the true conversion rate\"\"\"\n",
       "    if random.random() < true_conversion_rates[campaign_index]:\n",
       "        return 1  # Conversion\n",
       "    else:\n",
       "        return 0  # No conversion\n",
       "\n",
       "# Let's test our function\n",
       "test_results = [get_campaign_result(0) for _ in range(1000)]\n",
       "print(f\"Campaign 1 test conversion rate: {sum(test_results)/len(test_results):.3f} (should be close to {true_conversion_rates[0]})\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Exploration vs. Exploitation\n",
       "\n",
       "The key challenge in multi-armed bandit problems is balancing:\n",
       "\n",
       "- **Exploration**: Trying different campaigns to learn their conversion rates\n",
       "- **Exploitation**: Selecting the best-performing campaign based on current knowledge\n",
       "\n",
       "If we exploit too early, we might miss better campaigns. If we explore too much, we waste opportunities on lower-performing campaigns.\n",
       "\n",
       "### Random Strategy (Pure Exploration)\n",
       "\n",
       "Let's first implement a purely random strategy that selects campaigns with equal probability, regardless of their performance. This represents pure exploration with no exploitation."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def random_strategy(num_simulations=1000):\n",
       "    conversions = 0\n",
       "    campaign_counts = np.zeros(num_campaigns)\n",
       "    \n",
       "    for _ in range(num_simulations):\n",
       "        # Randomly select a campaign\n",
       "        campaign = random.randint(0, num_campaigns-1)\n",
       "        \n",
       "        # Get result and update counts\n",
       "        result = get_campaign_result(campaign)\n",
       "        conversions += result\n",
       "        campaign_counts[campaign] += 1\n",
       "    \n",
       "    print(f\"Random Strategy Results:\")\n",
       "    print(f\"Total Conversions: {conversions} out of {num_simulations} ({conversions/num_simulations:.3f})\")\n",
       "    print(f\"Campaign Selection Counts: {campaign_counts}\")\n",
       "    return conversions, campaign_counts\n",
       "\n",
       "random_conversions, random_counts = random_strategy(10000)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Greedy Strategy (Pure Exploitation)\n",
       "\n",
       "Now let's implement a purely greedy strategy that always chooses the campaign with the highest observed conversion rate. After a brief initial exploration phase, it will exploit the best option."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def greedy_strategy(num_simulations=1000):\n",
       "    conversions = 0\n",
       "    campaign_counts = np.zeros(num_campaigns)\n",
       "    campaign_conversions = np.zeros(num_campaigns)\n",
       "    \n",
       "    # Initial exploration: try each campaign once\n",
       "    for campaign in range(num_campaigns):\n",
       "        result = get_campaign_result(campaign)\n",
       "        conversions += result\n",
       "        campaign_counts[campaign] += 1\n",
       "        campaign_conversions[campaign] += result\n",
       "    \n",
       "    # Main simulation loop\n",
       "    for _ in range(num_simulations - num_campaigns):\n",
       "        # Calculate current conversion rates\n",
       "        conversion_rates = campaign_conversions / campaign_counts\n",
       "        \n",
       "        # Select campaign with highest rate\n",
       "        campaign = np.argmax(conversion_rates)\n",
       "        \n",
       "        # Get result and update counts\n",
       "        result = get_campaign_result(campaign)\n",
       "        conversions += result\n",
       "        campaign_counts[campaign] += 1\n",
       "        campaign_conversions[campaign] += result\n",
       "    \n",
       "    print(f\"Greedy Strategy Results:\")\n",
       "    print(f\"Total Conversions: {conversions} out of {num_simulations} ({conversions/num_simulations:.3f})\")\n",
       "    print(f\"Campaign Selection Counts: {campaign_counts}\")\n",
       "    print(f\"Estimated Conversion Rates: {campaign_conversions/campaign_counts}\")\n",
       "    return conversions, campaign_counts, campaign_conversions\n",
       "\n",
       "greedy_conversions, greedy_counts, greedy_campaign_conversions = greedy_strategy(10000)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Epsilon-Greedy Algorithm\n",
       "\n",
       "The epsilon-greedy algorithm provides a simple way to balance exploration and exploitation:\n",
       "\n",
       "- With probability ε (epsilon), select a random campaign (exploration)\n",
       "- With probability 1-ε, select the campaign with the highest observed conversion rate (exploitation)\n",
       "\n",
       "Let's implement this algorithm and test it with different values of epsilon."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def epsilon_greedy_strategy(epsilon, num_simulations=1000):\n",
       "    conversions = 0\n",
       "    campaign_counts = np.zeros(num_campaigns)\n",
       "    campaign_conversions = np.zeros(num_campaigns)\n",
       "    \n",
       "    # For tracking learning progress\n",
       "    history = []\n",
       "    \n",
       "    # Initial exploration: try each campaign once\n",
       "    for campaign in range(num_campaigns):\n",
       "        result = get_campaign_result(campaign)\n",
       "        conversions += result\n",
       "        campaign_counts[campaign] += 1\n",
       "        campaign_conversions[campaign] += result\n",
       "        \n",
       "        # Track cumulative conversion rate\n",
       "        history.append(conversions / (campaign + 1))\n",
       "    \n",
       "    # Main simulation loop\n",
       "    for i in range(num_simulations - num_campaigns):\n",
       "        # Epsilon-greedy action selection\n",
       "        if random.random() < epsilon:\n",
       "            # Exploration: choose random campaign\n",
       "            campaign = random.randint(0, num_campaigns-1)\n",
       "        else:\n",
       "            # Exploitation: choose campaign with highest estimated conversion rate\n",
       "            conversion_rates = campaign_conversions / campaign_counts\n",
       "            campaign = np.argmax(conversion_rates)\n",
       "        \n",
       "        # Get result and update counts\n",
       "        result = get_campaign_result(campaign)\n",
       "        conversions += result\n",
       "        campaign_counts[campaign] += 1\n",
       "        campaign_conversions[campaign] += result\n",
       "        \n",
       "        # Track cumulative conversion rate\n",
       "        history.append(conversions / (i + num_campaigns + 1))\n",
       "    \n",
       "    print(f\"Epsilon-Greedy (ε={epsilon}) Results:\")\n",
       "    print(f\"Total Conversions: {conversions} out of {num_simulations} ({conversions/num_simulations:.3f})\")\n",
       "    print(f\"Campaign Selection Counts: {campaign_counts}\")\n",
       "    print(f\"Estimated Conversion Rates: {campaign_conversions/campaign_counts}\")\n",
       "    return conversions, campaign_counts, campaign_conversions, history\n",
       "\n",
       "# Test with different epsilon values\n",
       "epsilons = [0.0, 0.1, 0.3, 0.5]\n",
       "results = {}\n",
       "\n",
       "for eps in epsilons:\n",
       "    results[eps] = epsilon_greedy_strategy(eps, 10000)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Analyzing the Results\n",
       "\n",
       "Let's visualize and compare the performance of different strategies:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare data for plotting\n",
       "strategies = ['Random'] + [f'ε-Greedy (ε={eps})' for eps in epsilons]\n",
       "conversion_rates = [random_conversions/10000] + [results[eps][0]/10000 for eps in epsilons]\n",
       "\n",
       "# Create bar chart\n",
       "plt.figure(figsize=(12, 6))\n",
       "bars = plt.bar(strategies, conversion_rates, color=sns.color_palette(\"viridis\", len(strategies)))\n",
       "plt.axhline(y=max(true_conversion_rates), color='r', linestyle='--', alpha=0.7, label=f'Best Campaign ({max(true_conversion_rates):.2f})')\n",
       "plt.axhline(y=sum(true_conversion_rates)/len(true_conversion_rates), color='gray', linestyle='--', alpha=0.7, \n",
       "            label=f'Average Campaign ({sum(true_conversion_rates)/len(true_conversion_rates):.2f})')\n",
       "\n",
       "# Add value labels on top of bars\n",
       "for bar in bars:\n",
       "    height = bar.get_height()\n",
       "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}', \n",
       "             ha='center', va='bottom', fontweight='bold')\n",
       "\n",
       "plt.ylim(0, max(true_conversion_rates) + 0.1)\n",
       "plt.ylabel('Overall Conversion Rate')\n",
       "plt.xlabel('Strategy')\n",
       "plt.title('Performance Comparison of Different Campaign Selection Strategies')\n",
       "plt.legend()\n",
       "plt.grid(axis='y', alpha=0.3)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot campaign selection counts\n",
       "selection_data = {\n",
       "    'Random': random_counts,\n",
       "}\n",
       "for eps in epsilons:\n",
       "    selection_data[f'ε-Greedy (ε={eps})'] = results[eps][1]\n",
       "\n",
       "# Create DataFrame for plotting\n",
       "selection_df = pd.DataFrame(selection_data, index=[f'Campaign {i+1} ({rate:.1f})' for i, rate in enumerate(true_conversion_rates)])\n",
       "\n",
       "# Create stacked bar chart\n",
       "ax = selection_df.plot(kind='bar', stacked=False, figsize=(14, 7), \n",
       "                     color=sns.color_palette(\"viridis\", len(selection_data)))\n",
       "\n",
       "plt.title('Campaign Selection Distribution by Strategy')\n",
       "plt.xlabel('Campaign (True Conversion Rate)')\n",
       "plt.ylabel('Number of Times Selected')\n",
       "plt.legend(title='Strategy')\n",
       "plt.grid(axis='y', alpha=0.3)\n",
       "\n",
       "# Add value labels to each bar\n",
       "for container in ax.containers:\n",
       "    ax.bar_label(container, fmt='%d', fontweight='bold')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot learning curves for epsilon-greedy strategies\n",
       "plt.figure(figsize=(14, 7))\n",
       "\n",
       "for eps in epsilons:\n",
       "    history = results[eps][3]\n",
       "    plt.plot(history, label=f'ε-Greedy (ε={eps})')\n",
       "\n",
       "plt.axhline(y=max(true_conversion_rates), color='r', linestyle='--', alpha=0.7, label=f'Best Campaign ({max(true_conversion_rates):.2f})')\n",
       "plt.axhline(y=sum(true_conversion_rates)/len(true_conversion_rates), color='gray', linestyle='--', alpha=0.7, \n",
       "            label=f'Average Campaign ({sum(true_conversion_rates)/len(true_conversion_rates):.2f})')\n",
       "\n",
       "plt.title('Learning Curve: Cumulative Conversion Rate Over Time')\n",
       "plt.xlabel('Number of Customers')\n",
       "plt.ylabel('Cumulative Conversion Rate')\n",
       "plt.legend()\n",
       "plt.grid(alpha=0.3)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Business Interpretation\n",
       "\n",
       "Let's analyze these results from a business perspective:\n",
       "\n",
       "1. **Random Strategy (Pure Exploration):**\n",
       "   - Evenly distributes traffic across all campaigns\n",
       "   - Achieves average performance (around 0.467)\n",
       "   - Provides good data about all campaigns, but sub-optimal overall performance\n",
       "   - Business Use Case: Initial testing phase when you have no prior data\n",
       "\n",
       "2. **Greedy Strategy (ε=0.0):**\n",
       "   - Quickly commits to the campaign that appears best in early testing\n",
       "   - Can get \"stuck\" with a sub-optimal campaign if early data is misleading\n",
       "   - When it finds the best campaign, achieves high performance\n",
       "   - Business Use Case: Short-term campaigns where quick optimization is needed\n",
       "\n",
       "3. **Balanced Strategy (ε=0.1):**\n",
       "   - Allocates 90% of traffic to the best-performing campaign, 10% to exploration\n",
       "   - Balances learning with performance optimization\n",
       "   - Usually finds the best campaign while maintaining strong overall performance\n",
       "   - Business Use Case: Most marketing campaigns, especially ongoing ones\n",
       "\n",
       "4. **High-Exploration Strategies (ε=0.3, ε=0.5):**\n",
       "   - Allocate significant traffic to exploration\n",
       "   - More likely to find the best campaign but at cost of overall performance\n",
       "   - Business Use Case: When campaign performance is highly uncertain or environment changes rapidly\n",
       "\n",
       "### Key Business Insights:\n",
       "\n",
       "- The optimal level of exploration (ε) depends on:\n",
       "  - The lifecycle stage of your campaigns\n",
       "  - How much performance variance exists between campaigns\n",
       "  - How quickly customer preferences change\n",
       "  - Your tolerance for short-term performance drops\n",
       "\n",
       "- In our simulation, ε=0.1 provided a good balance, but real-world applications might require adjustment\n",
       "\n",
       "- Consider decaying ε over time: start with higher exploration, then gradually reduce as you gain confidence in your estimates"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Advanced Topics\n",
       "\n",
       "While epsilon-greedy is a simple and effective approach, more sophisticated reinforcement learning techniques can provide additional benefits in marketing campaign optimization:\n",
       "\n",
       "### 1. Upper Confidence Bound (UCB) Algorithm\n",
       "\n",
       "UCB balances exploration and exploitation by considering both the estimated value and the uncertainty of each option. It explores options with high uncertainty and high potential.\n",
       "\n",
       "### 2. Thompson Sampling\n",
       "\n",
       "Thompson sampling uses Bayesian methods to represent uncertainty about campaign performance. It naturally balances exploration and exploitation based on probability distributions.\n",
       "\n",
       "### 3. Contextual Bandits\n",
       "\n",
       "Contextual bandits extend multi-armed bandits by considering customer features (context) when selecting campaigns. This allows for personalization based on customer segments or attributes.\n",
       "\n",
       "### 4. Non-Stationary Bandits\n",
       "\n",
       "Non-stationary bandits handle environments where conversion rates change over time, such as seasonal variations or changing customer preferences.\n",
       "\n",
       "Let's implement a simple version of UCB to see how it compares to epsilon-greedy:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def ucb_strategy(num_simulations=1000, c=2.0):\n",
       "    conversions = 0\n",
       "    campaign_counts = np.zeros(num_campaigns)\n",
       "    campaign_conversions = np.zeros(num_campaigns)\n",
       "    history = []\n",
       "    \n",
       "    # Initial exploration: try each campaign once\n",
       "    for campaign in range(num_campaigns):\n",
       "        result = get_campaign_result(campaign)\n",
       "        conversions += result\n",
       "        campaign_counts[campaign] += 1\n",
       "        campaign_conversions[campaign] += result\n",
       "        history.append(conversions / (campaign + 1))\n",
       "    \n",
       "    # Main simulation loop\n",
       "    for i in range(num_simulations - num_campaigns):\n",
       "        # Calculate UCB values for each campaign\n",
       "        t = i + num_campaigns\n",
       "        ucb_values = np.zeros(num_campaigns)\n",
       "        for j in range(num_campaigns):\n",
       "            if campaign_counts[j] > 0:\n",
       "                # Estimated conversion rate\n",
       "                exploitation_term = campaign_conversions[j] / campaign_counts[j]\n",
       "                # Exploration bonus\n",
       "                exploration_term = c * np.sqrt(np.log(t) / campaign_counts[j])\n",
       "                ucb_values[j] = exploitation_term + exploration_term\n",
       "            else:\n",
       "                ucb_values[j] = float('inf')  # Ensure we try each arm at least once\n",
       "        \n",
       "        # Select campaign with highest UCB value\n",
       "        campaign = np.argmax(ucb_values)\n",
       "        \n",
       "        # Get result and update counts\n",
       "        result = get_campaign_result(campaign)\n",
       "        conversions += result\n",
       "        campaign_counts[campaign] += 1\n",
       "        campaign_conversions[campaign] += result\n",
       "        history.append(conversions / (t + 1))\n",
       "    \n",
       "    print(f\"UCB Strategy (c={c}) Results:\")\n",
       "    print(f\"Total Conversions: {conversions} out of {num_simulations} ({conversions/num_simulations:.3f})\")\n",
       "    print(f\"Campaign Selection Counts: {campaign_counts}\")\n",
       "    print(f\"Estimated Conversion Rates: {campaign_conversions/campaign_counts}\")\n",
       "    return conversions, campaign_counts, campaign_conversions, history\n",
       "\n",
       "# Run UCB simulation\n",
       "ucb_results = ucb_strategy(10000, c=1.0)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Compare learning curves: UCB vs best epsilon-greedy\n",
       "plt.figure(figsize=(14, 7))\n",
       "\n",
       "plt.plot(ucb_results[3], label='UCB (c=1.0)', linewidth=2)\n",
       "plt.plot(results[0.1][3], label='ε-Greedy (ε=0.1)', linewidth=2)\n",
       "\n",
       "plt.axhline(y=max(true_conversion_rates), color='r', linestyle='--', alpha=0.7, label=f'Best Campaign ({max(true_conversion_rates):.2f})')\n",
       "plt.axhline(y=sum(true_conversion_rates)/len(true_conversion_rates), color='gray', linestyle='--', alpha=0.7, \n",
       "            label=f'Average Campaign ({sum(true_conversion_rates)/len(true_conversion_rates):.2f})')\n",
       "\n",
       "plt.title('Learning Curve: UCB vs Epsilon-Greedy')\n",
       "plt.xlabel('Number of Customers')\n",
       "plt.ylabel('Cumulative Conversion Rate')\n",
       "plt.legend()\n",
       "plt.grid(alpha=0.3)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Practical Implementation Considerations\n",
       "\n",
       "When implementing reinforcement learning for marketing campaign optimization in a real business environment, consider these practical aspects:\n",
       "\n",
       "### 1. Integration with Marketing Platforms\n",
       "\n",
       "- **Web/Email Marketing:** Implement via A/B testing frameworks\n",
       "- **Ad Platforms:** Use platform-specific targeting and optimization features\n",
       "- **CRM Systems:** Update customer segments based on learning results\n",
       "\n",
       "### 2. Evaluation Metrics\n",
       "\n",
       "- **Primary Metrics:** Conversion rate, ROAS, CTR\n",
       "- **Secondary Metrics:** Customer acquisition cost, customer lifetime value\n",
       "- **Learning Metrics:** Exploration rate, confidence intervals, regret\n",
       "\n",
       "### 3. Constraints and Considerations\n",
       "\n",
       "- **Budget Allocation:** Minimum/maximum spend per campaign\n",
       "- **Time Constraints:** Campaign deadlines, seasonal effects\n",
       "- **Audience Targeting:** Different algorithms for different segments\n",
       "- **Brand Guidelines:** Certain campaigns may have non-performance considerations\n",
       "\n",
       "### 4. Implementation Strategy\n",
       "\n",
       "1. **Start Small:** Begin with a limited campaign set\n",
       "2. **Test Against Baseline:** Compare with traditional A/B testing\n",
       "3. **Gradually Increase Scope:** Add more campaigns and features\n",
       "4. **Monitor and Adjust:** Update parameters based on performance\n",
       "\n",
       "### 5. Technical Implementation Options\n",
       "\n",
       "- **Custom Solution:** Implement reinforcement learning algorithms directly\n",
       "- **Marketing Platforms:** Some platforms offer built-in optimization\n",
       "- **Third-Party Tools:** Specialized reinforcement learning platforms"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Learning Challenge\n",
       "\n",
       "Now it's your turn to experiment with multi-armed bandit algorithms for marketing campaign optimization. Try to complete the following tasks:\n",
       "\n",
       "### Exercise 1: Implement a Decay Strategy\n",
       "\n",
       "Create a modified epsilon-greedy algorithm where epsilon decreases over time. Start with epsilon=0.5 and gradually reduce it to 0.05.\n",
       "\n",
       "### Exercise 2: Non-Stationary Environment\n",
       "\n",
       "Modify the simulation to handle a non-stationary environment where the true conversion rates change halfway through the simulation. How does this affect the performance of different algorithms?\n",
       "\n",
       "### Exercise 3: Contextual Bandits\n",
       "\n",
       "Implement a simple contextual bandit where customers belong to one of two segments, and each segment has different conversion rates for the campaigns. How can you modify the algorithms to account for this additional information?"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Exercise 1: Epsilon-greedy with decay\n",
       "def epsilon_greedy_with_decay(start_epsilon=0.5, end_epsilon=0.05, num_simulations=1000):\n",
       "    # YOUR CODE HERE\n",
       "    pass"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Exercise 2: Non-stationary environment\n",
       "def non_stationary_simulation():\n",
       "    # YOUR CODE HERE\n",
       "    pass"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Exercise 3: Contextual bandits\n",
       "def contextual_bandit_simulation():\n",
       "    # YOUR CODE HERE\n",
       "    pass"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "Multi-armed bandit algorithms provide a powerful framework for optimizing marketing campaign selection. By balancing exploration and exploitation, these algorithms can achieve better results than traditional A/B testing approaches, especially for ongoing campaigns.\n",
       "\n",
       "Key takeaways from this notebook:\n",
       "\n",
       "1. **Balancing exploration and exploitation is critical:**\n",
       "   - Too little exploration: May miss better campaigns\n",
       "   - Too much exploration: Wastes resources on inferior campaigns\n",
       "\n",
       "2. **Algorithm selection depends on business context:**\n",
       "   - Epsilon-greedy: Simple and effective for most applications\n",
       "   - UCB: Better theoretical guarantees, more sophisticated exploration\n",
       "   - Contextual bandits: When customer segments respond differently\n",
       "\n",
       "3. **Business implementation considerations:**\n",
       "   - Integration with existing marketing platforms\n",
       "   - Adaptation to changing conditions\n",
       "   - Balancing multiple business objectives\n",
       "\n",
       "By applying these reinforcement learning techniques, marketing teams can continuously optimize their campaigns, leading to higher conversion rates, better resource allocation, and improved ROI."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }