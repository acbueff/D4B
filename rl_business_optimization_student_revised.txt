{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning and Business Optimization\n",
    "\n",
    "This notebook combines comprehensive reinforcement learning concepts and practical business use-case examples for marketing campaign optimization. It covers:\n",
    " - RL fundamentals: MDPs, value functions, Q-learning, and neural network function approximators.\n",
    " - A business simulation: Multi-armed bandit approach to optimize marketing campaigns with various strategies (random, greedy, epsilon-greedy, UCB).\n",
    " - An assignment with automated pass/fail criteria based on simulation performance.\n",
    " - Theoretical reflection questions to help students understand and reflect on the material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optional: Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Reinforcement Learning Theory and Simulation\n",
    "\n",
    "### 1.1: Simple Environment Simulation\n",
    "\n",
    "We'll begin with a basic environment that simulates different rewards based on actions taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simple_environment(action):\n",
    "    \"\"\"Simulate a simple environment with two actions.\n",
    "    Action 0: reward=1 with 50% probability.\n",
    "    Action 1: reward=1 with 80% probability.\n",
    "    \"\"\"\n",
    "    if action == 0:\n",
    "        return 1 if random.random() < 0.5 else 0\n",
    "    elif action == 1:\n",
    "        return 1 if random.random() < 0.8 else 0\n",
    "    return 0\n",
    "\n",
    "\n",
    "def run_agent(steps=1000):\n",
    "    actions = [0, 1]\n",
    "    action_rewards = {0: 0, 1: 0}\n",
    "    action_counts = {0: 0, 1: 0}\n",
    "    for _ in range(steps):\n",
    "        action = random.choice(actions)\n",
    "        reward = simple_environment(action)\n",
    "        action_rewards[action] += reward\n",
    "        action_counts[action] += 1\n",
    "    avg_rewards = {a: action_rewards[a] / action_counts[a] if action_counts[a] != 0 else 0 for a in actions}\n",
    "    return avg_rewards\n",
    "\n",
    "\n",
    "# Test the environment\n",
    "avg_rewards = run_agent(1000)\n",
    "print(\"Average rewards after 1000 steps (Simple Environment):\", avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Simple MDP Example\n",
    "\n",
    "Now we'll implement a simple Markov Decision Process (MDP) with states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleMDP:\n",
    "    \"\"\"Simple MDP with states 0 to 4, where action 1 moves right and -1 moves left.\n",
    "    Reward of 1 is given when state reaches 4.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = list(range(5))\n",
    "        self.current_state = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.current_state + action\n",
    "        next_state = max(0, min(4, next_state))\n",
    "        reward = 1 if next_state == 4 else 0\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "# Test the MDP\n",
    "env = SimpleMDP()\n",
    "state = env.reset()\n",
    "print(\"Initial state (MDP):\", state)\n",
    "next_state, reward = env.step(1)\n",
    "print(\"After taking action 1, next state:\", next_state, \"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Inverse Reinforcement Learning - Generate Expert Trajectory\n",
    "\n",
    "Expert trajectories can be used for inverse reinforcement learning where we try to infer the reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_expert_trajectory(env, expert_policy, steps=10):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    for _ in range(steps):\n",
    "        action = expert_policy(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        trajectory.append((state, action, next_state, reward))\n",
    "        state = next_state\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def expert_policy(state):\n",
    "    return 1  # Always move right\n",
    "\n",
    "\n",
    "# Generate and display an expert trajectory\n",
    "env = SimpleMDP()\n",
    "trajectory = generate_expert_trajectory(env, expert_policy, steps=10)\n",
    "print(\"Expert Trajectory:\")\n",
    "for t in trajectory:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Policy Evaluation for a 3-State MDP (Value Function)\n",
    "\n",
    "Now let's implement policy evaluation to compute the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def policy_evaluation(P, V, gamma, theta=1e-4, states=[0,1,2]):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s not in P or not P[s]:\n",
    "                continue\n",
    "            v = V[s]\n",
    "            V[s] = sum(prob * (reward + gamma * V[next_state]) for next_state, prob, reward in P[s])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "\n",
    "# Test policy evaluation\n",
    "states = [0, 1, 2]\n",
    "P = {\n",
    "    0: [(1, 1.0, 0)],\n",
    "    1: [(0, 0.5, 0), (2, 0.5, 1)],\n",
    "    2: []  # terminal state\n",
    "}\n",
    "gamma = 0.9\n",
    "V = {s: 0 for s in states}\n",
    "V = policy_evaluation(P, V, gamma)\n",
    "print(\"Computed Value Function:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Q-Learning Simulation Example\n",
    "\n",
    "Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleQLearningEnv:\n",
    "    def __init__(self):\n",
    "        self.states = list(range(5))\n",
    "        self.current_state = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.current_state + action\n",
    "        next_state = max(0, min(4, next_state))\n",
    "        reward = 1 if next_state == 4 else 0\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "def q_learning(episodes=1000, epsilon=0.2):\n",
    "    env = SimpleQLearningEnv()\n",
    "    num_states = 5\n",
    "    actions = [-1, 1]  # left and right moves\n",
    "    Q_table = np.zeros((num_states, len(actions)))\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action_index = np.random.choice(len(actions))\n",
    "            else:\n",
    "                action_index = np.argmax(Q_table[state])\n",
    "            action = actions[action_index]\n",
    "            next_state, reward = env.step(action)\n",
    "            best_next_action = np.argmax(Q_table[next_state])\n",
    "            Q_table[state, action_index] += alpha * (reward + gamma * Q_table[next_state, best_next_action] - Q_table[state, action_index])\n",
    "            state = next_state\n",
    "            if state == 4:\n",
    "                done = True\n",
    "    return Q_table\n",
    "\n",
    "\n",
    "# Run Q-learning\n",
    "Q_table = q_learning(episodes=1000, epsilon=0.2)\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6: Neural Network Function Approximator for Q-values using PyTorch\n",
    "\n",
    "For complex state spaces, neural networks can approximate Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    class QNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super(QNetwork, self).__init__()\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(input_dim, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, output_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.fc(x)\n",
    "\n",
    "    input_dim = 1\n",
    "    output_dim = 2\n",
    "    q_net = QNetwork(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    states = torch.tensor([[0.0], [1.0], [2.0], [3.0]], dtype=torch.float32)\n",
    "    target_Q = torch.tensor([[0.5, 0.7],\n",
    "                              [0.6, 0.8],\n",
    "                              [0.7, 0.9],\n",
    "                              [0.8, 1.0]], dtype=torch.float32)\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = q_net(states)\n",
    "        loss = loss_fn(outputs, target_Q)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Trained Q-network outputs:\")\n",
    "    print(q_net(states).detach().numpy())\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Skipping Q-network demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Business Context - Marketing Campaign Optimization Using Multi-Armed Bandit\n",
    "\n",
    "### 2.1: Setup for Marketing Campaign Simulation\n",
    "\n",
    "We'll simulate marketing campaigns with different conversion rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "true_conversion_rates = [0.2, 0.5, 0.7]\n",
    "num_campaigns = len(true_conversion_rates)\n",
    "\n",
    "def get_campaign_result(campaign_index):\n",
    "    \"\"\"Return 1 (conversion) or 0 (no conversion) based on the campaign's true conversion rate.\"\"\"\n",
    "    if random.random() < true_conversion_rates[campaign_index]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Test the campaign simulation\n",
    "test_results = [get_campaign_result(0) for _ in range(1000)]\n",
    "print(f\"Campaign 1 test conversion rate: {sum(test_results)/len(test_results):.3f} (expected ~{true_conversion_rates[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Multi-Armed Bandit Simulation using Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simulate_bandit(epsilon, steps=1000):\n",
    "    \"\"\"\n",
    "    Simulate a multi-armed bandit using epsilon-greedy strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    epsilon -- exploration rate\n",
    "    steps -- number of steps to simulate\n",
    "    \n",
    "    Returns:\n",
    "    average_reward -- the average reward obtained\n",
    "    Q_estimates -- estimated values for each arm\n",
    "    counts -- number of times each arm was pulled\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    true_probs = true_conversion_rates\n",
    "    num_arms = len(true_probs)\n",
    "    \n",
    "    # TODO: Implement epsilon-greedy bandit algorithm\n",
    "    # Initialize estimated values for each arm and counts\n",
    "    # For each step:\n",
    "    #   1. Choose an arm based on epsilon-greedy policy\n",
    "    #   2. Get reward based on the chosen arm\n",
    "    #   3. Update the estimated value of the chosen arm\n",
    "    #   4. Track rewards and counts\n",
    "    # Return the average reward, estimated values, and counts\n",
    "    \n",
    "    return 0, np.zeros(num_arms), np.zeros(num_arms)  # Replace with your implementation\n",
    "\n",
    "\n",
    "# Test your bandit implementation\n",
    "# avg_reward, Q_estimates, counts = simulate_bandit(epsilon=0.1, steps=1000)\n",
    "# print(\"Epsilon=0.1 Bandit Simulation:\")\n",
    "# print(\"Average Reward:\", avg_reward)\n",
    "# print(\"Estimated Q-values:\", Q_estimates)\n",
    "# print(\"Counts:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Other Campaign Selection Strategies\n",
    "\n",
    "Let's implement and compare different strategies for selecting marketing campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def random_strategy(num_simulations=1000):\n",
    "    \"\"\"\n",
    "    Implement random selection strategy for marketing campaign selection.\n",
    "    \n",
    "    Parameters:\n",
    "    num_simulations -- number of customers to simulate\n",
    "    \n",
    "    Returns:\n",
    "    conversions -- total number of conversions\n",
    "    campaign_counts -- number of times each campaign was selected\n",
    "    \"\"\"\n",
    "    # TODO: Implement random strategy\n",
    "    # For each simulation:\n",
    "    #   1. Choose a random campaign\n",
    "    #   2. Get the result of the campaign\n",
    "    #   3. Track conversions and campaign counts\n",
    "    # Return the total conversions and campaign counts\n",
    "    \n",
    "    return 0, np.zeros(num_campaigns)  # Replace with your implementation\n",
    "\n",
    "\n",
    "def greedy_strategy(num_simulations=1000):\n",
    "    \"\"\"\n",
    "    Implement greedy selection strategy for marketing campaign selection.\n",
    "    \n",
    "    Parameters:\n",
    "    num_simulations -- number of customers to simulate\n",
    "    \n",
    "    Returns:\n",
    "    conversions -- total number of conversions\n",
    "    campaign_counts -- number of times each campaign was selected\n",
    "    campaign_conversions -- number of conversions for each campaign\n",
    "    \"\"\"\n",
    "    # TODO: Implement greedy strategy\n",
    "    # First, try each campaign once for initial exploration\n",
    "    # Then for the remaining simulations:\n",
    "    #   1. Choose the campaign with the highest conversion rate so far\n",
    "    #   2. Get the result of the campaign\n",
    "    #   3. Track conversions, counts, and campaign conversions\n",
    "    # Return the results\n",
    "    \n",
    "    return 0, np.zeros(num_campaigns), np.zeros(num_campaigns)  # Replace with your implementation\n",
    "\n",
    "\n",
    "def epsilon_greedy_strategy(epsilon, num_simulations=1000):\n",
    "    \"\"\"\n",
    "    Implement epsilon-greedy selection strategy for marketing campaign selection.\n",
    "    \n",
    "    Parameters:\n",
    "    epsilon -- exploration rate\n",
    "    num_simulations -- number of customers to simulate\n",
    "    \n",
    "    Returns:\n",
    "    conversions -- total number of conversions\n",
    "    campaign_counts -- number of times each campaign was selected\n",
    "    campaign_conversions -- number of conversions for each campaign\n",
    "    history -- list of cumulative conversion rates for each step\n",
    "    \"\"\"\n",
    "    # TODO: Implement epsilon-greedy strategy\n",
    "    # First, try each campaign once for initial exploration\n",
    "    # Then for the remaining simulations:\n",
    "    #   1. With probability epsilon, choose a random campaign\n",
    "    #   2. Otherwise, choose the campaign with the highest conversion rate so far\n",
    "    #   3. Get the result of the campaign\n",
    "    #   4. Track conversions, counts, campaign conversions, and history\n",
    "    # Return the results\n",
    "    \n",
    "    return 0, np.zeros(num_campaigns), np.zeros(num_campaigns), []  # Replace with your implementation\n",
    "\n",
    "\n",
    "def ucb_strategy(num_simulations=1000, c=2.0):\n",
    "    \"\"\"\n",
    "    Implement UCB selection strategy for marketing campaign selection.\n",
    "    \n",
    "    Parameters:\n",
    "    num_simulations -- number of customers to simulate\n",
    "    c -- exploration parameter for UCB\n",
    "    \n",
    "    Returns:\n",
    "    conversions -- total number of conversions\n",
    "    campaign_counts -- number of times each campaign was selected\n",
    "    campaign_conversions -- number of conversions for each campaign\n",
    "    history -- list of cumulative conversion rates for each step\n",
    "    \"\"\"\n",
    "    # TODO: Implement UCB strategy\n",
    "    # First, try each campaign once for initial exploration\n",
    "    # Then for the remaining simulations:\n",
    "    #   1. Calculate UCB values for each campaign\n",
    "    #   2. Choose the campaign with the highest UCB value\n",
    "    #   3. Get the result of the campaign\n",
    "    #   4. Track conversions, counts, campaign conversions, and history\n",
    "    # Return the results\n",
    "    \n",
    "    return 0, np.zeros(num_campaigns), np.zeros(num_campaigns), []  # Replace with your implementation\n",
    "\n",
    "\n",
    "# Test your strategy implementations\n",
    "# print(\"\\nTesting Random Strategy:\")\n",
    "# random_results = random_strategy(1000)\n",
    "# print(\"\\nTesting Greedy Strategy:\")\n",
    "# greedy_strategy(1000)\n",
    "# for eps in [0.0, 0.1, 0.3, 0.5]:\n",
    "#     print(f\"\\nTesting Epsilon-Greedy Strategy with epsilon={eps}:\")\n",
    "#     epsilon_greedy_strategy(eps, 1000)\n",
    "# print(\"\\nTesting UCB Strategy:\")\n",
    "# ucb_strategy(1000, c=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Visualization of Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_strategy_comparison(random_results, eps_results_dict, true_rates=true_conversion_rates):\n",
    "    \"\"\"\n",
    "    Plot a comparison of different strategies.\n",
    "    \n",
    "    Parameters:\n",
    "    random_results -- results from the random strategy\n",
    "    eps_results_dict -- dictionary of results from epsilon-greedy strategies with different epsilon values\n",
    "    true_rates -- true conversion rates of the campaigns\n",
    "    \"\"\"\n",
    "    # TODO: Implement visualization\n",
    "    # Create a bar chart comparing the performance of different strategies\n",
    "    # Add reference lines for the best campaign and average campaign performance\n",
    "    # Add labels and title to the chart\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_learning_curves(eps_history, ucb_history, true_rates=true_conversion_rates):\n",
    "    \"\"\"\n",
    "    Plot learning curves for different strategies.\n",
    "    \n",
    "    Parameters:\n",
    "    eps_history -- dictionary of learning curves for epsilon-greedy strategies with different epsilon values\n",
    "    ucb_history -- learning curve for UCB strategy\n",
    "    true_rates -- true conversion rates of the campaigns\n",
    "    \"\"\"\n",
    "    # TODO: Implement visualization\n",
    "    # Create line plots for each epsilon-greedy strategy and UCB\n",
    "    # Add reference lines for the best campaign and average campaign performance\n",
    "    # Add labels, title, and legend to the chart\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run experiments for visualization\n",
    "# eps_values = [0.0, 0.1, 0.3, 0.5]\n",
    "# eps_history = {}\n",
    "# eps_results = {}\n",
    "# for eps in eps_values:\n",
    "#     conv, counts, convs, history = epsilon_greedy_strategy(eps, 10000)\n",
    "#     eps_history[eps] = history\n",
    "#     eps_results[eps] = (conv, counts, convs)\n",
    "\n",
    "# random_results = random_strategy(10000)\n",
    "# ucb_conv, ucb_counts, ucb_convs, ucb_history = ucb_strategy(10000, c=1.0)\n",
    "\n",
    "# Plot the results\n",
    "# plot_strategy_comparison(random_results, eps_results)\n",
    "# plot_learning_curves(eps_history, ucb_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Assignment and Automated Grading\n",
    "\n",
    "This section contains an automated grading system that evaluates your implementation of the epsilon-greedy strategy for the marketing campaign optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def assignment_simulation(epsilon=0.1, steps=1000, threshold=0.65):\n",
    "    \"\"\"\n",
    "    Run a simulation to test your epsilon-greedy implementation.\n",
    "    \"\"\"\n",
    "    avg_reward, Q_estimates, counts = simulate_bandit(epsilon, steps)\n",
    "    print(\"Assignment Simulation:\")\n",
    "    print(\"Average Reward: {:.3f}\".format(avg_reward))\n",
    "    print(\"Estimated Conversion Rates:\", Q_estimates)\n",
    "    print(\"Counts:\", counts)\n",
    "    if avg_reward >= threshold:\n",
    "        print(\"PASS: Your algorithm achieved an average reward of {:.3f} (>= {:.2f}).\".format(avg_reward, threshold))\n",
    "    else:\n",
    "        print(\"FAIL: Your algorithm achieved an average reward of {:.3f} (< {:.2f}). Please review your implementation.\".format(avg_reward, threshold))\n",
    "\n",
    "# Run the assignment simulation\n",
    "# assignment_simulation(epsilon=0.1, steps=1000, threshold=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Theoretical Reflection and Discussion Questions\n",
    "\n",
    "1. What is the main idea behind trial-and-error learning in RL?\n",
    "2. How does the reward function guide an agent's behavior in RL?\n",
    "3. In the context of marketing campaigns, how does the balance between exploration and exploitation affect performance?\n",
    "4. How do different strategies (random, greedy, epsilon-greedy, UCB) compare in balancing exploration and exploitation?\n",
    "5. How can a value function or Q-function be used to make long-term business decisions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 