{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Yelp Business Insights Project - Student Version\n",
       "\n",
       "In this project, you will apply AI techniques including Natural Language Processing (NLP), deep learning, and recommendation systems to extract business insights from the Yelp Open Dataset. This project simulates real-world business applications similar to those discussed throughout the course.\n",
       "\n",
       "## Business Context\n",
       "\n",
       "Imagine you're working as a business intelligence consultant for a restaurant group that operates multiple establishments across different cities. Your client wants to leverage AI to:\n",
       "\n",
       "1. Understand customer sentiment to improve service quality (similar to how Coca-Cola analyzes social media)\n",
       "2. Identify key topics of concern in negative reviews (similar to JPMorgan's COIN document analysis)\n",
       "3. Build a recommendation system to increase customer engagement (similar to Netflix's recommendation engine)\n",
       "4. Predict which locations might need intervention based on recent review trends\n",
       "\n",
       "Each section of this project will address one of these business needs using AI techniques.\n",
       "\n",
       "## Project Outline\n",
       "\n",
       "1. **Data Exploration & Preparation**\n",
       "2. **Sentiment Analysis** - Understand customer opinions\n",
       "3. **Topic Modeling** - Extract key themes from reviews\n",
       "4. **Recommendation System** - Suggest similar businesses\n",
       "5. **Business Insights Dashboard** - Visualize key findings\n",
       "6. **Comparing Custom Solutions vs. AI Tools** - Make implementation decisions\n",
       "7. **Business Implementation & Ethical Considerations**\n",
       "\n",
       "Let's begin by setting up our environment and exploring the data."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Data Setup & Exploration\n",
       "\n",
       "First, let's import the necessary libraries and load our data. For this project, we'll use a subset of the Yelp Open Dataset."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Import necessary libraries\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "import re\n",
       "import nltk\n",
       "from nltk.corpus import stopwords\n",
       "from nltk.stem import WordNetLemmatizer\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
       "from sklearn.naive_bayes import MultinomialNB\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "from torch.utils.data import Dataset, DataLoader\n",
       "import warnings\n",
       "\n",
       "# Suppress warnings for cleaner output\n",
       "warnings.filterwarnings('ignore')\n",
       "\n",
       "# Download NLTK resources\n",
       "nltk.download('stopwords')\n",
       "nltk.download('wordnet')\n",
       "nltk.download('punkt')\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "np.random.seed(42)\n",
       "torch.manual_seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Loading the Yelp Dataset\n",
       "\n",
       "We'll work with two main datasets:\n",
       "1. **Reviews** - Contains customer feedback text and star ratings\n",
       "2. **Businesses** - Contains information about each establishment\n",
       "\n",
       "For this project, we'll use a sample of these datasets to make processing faster."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load the sample Yelp data\n",
       "# Note: In a real project, you would download the full Yelp dataset\n",
       "# For this exercise, we'll assume you have access to a sample file\n",
       "\n",
       "# If you don't have the sample files, you can create them with:\n",
       "# !wget https://www.example.com/yelp_sample_reviews.csv -O yelp_sample_reviews.csv\n",
       "# !wget https://www.example.com/yelp_sample_businesses.csv -O yelp_sample_businesses.csv\n",
       "\n",
       "# YOUR CODE HERE: Load the sample data\n",
       "# If the sample files don't exist, you can use this code to generate synthetic data\n",
       "import random\n",
       "\n",
       "# Create synthetic review data\n",
       "def generate_sample_reviews(n=5000):\n",
       "    business_ids = [f'b_{i}' for i in range(100)]\n",
       "    user_ids = [f'u_{i}' for i in range(500)]\n",
       "    \n",
       "    positive_phrases = [\n",
       "        \"Great food and service\", \"Amazing experience\", \"The staff was friendly\",\n",
       "        \"Best restaurant in town\", \"Excellent menu options\", \"Would definitely recommend\",\n",
       "        \"Perfect atmosphere\", \"Outstanding quality\", \"Loved everything about this place\"\n",
       "    ]\n",
       "    \n",
       "    negative_phrases = [\n",
       "        \"Poor service\", \"Disappointing experience\", \"The food was cold\",\n",
       "        \"Would not recommend\", \"Overpriced for the quality\", \"Rude staff\",\n",
       "        \"Terrible atmosphere\", \"Too noisy\", \"Long wait times\"\n",
       "    ]\n",
       "    \n",
       "    neutral_phrases = [\n",
       "        \"Average experience\", \"The food was okay\", \"Decent service\",\n",
       "        \"Nothing special\", \"Might return\", \"Somewhat satisfied\"\n",
       "    ]\n",
       "    \n",
       "    reviews = []\n",
       "    for _ in range(n):\n",
       "        stars = random.randint(1, 5)\n",
       "        business_id = random.choice(business_ids)\n",
       "        user_id = random.choice(user_ids)\n",
       "        \n",
       "        # Generate review text based on rating\n",
       "        if stars >= 4:\n",
       "            text = \" \".join(random.sample(positive_phrases, k=random.randint(1, 3)))\n",
       "            text += \" \" + random.choice(neutral_phrases) if random.random() > 0.7 else \"\"\n",
       "        elif stars <= 2:\n",
       "            text = \" \".join(random.sample(negative_phrases, k=random.randint(1, 3)))\n",
       "            text += \" \" + random.choice(neutral_phrases) if random.random() > 0.7 else \"\"\n",
       "        else:  # 3 stars\n",
       "            text = \" \".join(random.sample(neutral_phrases, k=random.randint(1, 2)))\n",
       "            text += \" \" + random.choice(positive_phrases) if random.random() > 0.5 else \"\"\n",
       "            text += \" \" + random.choice(negative_phrases) if random.random() > 0.5 else \"\"\n",
       "        \n",
       "        reviews.append({\n",
       "            'review_id': f'r_{_}',\n",
       "            'user_id': user_id,\n",
       "            'business_id': business_id,\n",
       "            'stars': stars,\n",
       "            'text': text,\n",
       "            'date': f'2023-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}'\n",
       "        })\n",
       "    \n",
       "    return pd.DataFrame(reviews)\n",
       "\n",
       "def generate_sample_businesses(n=100):\n",
       "    categories = ['Restaurants', 'Fast Food', 'Italian', 'Mexican', 'Chinese', 'Cafes', 'Bars', 'Pizza', 'Breakfast', 'Burgers']\n",
       "    cities = ['Phoenix', 'Las Vegas', 'Toronto', 'Charlotte', 'Pittsburgh', 'Montreal', 'Edinburgh', 'Cleveland']\n",
       "    \n",
       "    businesses = []\n",
       "    for i in range(n):\n",
       "        business_id = f'b_{i}'\n",
       "        name = f'Business {i}'\n",
       "        city = random.choice(cities)\n",
       "        stars = round(random.uniform(1.0, 5.0), 1)\n",
       "        review_count = random.randint(10, 500)\n",
       "        \n",
       "        # Assign 1-3 categories to each business\n",
       "        business_categories = ', '.join(random.sample(categories, k=random.randint(1, 3)))\n",
       "        \n",
       "        businesses.append({\n",
       "            'business_id': business_id,\n",
       "            'name': name,\n",
       "            'city': city,\n",
       "            'stars': stars,\n",
       "            'review_count': review_count,\n",
       "            'categories': business_categories\n",
       "        })\n",
       "    \n",
       "    return pd.DataFrame(businesses)\n",
       "\n",
       "# Generate sample data\n",
       "try:\n",
       "    reviews_df = pd.read_csv('yelp_sample_reviews.csv')\n",
       "    businesses_df = pd.read_csv('yelp_sample_businesses.csv')\n",
       "    print(\"Loaded existing sample data files.\")\n",
       "except FileNotFoundError:\n",
       "    print(\"Creating synthetic sample data...\")\n",
       "    reviews_df = generate_sample_reviews()\n",
       "    businesses_df = generate_sample_businesses()\n",
       "    \n",
       "    # Save the synthetic data\n",
       "    reviews_df.to_csv('yelp_sample_reviews.csv', index=False)\n",
       "    businesses_df.to_csv('yelp_sample_businesses.csv', index=False)\n",
       "    print(\"Created and saved synthetic sample data.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Exploratory Data Analysis\n",
       "\n",
       "Let's examine our data to understand its structure and contents. This step is crucial for any data science project because it helps us identify patterns, outliers, and potential issues before building models."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Examine the reviews dataset\n",
       "print(\"Reviews Dataset Shape:\", reviews_df.shape)\n",
       "print(\"\\nReviews Dataset Sample:\")\n",
       "reviews_df.head()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Examine the businesses dataset\n",
       "print(\"Businesses Dataset Shape:\", businesses_df.shape)\n",
       "print(\"\\nBusinesses Dataset Sample:\")\n",
       "businesses_df.head()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Basic statistics for star ratings\n",
       "print(\"Review Star Rating Distribution:\")\n",
       "print(reviews_df['stars'].value_counts().sort_index())\n",
       "\n",
       "# Visualize the star rating distribution\n",
       "plt.figure(figsize=(10, 6))\n",
       "sns.countplot(x='stars', data=reviews_df, palette='viridis')\n",
       "plt.title('Distribution of Star Ratings', fontsize=15)\n",
       "plt.xlabel('Star Rating', fontsize=12)\n",
       "plt.ylabel('Count', fontsize=12)\n",
       "plt.xticks(fontsize=10)\n",
       "plt.yticks(fontsize=10)\n",
       "plt.show()\n",
       "\n",
       "# Business insights: Calculate the average rating by city\n",
       "city_ratings = businesses_df.groupby('city')['stars'].mean().reset_index().sort_values('stars', ascending=False)\n",
       "\n",
       "plt.figure(figsize=(12, 6))\n",
       "sns.barplot(x='city', y='stars', data=city_ratings, palette='viridis')\n",
       "plt.title('Average Business Rating by City', fontsize=15)\n",
       "plt.xlabel('City', fontsize=12)\n",
       "plt.ylabel('Average Star Rating', fontsize=12)\n",
       "plt.xticks(rotation=45, fontsize=10)\n",
       "plt.yticks(fontsize=10)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Business Categories Analysis\n",
       "\n",
       "Let's examine the business categories to understand what types of establishments we're working with."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Extract all categories and count their frequency\n",
       "all_categories = []\n",
       "for categories in businesses_df['categories'].dropna():\n",
       "    all_categories.extend([cat.strip() for cat in categories.split(',')])\n",
       "\n",
       "category_counts = pd.Series(all_categories).value_counts().reset_index()\n",
       "category_counts.columns = ['category', 'count']\n",
       "\n",
       "# Plot the top 15 categories\n",
       "plt.figure(figsize=(12, 6))\n",
       "sns.barplot(x='count', y='category', data=category_counts.head(15), palette='viridis')\n",
       "plt.title('Top 15 Business Categories', fontsize=15)\n",
       "plt.xlabel('Count', fontsize=12)\n",
       "plt.ylabel('Category', fontsize=12)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Data Preprocessing\n",
       "\n",
       "Before we can apply NLP techniques, we need to clean and preprocess the review text. This includes:\n",
       "- Removing special characters and numbers\n",
       "- Converting text to lowercase\n",
       "- Removing stopwords (common words like \"the\", \"and\", etc.)\n",
       "- Lemmatizing words (reducing words to their base form)\n",
       "\n",
       "These steps help improve the performance of our text analysis models by removing noise and standardizing the text."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Text preprocessing function\n",
       "def preprocess_text(text):\n",
       "    if not isinstance(text, str):\n",
       "        return \"\"\n",
       "    \n",
       "    # Convert to lowercase and remove special characters\n",
       "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
       "    \n",
       "    # Tokenize the text\n",
       "    tokens = nltk.word_tokenize(text)\n",
       "    \n",
       "    # Remove stopwords and lemmatize\n",
       "    stop_words = set(stopwords.words('english'))\n",
       "    lemmatizer = WordNetLemmatizer()\n",
       "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
       "    \n",
       "    return ' '.join(cleaned_tokens)\n",
       "\n",
       "# Apply preprocessing to review text\n",
       "print(\"Preprocessing review text...\")\n",
       "reviews_df['processed_text'] = reviews_df['text'].apply(preprocess_text)\n",
       "print(\"Done!\")\n",
       "\n",
       "# Display a sample original and processed review\n",
       "sample_idx = np.random.randint(0, len(reviews_df))\n",
       "print(f\"\\nOriginal Review: {reviews_df.iloc[sample_idx]['text']}\")\n",
       "print(f\"Processed Review: {reviews_df.iloc[sample_idx]['processed_text']}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Sentiment Analysis\n",
       "\n",
       "### Business Context\n",
       "\n",
       "Just as Coca-Cola monitors social media sentiment to gauge public opinion about their brand, our restaurant group needs to automatically analyze thousands of customer reviews to understand sentiment. This allows them to quickly identify satisfaction trends and take action before reputation issues escalate.\n",
       "\n",
       "In this section, we'll:\n",
       "1. Create a sentiment classification model based on review star ratings\n",
       "2. Evaluate how well our model predicts sentiment from review text\n",
       "3. Apply the model to extract insights from unstructured text\n",
       "\n",
       "First, we'll convert star ratings into sentiment categories:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Convert star ratings to sentiment categories\n",
       "def star_to_sentiment(stars):\n",
       "    if stars >= 4:  # 4-5 stars\n",
       "        return 'positive'\n",
       "    elif stars <= 2:  # 1-2 stars\n",
       "        return 'negative'\n",
       "    else:  # 3 stars\n",
       "        return 'neutral'\n",
       "\n",
       "reviews_df['sentiment'] = reviews_df['stars'].apply(star_to_sentiment)\n",
       "\n",
       "# Display sentiment distribution\n",
       "sentiment_counts = reviews_df['sentiment'].value_counts()\n",
       "print(\"Sentiment Distribution:\")\n",
       "print(sentiment_counts)\n",
       "\n",
       "# Visualize the sentiment distribution\n",
       "plt.figure(figsize=(10, 6))\n",
       "sns.countplot(x='sentiment', data=reviews_df, palette={'positive': 'green', 'neutral': 'gray', 'negative': 'red'})\n",
       "plt.title('Distribution of Review Sentiment', fontsize=15)\n",
       "plt.xlabel('Sentiment', fontsize=12)\n",
       "plt.ylabel('Count', fontsize=12)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Building a Sentiment Classifier\n",
       "\n",
       "Now, we'll build a machine learning model to predict sentiment from review text. We'll use a simple approach first with a traditional machine learning model, then compare with a neural network approach."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare data for sentiment analysis\n",
       "# We'll focus on positive and negative reviews for simplicity (binary classification)\n",
       "binary_sentiment_df = reviews_df[reviews_df['sentiment'] != 'neutral']\n",
       "X = binary_sentiment_df['processed_text']\n",
       "y = (binary_sentiment_df['sentiment'] == 'positive').astype(int)  # 1 for positive, 0 for negative\n",
       "\n",
       "# Split the data into training and testing sets\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "print(f\"Training data size: {len(X_train)}\")\n",
       "print(f\"Testing data size: {len(X_test)}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Vectorize the text using TF-IDF\n",
       "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
       "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
       "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
       "\n",
       "# Train a Logistic Regression model\n",
       "lr_model = LogisticRegression(max_iter=1000)\n",
       "lr_model.fit(X_train_tfidf, y_train)\n",
       "\n",
       "# Make predictions\n",
       "y_pred = lr_model.predict(X_test_tfidf)\n",
       "\n",
       "# Evaluate the model\n",
       "accuracy = accuracy_score(y_test, y_pred)\n",
       "print(f\"Logistic Regression Sentiment Model Accuracy: {accuracy:.4f}\")\n",
       "print(\"\\nClassification Report:\")\n",
       "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
       "\n",
       "# Visualize the confusion matrix\n",
       "cm = confusion_matrix(y_test, y_pred)\n",
       "plt.figure(figsize=(8, 6))\n",
       "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
       "            xticklabels=['Negative', 'Positive'], \n",
       "            yticklabels=['Negative', 'Positive'])\n",
       "plt.title('Confusion Matrix')\n",
       "plt.ylabel('True Label')\n",
       "plt.xlabel('Predicted Label')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Feature Importance Analysis\n",
       "\n",
       "Let's examine which words are most predictive of positive and negative sentiment. This analysis can help business owners understand which aspects of their service are most likely to generate positive or negative reactions."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Get feature names from the TF-IDF vectorizer\n",
       "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
       "\n",
       "# Get the coefficients from the Logistic Regression model\n",
       "coefficients = lr_model.coef_[0]\n",
       "\n",
       "# Create a DataFrame to store feature importance\n",
       "feature_importance = pd.DataFrame({\n",
       "    'Feature': feature_names,\n",
       "    'Importance': coefficients\n",
       "})\n",
       "\n",
       "# Sort by absolute importance\n",
       "feature_importance['Abs_Importance'] = np.abs(feature_importance['Importance'])\n",
       "feature_importance = feature_importance.sort_values('Abs_Importance', ascending=False)\n",
       "\n",
       "# Display the top positive words (most predictive of positive sentiment)\n",
       "top_positive = feature_importance[feature_importance['Importance'] > 0].head(15)\n",
       "print(\"Top 15 Words Predictive of Positive Sentiment:\")\n",
       "display(top_positive[['Feature', 'Importance']])\n",
       "\n",
       "# Display the top negative words (most predictive of negative sentiment)\n",
       "top_negative = feature_importance[feature_importance['Importance'] < 0].head(15)\n",
       "print(\"\\nTop 15 Words Predictive of Negative Sentiment:\")\n",
       "display(top_negative[['Feature', 'Importance']])\n",
       "\n",
       "# Visualize the top positive and negative words\n",
       "plt.figure(figsize=(15, 8))\n",
       "\n",
       "plt.subplot(1, 2, 1)\n",
       "sns.barplot(x='Importance', y='Feature', data=top_positive, palette='Greens_r')\n",
       "plt.title('Top Words Predicting Positive Sentiment', fontsize=14)\n",
       "plt.xlabel('Importance', fontsize=12)\n",
       "\n",
       "plt.subplot(1, 2, 2)\n",
       "sns.barplot(x='Importance', y='Feature', data=top_negative, palette='Reds_r')\n",
       "plt.title('Top Words Predicting Negative Sentiment', fontsize=14)\n",
       "plt.xlabel('Importance', fontsize=12)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Advanced Sentiment Analysis with PyTorch (Deep Learning Approach)\n",
       "\n",
       "Now let's implement a deep learning approach using a simple neural network. This is similar to how companies like Amazon analyze their product reviews at scale."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# YOUR CODE HERE: Implement a simple deep learning model for sentiment analysis\n",
       "# Convert vectorized data to PyTorch tensors\n",
       "X_train_tensor = torch.FloatTensor(X_train_tfidf.toarray())\n",
       "X_test_tensor = torch.FloatTensor(X_test_tfidf.toarray())\n",
       "y_train_tensor = torch.FloatTensor(y_train.values)\n",
       "y_test_tensor = torch.FloatTensor(y_test.values)\n",
       "\n",
       "# Define a simple neural network model\n",
       "class SentimentNN(nn.Module):\n",
       "    def __init__(self, input_dim):\n",
       "        super(SentimentNN, self).__init__()\n",
       "        self.fc1 = nn.Linear(input_dim, 128)\n",
       "        self.fc2 = nn.Linear(128, 64)\n",
       "        self.fc3 = nn.Linear(64, 1)\n",
       "        self.relu = nn.ReLU()\n",
       "        self.dropout = nn.Dropout(0.3)\n",
       "        self.sigmoid = nn.Sigmoid()\n",
       "        \n",
       "    def forward(self, x):\n",
       "        x = self.relu(self.fc1(x))\n",
       "        x = self.dropout(x)\n",
       "        x = self.relu(self.fc2(x))\n",
       "        x = self.dropout(x)\n",
       "        x = self.sigmoid(self.fc3(x))\n",
       "        return x\n",
       "\n",
       "# Initialize the model\n",
       "input_dim = X_train_tfidf.shape[1]\n",
       "model = SentimentNN(input_dim)\n",
       "\n",
       "# Define loss function and optimizer\n",
       "criterion = nn.BCELoss()\n",
       "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
       "\n",
       "# Train the model\n",
       "num_epochs = 10\n",
       "batch_size = 64\n",
       "\n",
       "# Create DataLoader\n",
       "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor.view(-1, 1))\n",
       "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
       "\n",
       "# Training loop\n",
       "loss_history = []\n",
       "\n",
       "for epoch in range(num_epochs):\n",
       "    model.train()\n",
       "    running_loss = 0.0\n",
       "    \n",
       "    for inputs, labels in train_loader:\n",
       "        # Zero the parameter gradients\n",