{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning and Business Optimization\n",
    "\n",
    "This notebook combines comprehensive reinforcement learning concepts and practical business use-case examples for marketing campaign optimization. It covers:\n",
    " - RL fundamentals: MDPs, value functions, Q-learning, and neural network function approximators.\n",
    " - A business simulation: Multi-armed bandit approach to optimize marketing campaigns with various strategies (random, greedy, epsilon-greedy, UCB).\n",
    " - An assignment with automated pass/fail criteria based on simulation performance.\n",
    " - Theoretical reflection questions to help students understand and reflect on the material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optional: Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Reinforcement Learning Theory and Simulation\n",
    "\n",
    "### 1.1: Simple Environment Simulation\n",
    "\n",
    "We'll begin with a basic environment that simulates different rewards based on actions taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simple_environment(action):\n",
    "    \"\"\"Simulate a simple environment with two actions.\n",
    "    Action 0: reward=1 with 50% probability.\n",
    "    Action 1: reward=1 with 80% probability.\n",
    "    \"\"\"\n",
    "    if action == 0:\n",
    "        return 1 if random.random() < 0.5 else 0\n",
    "    elif action == 1:\n",
    "        return 1 if random.random() < 0.8 else 0\n",
    "    return 0\n",
    "\n",
    "\n",
    "def run_agent(steps=1000):\n",
    "    actions = [0, 1]\n",
    "    action_rewards = {0: 0, 1: 0}\n",
    "    action_counts = {0: 0, 1: 0}\n",
    "    for _ in range(steps):\n",
    "        action = random.choice(actions)\n",
    "        reward = simple_environment(action)\n",
    "        action_rewards[action] += reward\n",
    "        action_counts[action] += 1\n",
    "    avg_rewards = {a: action_rewards[a] / action_counts[a] if action_counts[a] != 0 else 0 for a in actions}\n",
    "    return avg_rewards\n",
    "\n",
    "\n",
    "# Test the environment\n",
    "avg_rewards = run_agent(1000)\n",
    "print(\"Average rewards after 1000 steps (Simple Environment):\", avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Simple MDP Example\n",
    "\n",
    "Now we'll implement a simple Markov Decision Process (MDP) with states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleMDP:\n",
    "    \"\"\"Simple MDP with states 0 to 4, where action 1 moves right and -1 moves left.\n",
    "    Reward of 1 is given when state reaches 4.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = list(range(5))\n",
    "        self.current_state = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.current_state + action\n",
    "        next_state = max(0, min(4, next_state))\n",
    "        reward = 1 if next_state == 4 else 0\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "# Test the MDP\n",
    "env = SimpleMDP()\n",
    "state = env.reset()\n",
    "print(\"Initial state (MDP):\", state)\n",
    "next_state, reward = env.step(1)\n",
    "print(\"After taking action 1, next state:\", next_state, \"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Inverse Reinforcement Learning - Generate Expert Trajectory\n",
    "\n",
    "Expert trajectories can be used for inverse reinforcement learning where we try to infer the reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_expert_trajectory(env, expert_policy, steps=10):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    for _ in range(steps):\n",
    "        action = expert_policy(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        trajectory.append((state, action, next_state, reward))\n",
    "        state = next_state\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def expert_policy(state):\n",
    "    return 1  # Always move right\n",
    "\n",
    "\n",
    "# Generate and display an expert trajectory\n",
    "env = SimpleMDP()\n",
    "trajectory = generate_expert_trajectory(env, expert_policy, steps=10)\n",
    "print(\"Expert Trajectory:\")\n",
    "for t in trajectory:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Policy Evaluation for a 3-State MDP (Value Function)\n",
    "\n",
    "Now let's implement policy evaluation to compute the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def policy_evaluation(P, V, gamma, theta=1e-4, states=[0,1,2]):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s not in P or not P[s]:\n",
    "                continue\n",
    "            v = V[s]\n",
    "            V[s] = sum(prob * (reward + gamma * V[next_state]) for next_state, prob, reward in P[s])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "\n",
    "# Test policy evaluation\n",
    "states = [0, 1, 2]\n",
    "P = {\n",
    "    0: [(1, 1.0, 0)],\n",
    "    1: [(0, 0.5, 0), (2, 0.5, 1)],\n",
    "    2: []  # terminal state\n",
    "}\n",
    "gamma = 0.9\n",
    "V = {s: 0 for s in states}\n",
    "V = policy_evaluation(P, V, gamma)\n",
    "print(\"Computed Value Function:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Q-Learning Simulation Example\n",
    "\n",
    "Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleQLearningEnv:\n",
    "    def __init__(self):\n",
    "        self.states = list(range(5))\n",
    "        self.current_state = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.current_state + action\n",
    "        next_state = max(0, min(4, next_state))\n",
    "        reward = 1 if next_state == 4 else 0\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "def q_learning(episodes=1000, epsilon=0.2):\n",
    "    env = SimpleQLearningEnv()\n",
    "    num_states = 5\n",
    "    actions = [-1, 1]  # left and right moves\n",
    "    Q_table = np.zeros((num_states, len(actions)))\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action_index = np.random.choice(len(actions))\n",
    "            else:\n",
    "                action_index = np.argmax(Q_table[state])\n",
    "            action = actions[action_index]\n",
    "            next_state, reward = env.step(action)\n",
    "            best_next_action = np.argmax(Q_table[next_state])\n",
    "            Q_table[state, action_index] += alpha * (reward + gamma * Q_table[next_state, best_next_action] - Q_table[state, action_index])\n",
    "            state = next_state\n",
    "            if state == 4:\n",
    "                done = True\n",
    "    return Q_table\n",
    "\n",
    "\n",
    "# Run Q-learning\n",
    "Q_table = q_learning(episodes=1000, epsilon=0.2)\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6: Neural Network Function Approximator for Q-values using PyTorch\n",
    "\n",
    "For complex state spaces, neural networks can approximate Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    class QNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super(QNetwork, self).__init__()\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(input_dim, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, output_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.fc(x)\n",
    "\n",
    "    input_dim = 1\n",
    "    output_dim = 2\n",
    "    q_net = QNetwork(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    states = torch.tensor([[0.0], [1.0], [2.0], [3.0]], dtype=torch.float32)\n",
    "    target_Q = torch.tensor([[0.5, 0.7],\n",
    "                              [0.6, 0.8],\n",
    "                              [0.7, 0.9],\n",
    "                              [0.8, 1.0]], dtype=torch.float32)\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = q_net(states)\n",
    "        loss = loss_fn(outputs, target_Q)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Trained Q-network outputs:\")\n",
    "    print(q_net(states).detach().numpy())\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Skipping Q-network demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Business Context - Marketing Campaign Optimization Using Multi-Armed Bandit\n",
    "\n",
    "### 2.1: Setup for Marketing Campaign Simulation\n",
    "\n",
    "We'll simulate marketing campaigns with different conversion rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "true_conversion_rates = [0.2, 0.5, 0.7]\n",
    "num_campaigns = len(true_conversion_rates)\n",
    "\n",
    "def get_campaign_result(campaign_index):\n",
    "    \"\"\"Return 1 (conversion) or 0 (no conversion) based on the campaign's true conversion rate.\"\"\"\n",
    "    if random.random() < true_conversion_rates[campaign_index]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Test the campaign simulation\n",
    "test_results = [get_campaign_result(0) for _ in range(1000)]\n",
    "print(f\"Campaign 1 test conversion rate: {sum(test_results)/len(test_results):.3f} (expected ~{true_conversion_rates[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Multi-Armed Bandit Simulation using Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simulate_bandit(epsilon, steps=1000):\n",
    "    \"\"\"\n",
    "    Simulate a multi-armed bandit using epsilon-greedy strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    epsilon -- exploration rate\n",
    "    steps -- number of steps to simulate\n",
    "    \n",
    "    Returns:\n",
    "    average_reward -- the average reward obtained\n",
    "    Q_estimates -- estimated values for each arm\n",
    "    counts -- number of times each arm was pulled\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    true_probs = true_conversion_rates\n",
    "    num_arms = len(true_probs)\n",
    "    Q_estimates = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    rewards = []\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # TODO: Implement epsilon-greedy action selection\n",
    "        # With probability epsilon, choose a random arm\n",
    "        # Otherwise, choose the arm with the highest estimated value\n",
    "        if np.random.rand() < epsilon:\n",
    "            chosen_arm = np.random.choice(num_arms)\n",
    "        else:\n",
    "            # YOUR CODE HERE: Choose the arm with the highest estimated value\n",
    "            chosen_arm = 0  # Replace this with your code\n",
    "        \n",
    "        # Generate reward based on the chosen arm's true probability\n",
    "        reward = 1 if np.random.rand() < true_probs[chosen_arm] else 0\n",
    "        \n",
    "        # TODO: Update estimated value for the chosen arm\n",
    "        # Update count for the chosen arm\n",
    "        counts[chosen_arm] += 1\n",
    "        \n",
    "        # YOUR CODE HERE: Update Q_estimates for the chosen arm using incremental average\n",
    "        # Formula: Q_new = Q_old + (reward - Q_old) / count\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    average_reward = np.mean(rewards)\n",
    "    return average_reward, Q_estimates, counts\n",
    "\n",
    "\n",
    "# Uncomment to test your bandit implementation\n",
    "# avg_reward, Q_estimates, counts = simulate_bandit(epsilon=0.1, steps=1000)\n",
    "# print(\"Epsilon=0.1 Bandit Simulation:\")\n",
    "# print(\"Average Reward:\", avg_reward)\n",
    "# print(\"Estimated Q-values:\", Q_estimates)\n",
    "# print(\"Counts:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Other Campaign Selection Strategies\n",
    "\n",
    "Let's implement and compare different strategies for selecting marketing campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def random_strategy(num_simulations=1000):\n",
    "    \"\"\"Implement random selection strategy for marketing campaign selection.\"\"\"\n",
    "    conversions = 0\n",
    "    campaign_counts = np.zeros(num_campaigns)\n",
    "    \n",
    "    # TODO: Implement random strategy\n",
    "    for _ in range(num_simulations):\n",
    "        # YOUR CODE HERE: Choose a random campaign\n",
    "        campaign = 0  # Replace this with your code to select a random campaign\n",
    "        \n",
    "        # Get the result and update tracking variables\n",
    "        result = get_campaign_result(campaign)\n",
    "        conversions += result\n",
    "        campaign_counts[campaign] += 1\n",
    "    \n",
    "    print(\"Random Strategy Results:\")\n",
    "    print(f\"Total Conversions: {conversions} out of {num_simulations} ({conversions/num_simulations:.3f})\")\n",
    "    print(\"Campaign Selection Counts:\", campaign_counts)\n",
    "    return conversions, campaign_counts\n",
    "\n",
    "\n",
    "def greedy_strategy(num_simulations=1000):\n",
    "    \"\"\"Implement greedy selection strategy for marketing campaign selection.\"\"\"\n",
    "    conversions = 0\n",
    "    campaign_counts = np.zeros(num_campaigns)\n",
    "    campaign_conversions = np.zeros(num_campaigns)\n",
    "    \n",
    "    # Initial exploration: try each campaign once\n",
    "    for campaign in range(num_campaigns):\n",
    "        result = get_campaign_result(campaign)\n",
    "        conversions += result\n",
    "        campaign_counts[campaign] += 1\n",
    "        campaign_conversions[campaign] += result\n",
    "    \n",
    "    # TODO: Implement greedy strategy for remaining simulations\n",
    "    for _ in range(num_simulations - num_campaigns):\n",
    "        # YOUR CODE HERE: Calculate conversion rates and choose the campaign with highest rate\n",
    "        conversion_rates = campaign_conversions / campaign_counts\n",
    "        campaign = 0  # Replace this with your code to select the highest conversion rate campaign\n",
    "        \n",
    "        # Get the result and update tracking variables\n",
    "        result = get_campaign_result(campaign)\n",
    "        conversions += result\n",
    "        campaign_counts[campaign] += 1\n",
    "        campaign_conversions[campaign] += result\n",
    "    \n",
    "    print(\"Greedy Strategy Results:\")\n",
    "    print(f\"Total Conversions: {conversions} out of {num_simulations} ({conversions/num_simulations:.3f})\")\n",
    "    print(\"Campaign Selection Counts:\", campaign_counts)\n",
    "    print(\"Estimated Conversion Rates:\", campaign_conversions / campaign_counts)\n",
    "    return conversions, campaign_counts, campaign_conversions\n",
    "\n",
    "\n",
    "def epsilon_greedy_strategy(epsilon, num_simulations=1000):\n",
    "    \"\"\"Implement epsilon-greedy selection strategy for marketing campaign selection.\"\"\"\n",
    "    conversions = 0\n",
    "    campaign_counts = np.zeros(num_campaigns)\n",
    "    campaign_conversions = np.zeros(num_campaigns)\n",
    "    history = []\n",
    "    \n",
    "    # Initial exploration: try each campaign once\n",
    "    for campaign in range(num_campaigns):\n",
    "        result = get_campaign_result(campaign)\n",
    "        conversions += result\n",
    "        campaign_counts[campaign] += 1\n",
    "        campaign_conversions[campaign] += result\n",
    "        history.append(conversions / (campaign + 1))\n",
    "    \n",
    "    # TODO: Implement epsilon-greedy strategy for remaining simulations\n",
    "    for i in range(num_simulations - num_campaigns):\n",
    "        # YOUR CODE HERE: With probability epsilon, choose random; otherwise choose best\n",
    "        if random.random() < epsilon:\n",
    "            campaign = 0  # Replace with code to choose a random campaign\n",
    "        else:\n",
    "            conversion_rates = campaign_conversions / campaign_counts\n",
    "            campaign = 0  # Replace with code to choose the campaign with highest rate\n",
    "        \n",
    "        # Get the result and update tracking variables\n",
    "        result = get_campaign_result(campaign)\n",
    "        conversions += result\n",
    "        campaign_counts[campaign] += 1\n",
    "        campaign_conversions[campaign] += result\n",
    "        history.append(conversions / (i + num_campaigns + 1))\n",
    "    \n",
    "    print(f\"Epsilon-Greedy (ε={epsilon}) Strategy Results:\")\n",
    "    print(f\"Total Conversions: {conversions} out of {num_simulations} ({conversions/num_simulations:.3f})\")\n",
    "    print(\"Campaign Selection Counts:\", campaign_counts)\n",
    "    print(\"Estimated Conversion Rates:\", campaign_conversions / campaign_counts)\n",
    "    return conversions, campaign_counts, campaign_conversions, history\n",
    "\n",
    "\n",
    "def ucb_strategy(num_simulations=1000, c=2.0):\n",
    "    \"\"\"Implement UCB selection strategy for marketing campaign selection.\"\"\"\n",
    "    conversions = 0\n",
    "    campaign_counts = np.zeros(num_campaigns)\n",
    "    campaign_conversions = np.zeros(num_campaigns)\n",
    "    history = []\n",
    "    \n",
    "    # Initial exploration: try each campaign once\n",
    "    for campaign in range(num_campaigns):\n",
    "        result = get_campaign_result(campaign)\n",
    "        conversions += result\n",
    "        campaign_counts[campaign] += 1\n",
    "        campaign_conversions[campaign] += result\n",
    "        history.append(conversions / (campaign + 1))\n",
    "    \n",
    "    # TODO: Implement UCB strategy for remaining simulations\n",
    "    for i in range(num_simulations - num_campaigns):\n",
    "        t = i + num_campaigns\n",
    "        \n",
    "        # YOUR CODE HERE: Calculate UCB values for each campaign\n",
    "        # Formula: UCB = exploitation_term + exploration_term\n",
    "        # exploitation_term = campaign_conversions / campaign_counts\n",
    "        # exploration_term = c * sqrt(log(t) / campaign_counts)\n",
    "        ucb_values = np.zeros(num_campaigns)\n",
    "        \n",
    "        # Choose the campaign with the highest UCB value\n",
    "        campaign = 0  # Replace with code to choose the campaign with highest UCB value\n",
    "        \n",
    "        # Get the result and update tracking variables\n",
    "        result = get_campaign_result(campaign)\n",
    "        conversions += result\n",
    "        campaign_counts[campaign] += 1\n",
    "        campaign_conversions[campaign] += result\n",
    "        history.append(conversions / (t + 1))\n",
    "    \n",
    "    print(f\"UCB Strategy (c={c}) Results:\")\n",
    "    print(f\"Total Conversions: {conversions} out of {num_simulations} ({conversions/num_simulations:.3f})\")\n",
    "    print(\"Campaign Selection Counts:\", campaign_counts)\n",
    "    print(\"Estimated Conversion Rates:\", campaign_conversions / campaign_counts)\n",
    "    return conversions, campaign_counts, campaign_conversions, history\n",
    "\n",
    "\n",
    "# Test your strategy implementations\n",
    "# print(\"\\nTesting Random Strategy:\")\n",
    "# random_results = random_strategy(1000)\n",
    "# print(\"\\nTesting Greedy Strategy:\")\n",
    "# greedy_strategy(1000)\n",
    "# for eps in [0.0, 0.1, 0.3, 0.5]:\n",
    "#     print(f\"\\nTesting Epsilon-Greedy Strategy with epsilon={eps}:\")\n",
    "#     epsilon_greedy_strategy(eps, 1000)\n",
    "# print(\"\\nTesting UCB Strategy:\")\n",
    "# ucb_strategy(1000, c=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Visualization of Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_strategy_comparison(random_results, eps_results_dict, true_rates=true_conversion_rates):\n",
    "    \"\"\"Plot a comparison of different strategies.\"\"\"\n",
    "    strategies = ['Random'] + [f'ε-Greedy (ε={eps})' for eps in eps_results_dict.keys()]\n",
    "    conversion_rates = [random_results[0] / 1000] + [eps_results_dict[eps][0] / 1000 for eps in eps_results_dict.keys()]\n",
    "    plt.figure(figsize=(12,6))\n",
    "    bars = plt.bar(strategies, conversion_rates, color=sns.color_palette(\"viridis\", len(strategies)))\n",
    "    plt.axhline(y=max(true_rates), color='r', linestyle='--', alpha=0.7, label=f'Best Campaign ({max(true_rates):.2f})')\n",
    "    plt.axhline(y=sum(true_rates)/len(true_rates), color='gray', linestyle='--', alpha=0.7, label=f'Average Campaign ({sum(true_rates)/len(true_rates):.2f})')\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    plt.ylim(0, max(true_rates) + 0.1)\n",
    "    plt.ylabel('Overall Conversion Rate')\n",
    "    plt.xlabel('Strategy')\n",
    "    plt.title('Performance Comparison of Campaign Selection Strategies')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_curves(eps_history, ucb_history, true_rates=true_conversion_rates):\n",
    "    \"\"\"Plot learning curves for different strategies.\"\"\"\n",
    "    plt.figure(figsize=(14,7))\n",
    "    for eps, history in eps_history.items():\n",
    "        plt.plot(history, label=f'ε-Greedy (ε={eps})')\n",
    "    plt.plot(ucb_history, label='UCB', linewidth=2)\n",
    "    plt.axhline(y=max(true_rates), color='r', linestyle='--', alpha=0.7, label=f'Best Campaign ({max(true_rates):.2f})')\n",
    "    plt.axhline(y=sum(true_rates)/len(true_rates), color='gray', linestyle='--', alpha=0.7, label=f'Average Campaign ({sum(true_rates)/len(true_rates):.2f})')\n",
    "    plt.title('Learning Curve: Cumulative Conversion Rate Over Time')\n",
    "    plt.xlabel('Number of Customers')\n",
    "    plt.ylabel('Cumulative Conversion Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run experiments for visualization\n",
    "# eps_values = [0.0, 0.1, 0.3, 0.5]\n",
    "# eps_history = {}\n",
    "# eps_results = {}\n",
    "# for eps in eps_values:\n",
    "#     conv, counts, convs, history = epsilon_greedy_strategy(eps, 10000)\n",
    "#     eps_history[eps] = history\n",
    "#     eps_results[eps] = (conv, counts, convs)\n",
    "\n",
    "# random_results = random_strategy(10000)\n",
    "# ucb_conv, ucb_counts, ucb_convs, ucb_history = ucb_strategy(10000, c=1.0)\n",
    "\n",
    "# Plot the results\n",
    "# plot_strategy_comparison(random_results, eps_results)\n",
    "# plot_learning_curves(eps_history, ucb_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Assignment and Automated Grading\n",
    "\n",
    "This section contains an automated grading system that evaluates your implementation of the epsilon-greedy strategy for the marketing campaign optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def assignment_simulation(epsilon=0.1, steps=1000, threshold=0.65):\n",
    "    \"\"\"Run a simulation to test your epsilon-greedy implementation.\"\"\"\n",
    "    avg_reward, Q_estimates, counts = simulate_bandit(epsilon, steps)\n",
    "    print(\"Assignment Simulation:\")\n",
    "    print(\"Average Reward: {:.3f}\".format(avg_reward))\n",
    "    print(\"Estimated Conversion Rates:\", Q_estimates)\n",
    "    print(\"Counts:\", counts)\n",
    "    if avg_reward >= threshold:\n",
    "        print(\"PASS: Your algorithm achieved an average reward of {:.3f} (>= {:.2f}).\".format(avg_reward, threshold))\n",
    "    else:\n",
    "        print(\"FAIL: Your algorithm achieved an average reward of {:.3f} (< {:.2f}). Please review your implementation.\".format(avg_reward, threshold))\n",
    "\n",
    "# Run the assignment simulation\n",
    "# assignment_simulation(epsilon=0.1, steps=1000, threshold=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Theoretical Reflection and Discussion Questions\n",
    "\n",
    "1. What is the main idea behind trial-and-error learning in RL?\n",
    "2. How does the reward function guide an agent's behavior in RL?\n",
    "3. In the context of marketing campaigns, how does the balance between exploration and exploitation affect performance?\n",
    "4. How do different strategies (random, greedy, epsilon-greedy, UCB) compare in balancing exploration and exploitation?\n",
    "5. How can a value function or Q-function be used to make long-term business decisions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 