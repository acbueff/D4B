{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "**Definition:**  \n",
    "NLP is the field that makes human language accessible to computers. It enables machines to read, interpret, and generate text, which is essential for applications such as intelligent search engines, machine translation, and dialogue systems.\n",
    "\n",
    "**Business Application:**  \n",
    "Consider how a customer service chatbot uses NLP to understand and respond to client inquiries in real time.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- What are some business applications where understanding and generating human language could provide a competitive advantage?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple demonstration of text processing: tokenizing a sentence into words.\n",
    "\n",
    "sample_text = \"Welcome to the world of Natural Language Processing for business applications!\"\n",
    "\n",
    "# Tokenization: splitting the text into words\n",
    "tokens = sample_text.split()\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nTokenized Words:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "This demo shows how we can break a sentence into its individual words (tokens), a fundamental step in NLP. In real-world applications, tokenization is the first step in tasks like search, sentiment analysis, and automated customer support.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- Why is tokenization important in processing natural language data?  \n",
    "- Can you think of a scenario in your business where extracting key words from text might be useful?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. General-Purpose Linguistic Representations\n",
    "\n",
    "There are two major paradigms in NLP:\n",
    "1. **Linguistic Knowledge-Based Approaches:**  \n",
    "   - Use rule-based pipelines and linguistic rules (e.g., parts-of-speech tagging, dependency trees).\n",
    "2. **Deep Learning-Based Approaches:**  \n",
    "   - Use end-to-end neural networks to learn representations directly from raw text.\n",
    "\n",
    "**Business Application:**  \n",
    "For example, a sentiment analysis tool might use linguistic rules to identify opinion words or use deep learning to learn these patterns automatically.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- What are the benefits of using deep learning-based approaches over rule-based methods in processing language data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use spaCy, a popular NLP library, to perform tokenization and part-of-speech (POS) tagging.\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Our new product is receiving excellent reviews from customers worldwide.\")\n",
    "\n",
    "print(\"Tokens and their POS tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.pos_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "In this example, spaCy automatically tokenizes the text and assigns a part-of-speech tag to each token. This linguistic information can be used to better understand the structure and meaning of the text.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- How might POS tagging be useful in a business context (e.g., in customer feedback analysis)?  \n",
    "- What advantages does using an established library like spaCy offer over building a rule-based system from scratch?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Language as a Unique Data Type\n",
    "\n",
    "Unlike images or audio, text is **discrete** and has an implicit **hierarchical structure** (letters form words, words form sentences, etc.). Additionally, word frequencies often follow a power-law distribution, meaning a few words appear very frequently while many appear rarely.\n",
    "\n",
    "**Business Application:**  \n",
    "Understanding these properties is crucial when designing systems for customer feedback analysis or market research, where uncommon words may carry significant information.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- Why might the discrete nature of text pose challenges when designing algorithms for text analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sample_text = \"\"\"Natural Language Processing enables computers to understand human language.\n",
    "It is used in chatbots, search engines, and sentiment analysis.\n",
    "Understanding word frequencies is key in many NLP applications.\"\"\"\n",
    "\n",
    "# Convert text to lowercase and split into words\n",
    "words = sample_text.lower().split()\n",
    "word_counts = Counter(words)\n",
    "\n",
    "print(\"Word Frequency Distribution:\")\n",
    "print(word_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "This demo shows the frequency distribution of words in a sample text. Notice that common words (like \"is\" or \"in\") occur very frequently while others may appear only once.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- What challenges might arise from the uneven distribution of words when analyzing text data?  \n",
    "- How could this information be used to improve business decision-making (e.g., by focusing on less common but more informative terms)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Word Representations\n",
    "\n",
    "There are two popular ways to represent words:\n",
    "- **One-hot Vectors:**  \n",
    "  - High-dimensional, sparse vectors where each word is represented by a unique index.\n",
    "- **Word Embeddings:**  \n",
    "  - Dense, lower-dimensional representations that capture semantic relationships between words.\n",
    "\n",
    "**Business Application:**  \n",
    "Word embeddings can help in understanding customer sentiment, identifying trends, and enabling semantic search in large text corpora.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- What might be some limitations of one-hot encoding compared to word embeddings when analyzing large text datasets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a small vocabulary\n",
    "vocabulary = [\"machine\", \"learning\", \"business\", \"data\"]\n",
    "\n",
    "# One-hot encoding for the word \"business\"\n",
    "word_index = vocabulary.index(\"business\")\n",
    "one_hot = np.zeros(len(vocabulary))\n",
    "one_hot[word_index] = 1\n",
    "\n",
    "print(\"One-Hot Encoding for 'business':\")\n",
    "print(one_hot)\n",
    "\n",
    "# For demonstration, assume we have pre-trained word embeddings (simulated here)\n",
    "# Each word is represented by a dense 3-dimensional vector.\n",
    "word_embeddings = {\n",
    "    \"machine\": np.array([0.2, 0.1, 0.7]),\n",
    "    \"learning\": np.array([0.3, 0.4, 0.5]),\n",
    "    \"business\": np.array([0.9, 0.1, 0.3]),\n",
    "    \"data\": np.array([0.5, 0.8, 0.2])\n",
    "}\n",
    "\n",
    "print(\"\\nWord Embedding for 'business':\")\n",
    "print(word_embeddings[\"business\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "In this demo, we see that one-hot vectors are sparse and simple, while word embeddings are dense and can capture semantic similarities between words. For example, words used in similar contexts tend to have similar embeddings.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- How do dense word embeddings overcome the limitations of one-hot encoding?  \n",
    "- In what business scenarios might the semantic relationships captured by word embeddings be particularly valuable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The Distributional Hypothesis\n",
    "\n",
    "**The Distributional Hypothesis** states:  \n",
    "*\"You shall know a word by the company it keeps.\"*  \n",
    "This means that words appearing in similar contexts tend to have similar meanings.\n",
    "\n",
    "**Business Application:**  \n",
    "In market analysis, understanding the context in which products are mentioned can reveal customer perceptions and emerging trends.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- How might the distributional hypothesis be applied to improve customer sentiment analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two simple sentences with similar contexts\n",
    "sentence1 = \"The new smartphone has an excellent battery life.\"\n",
    "sentence2 = \"The latest mobile phone offers great battery performance.\"\n",
    "\n",
    "# For demonstration, we will simulate context similarity by comparing the overlap in keywords.\n",
    "keywords1 = set(sentence1.lower().split())\n",
    "keywords2 = set(sentence2.lower().split())\n",
    "\n",
    "common_keywords = keywords1.intersection(keywords2)\n",
    "\n",
    "print(\"Common keywords in both sentences:\")\n",
    "print(common_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "This simple demonstration shows that sentences with similar contexts share common keywords, supporting the distributional hypothesis. In practice, advanced methods use co-occurrence statistics to learn richer word representations.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- How does the similarity in word contexts help in understanding the meaning of words?  \n",
    "- What are some limitations of using simple keyword overlaps to capture context?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Co-occurrence and Word Embeddings\n",
    "\n",
    "Co-occurrence matrices capture how frequently words appear together in a corpus. These matrices can be factorized to produce word embeddings that capture semantic similarity.\n",
    "\n",
    "**Business Application:**  \n",
    "Understanding word co-occurrence can enhance search algorithms, making them more semantically aware and improving customer query matching.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- How might co-occurrence information improve the relevance of search results on a business website?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample corpus: a list of sentences\n",
    "corpus = [\n",
    "    \"data science is revolutionizing business\",\n",
    "    \"machine learning transforms data into insights\",\n",
    "    \"business decisions are driven by data and analytics\"\n",
    "]\n",
    "\n",
    "# Build a simple vocabulary\n",
    "vocab = sorted(set(\" \".join(corpus).split()))\n",
    "vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize a co-occurrence matrix\n",
    "co_occurrence = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "# Define a simple window size (context of 1 word to left/right)\n",
    "window_size = 1\n",
    "\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words):\n",
    "        word_idx = vocab_index[word]\n",
    "        # Look at neighbors in the window\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(words), i + window_size + 1)\n",
    "        for j in range(start, end):\n",
    "            if i == j:\n",
    "                continue\n",
    "            neighbor_idx = vocab_index[words[j]]\n",
    "            co_occurrence[word_idx, neighbor_idx] += 1\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n",
    "print(\"\\nCo-occurrence Matrix:\")\n",
    "print(co_occurrence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "This code builds a co-occurrence matrix from a small corpus. In larger systems, such matrices (or their factorized versions) help produce word embeddings that capture semantic similarity.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- What might be the advantages of using co-occurrence data for learning word representations?  \n",
    "- How could these techniques be applied to enhance business intelligence systems?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluating Word Embeddings\n",
    "\n",
    "Word embeddings can be evaluated both qualitatively (through visualization) and quantitatively (using similarity metrics like cosine similarity). Dimensionality reduction techniques like PCA and t-SNE help visualize high-dimensional embeddings.\n",
    "\n",
    "**Business Application:**  \n",
    "Visualization can help marketing teams understand semantic clusters in customer feedback, while similarity computations can improve recommendation systems.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- Why is it useful to evaluate and visualize word embeddings in real-world applications?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For demonstration, we use our simulated word embeddings from section 4.\n",
    "words = list(word_embeddings.keys())\n",
    "embeddings = np.array([word_embeddings[word] for word in words])\n",
    "\n",
    "# Reduce dimensions to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "\n",
    "plt.title(\"PCA Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "The PCA visualization shows how words are distributed in a 2D space based on their embeddings. Words with similar meanings (or contexts) tend to be closer together.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- How might visualization of word embeddings aid in understanding customer sentiments?  \n",
    "- What does the proximity of two words in the reduced space imply about their relationship?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Large Pre-Trained Language Models\n",
    "\n",
    "Large language models are trained on vast amounts of data to understand and generate language. They serve as a foundation for many NLP tasks through transfer learning.\n",
    "\n",
    "**Business Application:**  \n",
    "These models power advanced features like chatbots, automated report generation, and intelligent search systems that can understand complex queries.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- How does pre-training on large corpora benefit business applications such as customer support or content recommendation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Using Hugging Face's transformers pipeline for text generation.\n",
    "# For business applications, this can be used to generate product descriptions or customer responses.\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", max_length=50)\n",
    "\n",
    "prompt = \"Our innovative product offers\"\n",
    "generated_text = generator(prompt, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "In this demo, a pre-trained language model (GPT-2) generates text based on a prompt. Such models can be fine-tuned for specific tasks, enabling applications like automated report generation and personalized marketing messages.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- What are the benefits and challenges of using large pre-trained models in a business setting?  \n",
    "- How might transfer learning improve the efficiency of deploying NLP applications?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Generative Pre-Training and Discriminative Fine-Tuning\n",
    "\n",
    "Many modern NLP systems first pre-train a language model on a large corpus (generative pre-training) and then fine-tune it on a specific task (discriminative fine-tuning). This approach leverages general language understanding for task-specific performance.\n",
    "\n",
    "**Business Application:**  \n",
    "Fine-tuning allows companies to adapt powerful language models to tasks like sentiment analysis, spam detection, or customer feedback categorization with relatively little data.\n",
    "\n",
    "In many business scenarios, companies need to quickly generate professional responses—for example, to address customer concerns about delayed shipments. In this demo, we fine-tune a pre-trained language model on a small dataset of customer support emails. This process leverages the general language knowledge already learned by the model (generative pre-training) and adapts it to our specific task (discriminative fine-tuning).\n",
    "\n",
    "**Task:**  \n",
    "- Fine-tune the pre-trained model on a few examples of customer support emails regarding delayed shipments.\n",
    "- Use the fine-tuned model to generate a professional email response when given a prompt.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- How might fine-tuning a pre-trained model on domain-specific data improve its performance for targeted business tasks?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning a pre-trained generative language model (distilgpt2) on a small customer support email dataset.\n",
    "\n",
    "# 1. Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 2. Define a small dataset of customer support emails regarding delayed shipments\n",
    "data = [\n",
    "    {\"text\": \"Subject: Delayed Shipment\\n\\nDear Customer,\\nWe regret to inform you that your shipment has been delayed due to unforeseen circumstances. We are working to resolve the issue as soon as possible.\\n\\nSincerely,\\nCustomer Support\"},\n",
    "    {\"text\": \"Subject: Shipping Delay Notice\\n\\nDear Valued Customer,\\nUnfortunately, your order is experiencing a delay. We apologize for any inconvenience and will update you shortly.\\n\\nBest regards,\\nCustomer Service Team\"},\n",
    "    {\"text\": \"Subject: Update on Your Order\\n\\nHello,\\nYour order has encountered a minor delay. Our team is actively addressing this, and we appreciate your patience.\\n\\nThank you,\\nSupport Team\"}\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries into a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# 3. Load the tokenizer and model from Hugging Face\n",
    "model_name = \"distilgpt2\"  # A small model for demonstration purposes\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Fix: Set the padding token to the EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 4. Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Use a fixed maximum length for simplicity (adjust as needed)\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# 5. Create a data collator for language modeling (no masked LM for causal models)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 6. Define training arguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                # Fine-tune for 3 epochs\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    ")\n",
    "\n",
    "# 7. Create the Trainer and fine-tune the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning on the small customer support email dataset...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete.\")\n",
    "\n",
    "# 8. Test the fine-tuned model using a prompt\n",
    "prompt = \"Subject: Delayed Shipment\\n\\nDear Customer,\"\n",
    "\n",
    "# Tokenize and move the input to the same device as the model\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# Generate a response using the fine-tuned model\n",
    "output_ids = model.generate(input_ids, max_length=100, num_return_sequences=1, do_sample=True, top_k=50)\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Email:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "The two-step process of pre-training and fine-tuning enables models to benefit from both large-scale language understanding and task-specific adjustments. This approach is particularly effective in business applications where labeled data may be limited.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- What are the trade-offs between generative pre-training and discriminative fine-tuning?  \n",
    "- How does fine-tuning help tailor a general model to a business-specific problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Transformer-Based NLP Models\n",
    "\n",
    "Transformer architectures (such as GPT, BERT, etc.) have revolutionized NLP. They use self-attention mechanisms to process sequences and are highly effective for a range of tasks, from text classification to machine translation.\n",
    "\n",
    "**Business Application:**  \n",
    "Transformers are used to build systems for customer support, automated content moderation, and personalized recommendations.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- How do transformer models differ from earlier NLP models in handling long-range dependencies in text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a pre-trained transformer model for sentiment analysis\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "sample_text = \"The customer service at our company is exceptional and very responsive.\"\n",
    "result = classifier(sample_text)\n",
    "\n",
    "print(\"Sentiment Analysis Result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "This demo shows how a transformer-based model can be used out-of-the-box for text classification tasks, such as sentiment analysis. Such capabilities can help businesses quickly gauge customer opinions and tailor their responses.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- What advantages do transformer-based models offer for real-time customer feedback analysis?  \n",
    "- How might the ability to quickly adapt to new tasks benefit business operations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Prompting for NLP\n",
    "\n",
    "Prompting involves guiding a language model's output by providing a well-crafted input. This can be used in zero-shot or few-shot settings to achieve tasks without extensive fine-tuning.\n",
    "\n",
    "**Business Application:**  \n",
    "Prompting allows businesses to leverage large language models for tasks like automated customer emails, product recommendations, or interactive chatbots, even without specialized training data.\n",
    "\n",
    "**Before the Demo Question:**  \n",
    "- How might carefully designed prompts improve the performance of a language model in a business context?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a simple prompt with a text generation model to demonstrate the concept\n",
    "prompt = \"Generate a professional customer service email regarding a delayed shipment:\"\n",
    "\n",
    "generated_email = generator(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated Email:\")\n",
    "print(generated_email[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**  \n",
    "In this demo, a prompt is used to guide a language model to generate a customer service email. This approach demonstrates how prompting can adapt a general-purpose model to specific business tasks.\n",
    "\n",
    "**Discussion Questions:**  \n",
    "- What are some best practices when designing prompts for language models?  \n",
    "- Can you think of other business scenarios where prompting might be an effective strategy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
