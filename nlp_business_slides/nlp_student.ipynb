{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined NLP Course Notebook (Student Version)\n",
    "\n",
    "This notebook integrates concepts from general NLP introduction and a specific case study on Sentiment Analysis. It includes explanations, code demonstrations, and reflective questions. Some sections require you to complete the code as part of the learning exercises.\n",
    "\n",
    "**Sources:**\n",
    "* Based on `natural_language_processing.ipynb` and `sentiment_analysis_demo.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "**Definition:** \n",
    "NLP is the field that makes human language accessible to computers. It enables machines to read, interpret, and generate text, which is essential for applications such as intelligent search engines, machine translation, and dialogue systems.\n",
    "\n",
    "**Business Application:** \n",
    "Consider how a customer service chatbot uses NLP to understand and respond to client inquiries in real time.\n",
    "\n",
    "**Before the Demo Question:** \n",
    "- What are some business applications where understanding and generating human language could provide a competitive advantage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple demonstration of text processing: tokenizing a sentence into words.\n",
    "\n",
    "sample_text_intro = \"Welcome to the world of Natural Language Processing for business applications!\"\n",
    "\n",
    "# Tokenization: splitting the text into words\n",
    "tokens_intro = sample_text_intro.split()\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(sample_text_intro)\n",
    "print(\"\\nTokenized Words:\")\n",
    "print(tokens_intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "This demo shows how we can break a sentence into its individual words (tokens), a fundamental step in NLP. In real-world applications, tokenization is the first step in tasks like search, sentiment analysis, and automated customer support.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- Why is tokenization important in processing natural language data? \n",
    "- Can you think of a scenario in your business where extracting key words from text might be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization and Text Preprocessing\n",
    "\n",
    "**Definition:** \n",
    "Text preprocessing involves cleaning and preparing text data for analysis. This includes steps like tokenization (breaking text into words or sentences), lowercasing, removing punctuation and stop words (common words like 'the', 'is', 'in'), and stemming/lemmatization (reducing words to their root form).\n",
    "\n",
    "**Business Application:** \n",
    "Preprocessing ensures consistency in analyzing customer feedback forms, removing noise to focus on meaningful content.\n",
    "\n",
    "**Before the Demo Question:** \n",
    "- Why is it important to clean text data before analyzing it? What kind of 'noise' might exist in raw text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo using NLTK for more advanced preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data (if not already downloaded)\n",
    "# You might need to run these downloads once\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK 'wordnet' data...\")\n",
    "    nltk.download('wordnet')\n",
    "    print(\"Download complete.\")\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK 'stopwords' data...\")\n",
    "    nltk.download('stopwords')\n",
    "    print(\"Download complete.\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK 'punkt' data...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "sample_text_proc = \"This is an example sentence, showing off the stop words filtration and stemming! It's quite amazing.\"\n",
    "\n",
    "# 1. Tokenization\n",
    "tokens = word_tokenize(sample_text_proc)\n",
    "\n",
    "# 2. Lowercasing\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "# 3. Removing Punctuation\n",
    "tokens = [word for word in tokens if word.isalnum()] # Keep only alphanumeric tokens\n",
    "\n",
    "# 4. Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# 5. Stemming (reducing words to root form)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# 6. Lemmatization (reducing words to base/dictionary form)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(\"Original Text:\", sample_text_proc)\n",
    "print(\"\\nTokenized & Lowercased:\", word_tokenize(sample_text_proc.lower()))\n",
    "print(\"\\nAfter Removing Punctuation & Stopwords:\", tokens)\n",
    "print(\"\\nStemmed Tokens:\", stemmed_tokens)\n",
    "print(\"\\nLemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "This demo illustrates several common preprocessing steps. Stemming is faster but can produce non-dictionary words, while lemmatization is slower but yields actual words. The choice depends on the specific NLP task and desired outcome.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- When might you prefer stemming over lemmatization, or vice versa?\n",
    "- How could removing stop words impact the analysis of certain types of text (e.g., legal documents vs. social media posts)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Representation: Vectorization\n",
    "\n",
    "**Definition:** \n",
    "Computers understand numbers, not text. Vectorization converts text data into numerical vectors. Common techniques include Bag-of-Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and word embeddings (like Word2Vec, GloVe).\n",
    "\n",
    "**Business Application:** \n",
    "Vectorization allows machine learning models to process text for tasks like document classification (e.g., sorting emails into folders) or sentiment analysis.\n",
    "\n",
    "**Before the Demo Question:** \n",
    "- How can we represent the meaning or content of text using only numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo using TF-IDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the data\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the TF-IDF matrix as a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=['Doc1', 'Doc2', 'Doc3', 'Doc4'])\n",
    "\n",
    "print(\"Corpus:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"Doc{i+1}: {doc}\")\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "The TF-IDF matrix represents each document as a vector where each dimension corresponds to a word in the vocabulary. The values indicate the importance of each word in the document relative to the entire corpus. Words that are frequent in a specific document but rare across all documents get higher scores.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- What does a high TF-IDF score for a word in a document signify?\n",
    "- How does TF-IDF differ from a simple Bag-of-Words count? What are the advantages of TF-IDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Case Study: Sentiment Analysis for Product Reviews\n",
    "\n",
    "This section provides a practical, in-depth implementation of sentiment analysis, drawing heavily from the `sentiment_analysis_demo.ipynb` structure and content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Business Context\n",
    "\n",
    "Companies like Amazon process millions of customer reviews daily. Automatically analyzing the sentiment of these reviews allows them to:\n",
    "- Identify products with quality issues\n",
    "- Highlight highly-rated products\n",
    "- Track customer satisfaction trends\n",
    "- Feed data into recommendation systems\n",
    "- Respond proactively to negative feedback\n",
    "\n",
    "Understanding customer sentiment at scale is crucial for product development, marketing, and customer relationship management. By the end of this section, you'll understand how sentiment analysis works and how to implement a basic version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load a sample dataset of product reviews. For this demo, we'll create a small, illustrative dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re # Regular expressions for cleaning\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC # Support Vector Classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# NLTK for preprocessing (ensure downloads from section 2)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Settings for plots\n",
    "sns.set_style('darkgrid')\n",
    "plt.style.use('seaborn-v0_8-talk') # Using a visually appealing style\n",
    "\n",
    "# --- Sample Data Creation ---\n",
    "# In a real scenario, you would load data from a CSV or database\n",
    "data = {\n",
    "    'review_text': [\n",
    "        'This product is amazing! Highly recommend.',\n",
    "        'Absolutely terrible quality. Broke after one use.',\n",
    "        'It works okay, nothing special.',\n",
    "        'Love it! Best purchase ever.',\n",
    "        'Waste of money. Very disappointed.',\n",
    "        'Decent product for the price.',\n",
    "        'I am satisfied with this item.',\n",
    "        'Poor customer service and faulty product.',\n",
    "        'Excellent value, works perfectly.',\n",
    "        'Not bad, but could be better.',\n",
    "        'The product arrived damaged and unusable.',\n",
    "        'Fantastic! Exceeded my expectations.',\n",
    "        'Mediocre performance, would not buy again.',\n",
    "        'Just what I needed, great quality.',\n",
    "        'The instructions were unclear, making it hard to use.'\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        'positive',\n",
    "        'negative',\n",
    "        'neutral',\n",
    "        'positive',\n",
    "        'negative',\n",
    "        'neutral',\n",
    "        'positive',\n",
    "        'negative',\n",
    "        'positive',\n",
    "        'neutral',\n",
    "        'negative',\n",
    "        'positive',\n",
    "        'negative',\n",
    "        'positive',\n",
    "        'negative' # Subjective, but leaning negative due to usability issue\n",
    "    ]\n",
    "}\n",
    "df_reviews = pd.DataFrame(data)\n",
    "\n",
    "print(\"Sample Review Data:\")\n",
    "print(df_reviews.head())\n",
    "print(\"\\nData Shape:\", df_reviews.shape)\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(df_reviews['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data Exploration and Preprocessing\n",
    "\n",
    "Before building the model, we need to clean the text data. This involves steps similar to those in Section 2, but tailored for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text Preprocessing Function ---\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Remove punctuation and numbers (keep spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # 3. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # 4. Remove Stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # 5. Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # 6. Join back into string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the review text\n",
    "df_reviews['processed_text'] = df_reviews['review_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\nReviews after Preprocessing:\")\n",
    "print(df_reviews[['review_text', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Extraction (Vectorization)\n",
    "\n",
    "We'll use TF-IDF to convert the processed text into numerical features suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df_reviews['processed_text']\n",
    "y = df_reviews['sentiment']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer (within a pipeline later)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000) # Limit features for efficiency\n",
    "\n",
    "# Fit and transform the training data (Example - will be done in pipeline)\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "# print(f\"\\nTF-IDF Matrix Shape (Train): {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model Building and Training\n",
    "\n",
    "We will train two common text classification models: Multinomial Naive Bayes and Logistic Regression. We use Scikit-learn's `Pipeline` to chain the vectorization and classification steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 1: Multinomial Naive Bayes --- \n",
    "nb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000, ngram_range=(1, 2))), # Use uni- and bi-grams\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "print(\"\\nTraining Naive Bayes Model...\")\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "print(\"Naive Bayes Training Complete.\")\n",
    "\n",
    "# --- Model 2: Logistic Regression --- \n",
    "lr_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
    "    ('clf', LogisticRegression(solver='liblinear', multi_class='auto', random_state=42)) # Good solver for smaller datasets\n",
    "])\n",
    "\n",
    "print(\"\\nTraining Logistic Regression Model...\")\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "print(\"Logistic Regression Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Model Evaluation\n",
    "\n",
    "Let's evaluate the performance of our trained models on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Naive Bayes --- \n",
    "y_pred_nb = nb_pipeline.predict(X_test)\n",
    "print(\"\\n--- Naive Bayes Evaluation ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb, labels=nb_pipeline.classes_)\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', xticklabels=nb_pipeline.classes_, yticklabels=nb_pipeline.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Naive Bayes Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# --- Evaluate Logistic Regression --- \n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "print(\"\\n--- Logistic Regression Evaluation ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr, labels=lr_pipeline.classes_)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', xticklabels=lr_pipeline.classes_, yticklabels=lr_pipeline.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**\n",
    "The classification report provides precision, recall, and F1-score for each class (positive, negative, neutral). The confusion matrix shows how many instances of each true class were predicted as each possible class. Accuracy gives the overall percentage of correct predictions.\n",
    "\n",
    "*Note: With a very small dataset like this example, performance metrics might be volatile and not fully representative. A larger dataset is needed for robust evaluation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Applying the Model to New Reviews\n",
    "\n",
    "Let's use the better-performing model (based on evaluation, though results may vary with this small dataset) to predict the sentiment of new, unseen reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming Logistic Regression performed slightly better or is preferred\n",
    "chosen_model = lr_pipeline \n",
    "\n",
    "new_reviews = [\n",
    "    \"This is the best product I have ever bought! So happy!\",\n",
    "    \"Completely useless, broke within a week.\",\n",
    "    \"It's alright, does the job but nothing fancy.\",\n",
    "    \"Terrible experience, would not recommend to anyone.\"\n",
    "]\n",
    "\n",
    "# Preprocess the new reviews\n",
    "processed_new_reviews = [preprocess_text(review) for review in new_reviews]\n",
    "\n",
    "# Predict sentiment\n",
    "predicted_sentiments = chosen_model.predict(new_reviews) # Pipeline handles preprocessing implicitly if raw text is passed\n",
    "predicted_probabilities = chosen_model.predict_proba(new_reviews)\n",
    "\n",
    "print(\"\\n--- Predictions on New Reviews ---\")\n",
    "for review, sentiment, probs in zip(new_reviews, predicted_sentiments, predicted_probabilities):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    # Show probabilities for each class\n",
    "    prob_dict = {label: f\"{prob:.2%}\" for label, prob in zip(chosen_model.classes_, probs)}\n",
    "    print(f\"Probabilities: {prob_dict}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Business Application: Extracting Insights\n",
    "\n",
    "How can a business use these predictions?\n",
    "\n",
    "1.  **Track Sentiment Trends:** Aggregate sentiment scores over time (e.g., daily, weekly) to monitor overall customer satisfaction for a product or service.\n",
    "2.  **Identify Urgent Issues:** Filter for highly negative reviews. Analyze the text of these reviews (using techniques like topic modeling or keyword extraction) to pinpoint specific problems (e.g., 'shipping damage', 'poor battery life', 'confusing instructions').\n",
    "3.  **Highlight Positive Feedback:** Identify highly positive reviews. Use excerpts in marketing materials or testimonials (with permission). Analyze positive themes to understand what customers value most.\n",
    "4.  **Prioritize Product Improvements:** Correlate sentiment with specific product features mentioned in reviews (requires more advanced Aspect-Based Sentiment Analysis) to guide development efforts.\n",
    "5.  **Improve Customer Support:** Route negative reviews to support teams for follow-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulate analyzing the original dataframe with predictions\n",
    "# (We'll predict on the full dataset for demonstration)\n",
    "df_reviews['predicted_sentiment'] = chosen_model.predict(df_reviews['review_text'])\n",
    "\n",
    "print(\"\\n--- Analysis Example: Identifying Negative Reviews ---\")\n",
    "negative_reviews = df_reviews[df_reviews['predicted_sentiment'] == 'negative']\n",
    "print(f\"Found {len(negative_reviews)} potentially negative reviews:\")\n",
    "print(negative_reviews[['review_text', 'predicted_sentiment']])\n",
    "\n",
    "# Further analysis could involve finding common words in negative reviews\n",
    "from collections import Counter\n",
    "\n",
    "negative_texts = ' '.join(negative_reviews['processed_text'])\n",
    "word_counts = Counter(word_tokenize(negative_texts))\n",
    "\n",
    "print(\"\\nMost common words in predicted negative reviews:\")\n",
    "print(word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Learning Challenge\n",
    "\n",
    "Your task is to try and improve the sentiment analysis model's performance. **Complete the code in the next cell.**\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1.  **Experiment with Vectorization:** \n",
    "    * Try using `CountVectorizer` instead of `TfidfVectorizer` in one of the pipelines.\n",
    "    * Adjust parameters within `TfidfVectorizer` (e.g., `min_df`, `max_df`, `ngram_range=(1, 3)`). Does changing the n-gram range help capture more context?\n",
    "2.  **Try a Different Model:** \n",
    "    * Implement and evaluate a `LinearSVC` (Support Vector Classifier) model. Use a `Pipeline` similar to the other models.\n",
    "\n",
    "**Goal:** Achieve a higher accuracy score or better F1-scores on the test set compared to the initial Naive Bayes and Logistic Regression models.\n",
    "\n",
    "**Report:** Briefly document which changes you made and what impact they had on the evaluation metrics (Accuracy, Classification Report) in a markdown cell or code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Challenge: Improve Sentiment Analysis Model --- \n",
    "\n",
    "print(\"--- Original Model Scores (for comparison) ---\")\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "\n",
    "# --- YOUR CODE HERE --- \n",
    "\n",
    "# Task 1: Experiment with Vectorization (Example: Modify LR pipeline)\n",
    "# Define a new pipeline, perhaps lr_pipeline_v2\n",
    "# Hint: Change TfidfVectorizer to CountVectorizer OR adjust TfidfVectorizer parameters\n",
    "# lr_pipeline_v2 = Pipeline([\n",
    "#    ('vectorizer', CountVectorizer(...)), # Or TfidfVectorizer with different params\n",
    "#    ('clf', LogisticRegression(...))\n",
    "# ])\n",
    "# print(\"\\nTraining Model with different vectorizer...\")\n",
    "# lr_pipeline_v2.fit(X_train, y_train)\n",
    "# y_pred_lr_v2 = lr_pipeline_v2.predict(X_test)\n",
    "# print(\"\\n--- Evaluation (Vectorizer Change) ---\")\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr_v2))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr_v2))\n",
    "\n",
    "\n",
    "# Task 2: Implement and Evaluate LinearSVC\n",
    "# Define a new pipeline, perhaps svc_pipeline\n",
    "# svc_pipeline = Pipeline([\n",
    "#    ('tfidf', TfidfVectorizer(max_features=1000, ngram_range=(1, 2))), # Use consistent vectorizer for comparison\n",
    "#    ('clf', LinearSVC(random_state=42, dual=False, max_iter=1000)) \n",
    "# ])\n",
    "# print(\"\\nTraining LinearSVC Model...\")\n",
    "# svc_pipeline.fit(X_train, y_train)\n",
    "# y_pred_svc = svc_pipeline.predict(X_test)\n",
    "# print(\"\\n--- LinearSVC Evaluation ---\")\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svc))\n",
    "\n",
    "\n",
    "# --- Report Your Findings --- \n",
    "# Add comments here or in a new markdown cell explaining what you tried and the results.\n",
    "# For example:\n",
    "# \"Tried CountVectorizer with Logistic Regression. Accuracy was X.XXX. \n",
    "# LinearSVC with TF-IDF (ngram 1,2) achieved accuracy Y.YYY, which was the best.\" \n",
    "\n",
    "print(\"\\nChallenge section complete. Remember to fill in the code above and report your findings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Business Implications\n",
    "\n",
    "This case study demonstrates that even simple NLP techniques can provide valuable business insights from unstructured text data like customer reviews. \n",
    "\n",
    "Key Takeaways:\n",
    "- **Scalability:** Sentiment analysis automates the processing of vast amounts of text that would be impossible to handle manually.\n",
    "- **Actionable Insights:** It transforms raw feedback into quantifiable metrics and identifiable themes, enabling data-driven decisions.\n",
    "- **Competitive Advantage:** Businesses that effectively leverage customer feedback gain an edge in product quality, customer satisfaction, and market responsiveness.\n",
    "\n",
    "While our model is basic, production systems use more sophisticated techniques (like deep learning models - BERT, RoBERTa) for higher accuracy and nuance (e.g., aspect-based sentiment), but the core principles of preprocessing, vectorization, modeling, and evaluation remain fundamental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition (NER)\n",
    "\n",
    "**Definition:** \n",
    "NER identifies and categorizes key entities in text, such as names of people, organizations, locations, dates, and monetary values.\n",
    "\n",
    "**Business Application:** \n",
    "Extracting company names and locations from news articles for market intelligence, or identifying product names in customer support tickets.\n",
    "\n",
    "**Before the Demo Question:** \n",
    "- If you could automatically extract all mentions of competitors or product names from online articles, how could your business use that information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo using spaCy for NER\n",
    "import spacy\n",
    "\n",
    "# Load a pre-trained spaCy model (download if needed)\n",
    "# You might need to run this command in your terminal or a code cell:\n",
    "# python -m spacy download en_core_web_sm \n",
    "try:\n",
    "    nlp_ner = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print('Downloading language model for spaCy NER...')\n",
    "    print('Please run `python -m spacy download en_core_web_sm` in your terminal or environment.')\n",
    "    # Attempting download via subprocess (might require permissions)\n",
    "    import subprocess\n",
    "    try:\n",
    "       subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'], check=True)\n",
    "       nlp_ner = spacy.load('en_core_web_sm')\n",
    "       print(\"Model downloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not automatically download model. Error: {e}\")\n",
    "        nlp_ner = None\n",
    "\n",
    "text_ner = \"Apple Inc. is planning to open a new store in Stockholm, Sweden next month, costing over $5 million.\"\n",
    "\n",
    "if nlp_ner:\n",
    "    doc_ner = nlp_ner(text_ner)\n",
    "    print(f\"Text: {text_ner}\\n\")\n",
    "    print(\"Named Entities Found:\")\n",
    "    if not doc_ner.ents:\n",
    "        print(\"No entities found by this model.\")\n",
    "    else:\n",
    "        for ent in doc_ner.ents:\n",
    "            print(f\"- {ent.text} ({ent.label_})\")\n",
    "else:\n",
    "    print(\"Skipping NER demo as spaCy model couldn't be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "The spaCy library quickly identifies common entities like organizations (ORG), locations (GPE - Geopolitical Entity), dates (DATE), and money (MONEY). This structured information is much easier for computer systems to work with than raw text.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- What are the limitations of pre-trained NER models? Might they miss domain-specific entities (e.g., specific financial instrument names)?\n",
    "- How could NER be combined with sentiment analysis for more nuanced insights (e.g., finding sentiment towards specific organizations mentioned in news)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Topic Modeling\n",
    "\n",
    "**Definition:** \n",
    "Topic modeling is an unsupervised technique used to discover abstract \"topics\" that occur in a collection of documents. Algorithms like Latent Dirichlet Allocation (LDA) identify groups of words that frequently appear together, representing underlying themes.\n",
    "\n",
    "**Business Application:** \n",
    "Analyzing large volumes of customer survey responses or support emails to automatically identify the main themes or issues being discussed.\n",
    "\n",
    "**Before the Demo Question:** \n",
    "- Imagine you have thousands of open-ended survey responses. How could you quickly get a sense of the main topics customers are talking about without reading every single response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo using LDA with Scikit-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents (replace with real data)\n",
    "topic_corpus = [\n",
    "    \"The new software update has improved performance significantly.\",\n",
    "    \"Customer support was very helpful resolving the issue.\",\n",
    "    \"I found a bug in the latest software version.\",\n",
    "    \"The price is reasonable for the features offered.\",\n",
    "    \"Login issues need to be fixed in the next update.\",\n",
    "    \"Excellent support team, quick response time.\",\n",
    "    \"Feature request: add integration with other tools.\",\n",
    "    \"Billing problem took too long to resolve.\",\n",
    "    \"Performance is slow after the recent software patch.\"\n",
    "]\n",
    "\n",
    "# Vectorize the text data using CountVectorizer (suitable for LDA)\n",
    "# Apply similar preprocessing as before (lowercase, remove stops, etc.)\n",
    "count_vectorizer = CountVectorizer(stop_words='english', lowercase=True, max_df=0.9, min_df=2) # Ignore terms too frequent or too rare\n",
    "X_counts = count_vectorizer.fit_transform(topic_corpus)\n",
    "\n",
    "# Define the number of topics to find\n",
    "num_topics = 3 \n",
    "\n",
    "# Initialize and fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X_counts)\n",
    "\n",
    "# Display the topics\n",
    "feature_names_topic = count_vectorizer.get_feature_names_out()\n",
    "print(f\"\\n--- Top Words per Topic (Found by LDA with {num_topics} topics) ---\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    # Get the indices of the top words for this topic\n",
    "    top_word_indices = topic.argsort()[:-6:-1] # Get top 5 words\n",
    "    top_words = [feature_names_topic[i] for i in top_word_indices]\n",
    "    print(f\"Topic #{topic_idx+1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Assign topics to documents (optional)\n",
    "# topic_distribution = lda.transform(X_counts)\n",
    "# print(\"\\nTopic distribution for first document:\", topic_distribution[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "LDA identifies clusters of co-occurring words, representing latent topics. The interpretation of these topics often requires human judgment (e.g., Topic #1 seems related to 'software/updates/performance', Topic #2 to 'support/issues'). The number of topics is a hyperparameter that often needs tuning.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- How would you determine the optimal number of topics for a given dataset?\n",
    "- What are the challenges in interpreting the topics generated by LDA? How can the results be made more actionable for business decisions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Summarization\n",
    "\n",
    "**Definition:** \n",
    "Text summarization automatically creates a shorter version of a text document while retaining the most important information. There are two main types: extractive (selecting key sentences from the original) and abstractive (generating new sentences that capture the essence).\n",
    "\n",
    "**Business Application:** \n",
    "Generating concise summaries of long reports, news articles, or meeting transcripts to save time and quickly grasp key points.\n",
    "\n",
    "**Before the Demo Question:** \n",
    "- How much time could be saved in your organization if long documents could be reliably summarized automatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo using Hugging Face Transformers for Abstractive Summarization\n",
    "# Note: Requires internet connection and installing transformers & pytorch/tensorflow\n",
    "# You might need to run: pip install transformers torch # or tensorflow\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    # Initialize the summarization pipeline (using a smaller model for faster demo)\n",
    "    # Using distilbart-cnn-6-6 which is smaller than the default bart-large-cnn\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-6-6\")\n",
    "except ImportError:\n",
    "    print(\"Could not import transformers. Please install it: pip install transformers torch\")\n",
    "    summarizer = None\n",
    "except Exception as e:\n",
    "    print(f\"Could not load summarization model. Error: {e}\")\n",
    "    print(\"Skipping summarization demo. Ensure 'transformers' and a backend (torch/tensorflow) are installed.\")\n",
    "    summarizer = None\n",
    "\n",
    "long_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) focused on enabling computers \n",
    "to understand, interpret, and generate human language. It combines computational linguistics—rule-based modeling \n",
    "of human language—with statistical, machine learning, and deep learning models. Together, these technologies \n",
    "enable computers to process human language in the form of text or voice data and to ‘understand’ its full \n",
    "meaning, complete with the speaker’s or writer’s intent and sentiment. NLP drives computer programs that \n",
    "translate text from one language to another, respond to spoken commands, and summarize large volumes of text \n",
    "rapidly—even in real time. There's a high likelihood you’ve interacted with NLP in the form of voice-operated \n",
    "GPS systems, digital assistants, speech-to-text dictation software, customer service chatbots, and other \n",
    "consumer conveniences. But NLP also plays a growing role in enterprise solutions that help streamline business \n",
    "operations, increase employee productivity, and simplify mission-critical business processes.\n",
    "\"\"\"\n",
    "\n",
    "if summarizer:\n",
    "    print(\"Original Text Length:\", len(long_text))\n",
    "    # Generate summary (adjust max/min length as needed)\n",
    "    try:\n",
    "        summary = summarizer(long_text, max_length=60, min_length=20, do_sample=False)\n",
    "        print(\"\\nGenerated Summary:\")\n",
    "        print(summary[0]['summary_text'])\n",
    "        print(\"Summary Length:\", len(summary[0]['summary_text']))\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during summarization: {e}\")\n",
    "        print(\"This might be due to model download issues or resource constraints.\")\n",
    "else:\n",
    "    print(\"Skipping summarization execution as the pipeline could not be initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "Abstractive summarization models, often based on Transformers, can generate fluent, concise summaries that are not just copied sentences. The quality depends heavily on the model and the input text complexity.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- What are the potential risks of relying on automated summaries (e.g., loss of nuance, factual inaccuracies)?\n",
    "- In which business scenarios would extractive summarization be preferred over abstractive, and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Machine Translation\n",
    "\n",
    "**Definition:** \n",
    "Machine translation automatically converts text from one language to another.\n",
    "\n",
    "**Business Application:** \n",
    "Translating product documentation for global markets, providing multilingual customer support, analyzing foreign language market reports.\n",
    "\n",
    "**Before the Demo Question:** \n",
    "- How does language act as a barrier in your business operations or expansion plans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo using Hugging Face Transformers for Translation\n",
    "# Note: Requires internet connection and installing transformers & pytorch/tensorflow\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    # Initialize translation pipeline (e.g., English to French)\n",
    "    # Using a smaller T5 model for efficiency\n",
    "    translator = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "except ImportError:\n",
    "    print(\"Could not import transformers. Please install it: pip install transformers torch\")\n",
    "    translator = None\n",
    "except Exception as e:\n",
    "    print(f\"Could not load translation model. Error: {e}\")\n",
    "    print(\"Skipping translation demo. Ensure 'transformers' and a backend (torch/tensorflow) are installed.\")\n",
    "    translator = None\n",
    "\n",
    "text_to_translate = \"Natural Language Processing is a fascinating field with many business applications.\"\n",
    "\n",
    "if translator:\n",
    "    print(\"Original English Text:\")\n",
    "    print(text_to_translate)\n",
    "\n",
    "    # Perform translation\n",
    "    try:\n",
    "        translation = translator(text_to_translate, max_length=100)\n",
    "        print(\"\\nTranslated French Text:\")\n",
    "        print(translation[0]['translation_text'])\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during translation: {e}\")\n",
    "        print(\"This might be due to model download issues or resource constraints.\")\n",
    "else:\n",
    "    print(\"Skipping translation execution as the pipeline could not be initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "Modern machine translation models (often sequence-to-sequence Transformers) can produce remarkably fluent translations for common language pairs. However, quality can vary for less common languages or highly technical/idiomatic text.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- When is machine translation 'good enough' for business use, and when is human translation still essential?\n",
    "- How can businesses evaluate the quality of machine translation output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Introduction to Transformers and Advanced Models\n",
    "\n",
    "**Definition:** \n",
    "Transformers are a type of deep learning architecture that has revolutionized NLP. Models like BERT, GPT, RoBERTa, and T5 use attention mechanisms to weigh the importance of different words in a sequence, leading to state-of-the-art performance on many tasks.\n",
    "\n",
    "**Business Application:** \n",
    "Powering sophisticated chatbots, advanced search engines that understand query intent, highly accurate sentiment analysis, and realistic text generation.\n",
    "\n",
    "**Before the Demo Question:** \n",
    "- Have you interacted with AI systems recently (like ChatGPT or advanced search engines) that seem to understand language much better than older systems? What makes them feel different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Using a pre-trained BERT model for Fill-Mask task (predicting masked words)\n",
    "# Note: Requires internet connection and installing transformers & pytorch/tensorflow\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "except ImportError:\n",
    "    print(\"Could not import transformers. Please install it: pip install transformers torch\")\n",
    "    unmasker = None\n",
    "except Exception as e:\n",
    "    print(f\"Could not load fill-mask model. Error: {e}\")\n",
    "    print(\"Skipping fill-mask demo. Ensure 'transformers' and a backend (torch/tensorflow) are installed.\")\n",
    "    unmasker = None\n",
    "\n",
    "masked_sentence = \"Stockholm is the capital of [MASK].\"\n",
    "\n",
    "if unmasker:\n",
    "    print(f\"Sentence: {masked_sentence}\\n\")\n",
    "    print(\"BERT's Top Predictions for [MASK]:\")\n",
    "    try:\n",
    "        predictions = unmasker(masked_sentence)\n",
    "        # The output format can sometimes be a list of lists, handle accordingly\n",
    "        if predictions and isinstance(predictions[0], list):\n",
    "            predictions = predictions[0]\n",
    "            \n",
    "        for i, pred in enumerate(predictions):\n",
    "             if isinstance(pred, dict):\n",
    "                 print(f\" {i+1}. {pred['token_str']} (Score: {pred['score']:.4f})\")\n",
    "             else:\n",
    "                 print(f\"Unexpected prediction format item: {pred}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during fill-mask prediction: {e}\")\n",
    "        print(\"This might be due to model download issues or resource constraints.\")\n",
    "else:\n",
    "    print(\"Skipping fill-mask execution as the pipeline could not be initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "This demo shows BERT's ability to understand context. By looking at the surrounding words ('Stockholm', 'capital'), it correctly predicts 'sweden' with high confidence. This contextual understanding is a key strength of Transformer models.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- How does the 'attention mechanism' in Transformers help them understand context better than older models like RNNs or LSTMs?\n",
    "- What are the computational costs and data requirements associated with training large Transformer models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fine-tuning Pre-trained Models\n",
    "\n",
    "**Definition:** \n",
    "Fine-tuning involves taking a large, pre-trained language model (like BERT or GPT) that has learned general language patterns from massive datasets, and further training it on a smaller, task-specific dataset. This adapts the model to a particular domain or task (like classifying legal documents or analyzing medical notes).\n",
    "\n",
    "**Business Application:** \n",
    "Adapting a general sentiment analysis model to understand industry-specific jargon in financial news, or training a chatbot on company-specific FAQs.\n",
    "\n",
    "**Before the Demo Question:** \n",
    "- Why might a general-purpose language model struggle with highly specialized text (e.g., scientific papers, legal contracts)? How could fine-tuning help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Demo: Fine-tuning for Sentiment Classification (Illustrative)\n",
    "# Note: Actual fine-tuning requires significant setup, data, and compute resources.\n",
    "# This cell provides a conceptual overview using Hugging Face trainer API structure.\n",
    "\n",
    "# --- This is PSEUDOCODE / CONCEPTUAL --- \n",
    "'''\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset # Example library to handle datasets\n",
    "\n",
    "# 1. Load a pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\" \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3) # e.g., positive, negative, neutral\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Load your task-specific dataset (e.g., our product reviews)\n",
    "# Assume 'dataset' has 'train' and 'test' splits with 'text' and 'label' columns\n",
    "# dataset = load_dataset('your_sentiment_dataset_script.py') \n",
    "# Example using our previous DataFrame (needs conversion to Hugging Face Dataset object)\n",
    "# from datasets import Dataset\n",
    "# hf_dataset = Dataset.from_pandas(df_reviews[['processed_text', 'sentiment']]) \n",
    "# # Map labels to integers\n",
    "# label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "# def map_labels(example):\n",
    "#     example['label'] = label_map[example['sentiment']]\n",
    "#     return example\n",
    "# hf_dataset = hf_dataset.map(map_labels)\n",
    "# # Tokenize\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True)\n",
    "# tokenized_datasets = hf_dataset.map(tokenize_function, batched=True)\n",
    "# # Split (if not already done)\n",
    "# tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "\n",
    "# 3. Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs (usually small for fine-tuning)\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    warmup_steps=10,                 # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"epoch\"      # Evaluate each epoch\n",
    ")\n",
    "\n",
    "# 4. Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"], # Placeholder\n",
    "    eval_dataset=tokenized_datasets[\"test\"]   # Placeholder\n",
    "    # compute_metrics=compute_accuracy_metric # Function to compute metrics\n",
    ")\n",
    "\n",
    "# 5. Start Fine-tuning\n",
    "# trainer.train()\n",
    "\n",
    "# 6. Evaluate the fine-tuned model\n",
    "# trainer.evaluate()\n",
    "\n",
    "# 7. Save the fine-tuned model\n",
    "# trainer.save_model(\"./fine_tuned_sentiment_model\")\n",
    "'''\n",
    "\n",
    "print(\"This cell contains conceptual code for fine-tuning.\")\n",
    "print(\"Actual execution requires a specific dataset, environment setup, and significant compute time.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "Fine-tuning allows leveraging the power of large pre-trained models without the need to train them from scratch, making state-of-the-art NLP accessible for specific business problems with moderate amounts of labeled data.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- What are the key steps involved in preparing a dataset for fine-tuning a Transformer model?\n",
    "- How does fine-tuning compare to traditional machine learning approaches (like those used in the sentiment analysis case study) in terms of performance, data needs, and complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prompting Language Models - Assignment\n",
    "\n",
    "**Definition:** \n",
    "Prompting involves crafting specific text inputs (prompts) to guide large language models (LLMs like GPT-3/4) to perform desired tasks without explicit fine-tuning. The way a prompt is phrased significantly influences the quality and relevance of the model's output.\n",
    "\n",
    "**Business Application:** \n",
    "Using LLMs for rapid prototyping of NLP tasks like text generation (writing marketing copy), summarization, question answering, and simple classification by designing effective prompts.\n",
    "\n",
    "**Assignment Task:**\n",
    "Your final assignment is to practice prompt engineering for common business tasks. In the code cell below, you will:\n",
    "1.  **Define Prompts:** Write clear and effective prompts for the following three scenarios:\n",
    "    * **Scenario A (Email Drafting):** Generate a short follow-up email to a potential client after a meeting. Mention that you enjoyed the conversation about [mention a specific topic, e.g., 'their supply chain needs'] and suggest scheduling a brief call next week to discuss next steps.\n",
    "    * **Scenario B (Summarization):** Summarize the key decisions made in a hypothetical meeting described in the `meeting_notes` variable (provided in the code cell). The summary should be 2-3 bullet points.\n",
    "    * **Scenario C (Classification):** Classify customer feedback messages (provided in the `feedback_list` variable) into one of three categories: 'Bug Report', 'Feature Request', or 'General Inquiry'. Use a few-shot prompting approach (provide 1-2 examples within the prompt).\n",
    "2.  **Use the LLM Function:** Use the provided conceptual function `llm_generate(prompt, max_tokens)` to generate text based on your prompts.\n",
    "3.  **Print Results:** Print the generated text for each scenario clearly.\n",
    "\n",
    "**Goal:** Successfully generate relevant and coherent text for each business scenario by crafting effective prompts. This exercise focuses on the prompt design, not the underlying LLM technology (which is simulated here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Demo & Assignment: Prompting Language Models\n",
    "\n",
    "# --- Provided Conceptual LLM Function (Simulated) --- \n",
    "# This function simulates calling a large language model.\n",
    "# In a real scenario, this would involve API calls to services like OpenAI, Anthropic, etc.\n",
    "# For this assignment, it returns predefined placeholder text based on keywords in the prompt.\n",
    "def llm_generate(prompt, max_tokens=100):\n",
    "    #Simulates generating text from a prompt.\n",
    "    prompt_lower = prompt.lower()\n",
    "    print(f\"\\n--- Simulating LLM Call with max_tokens={max_tokens} ---\")\n",
    "    # Simple keyword-based simulation\n",
    "    if \"follow-up email\" in prompt_lower and \"client\" in prompt_lower:\n",
    "        return \"Subject: Following Up\\n\\nHi [Client Name],\\nIt was great speaking with you about [Specific Topic Mentioned]. Would you be available for a quick call next week to discuss next steps?\\n\\nBest regards,\\n[Your Name]\"\n",
    "    elif \"summarize\" in prompt_lower and \"meeting notes\" in prompt_lower:\n",
    "        return \"- Decision 1: Approved the Q3 budget.\\n- Decision 2: Agreed to postpone the product launch to October.\\n- Action Item: Marketing team to revise launch plan.\"\n",
    "    elif \"classify\" in prompt_lower and \"customer feedback\" in prompt_lower:\n",
    "        # Simulate classification based on the last review in a few-shot prompt\n",
    "        if \"doesn't load\" in prompt_lower:\n",
    "            return \"Bug Report\"\n",
    "        elif \"integrate with calendar\" in prompt_lower:\n",
    "            return \"Feature Request\"\n",
    "        else:\n",
    "            return \"General Inquiry\" # Default fallback\n",
    "    else:\n",
    "        return f\"[Simulated LLM Response for prompt starting with: '{prompt[:50]}...']\"\n",
    "\n",
    "# --- Provided Data for Scenarios --- \n",
    "meeting_notes = \"\"\"\n",
    "Meeting Minutes - Project Phoenix - April 1, 2025\n",
    "Attendees: Alice, Bob, Charlie\n",
    "Discussion: Reviewed Q3 budget proposal. Alice presented revised figures. Bob raised concerns about marketing spend. After discussion, the budget was approved with minor adjustments.\n",
    "Product Launch: Discussed timeline for the new 'Phoenix' software. Charlie indicated development is slightly behind schedule. Agreed to postpone the official launch from September to October 15th.\n",
    "Action Items: Marketing team needs to update the launch communications plan based on the new date.\n",
    "\"\"\"\n",
    "\n",
    "feedback_list = [\n",
    "    \"The login page doesn't load on Firefox.\",\n",
    "    \"Could you please add an option to integrate with Google Calendar?\",\n",
    "    \"How do I reset my password?\"\n",
    "]\n",
    "\n",
    "# --- Assignment: Write Your Prompts and Generate Text --- \n",
    "\n",
    "# Scenario A: Email Drafting\n",
    "print(\"\\n--- Scenario A: Email Drafting ---\")\n",
    "prompt_email = \"\"\" \n",
    "# --- YOUR PROMPT A HERE --- \n",
    "# Write a prompt to generate a follow-up email to a potential client \n",
    "# after a meeting about their 'supply chain needs'. Suggest a call next week.\n",
    "\"\"\"\n",
    "# generated_email = llm_generate(prompt_email, max_tokens=150)\n",
    "# print(\"Generated Email:\")\n",
    "# print(generated_email)\n",
    "\n",
    "# Scenario B: Summarization\n",
    "print(\"\\n--- Scenario B: Summarization ---\")\n",
    "prompt_summary = f\"\"\" \n",
    "# --- YOUR PROMPT B HERE --- \n",
    "# Write a prompt to summarize the key decisions from the meeting_notes below \n",
    "# into 2-3 bullet points.\n",
    "# Meeting Notes:\n",
    "# {meeting_notes}\n",
    "\"\"\"\n",
    "# generated_summary = llm_generate(prompt_summary, max_tokens=75)\n",
    "# print(\"Generated Summary:\")\n",
    "# print(generated_summary)\n",
    "\n",
    "# Scenario C: Classification (Few-Shot)\n",
    "print(\"\\n--- Scenario C: Classification ---\")\n",
    "# You need to classify the items in 'feedback_list'. \n",
    "# Craft ONE prompt that includes examples (few-shot) to classify the LAST item.\n",
    "prompt_classify = \"\"\"\n",
    "# --- YOUR PROMPT C HERE --- \n",
    "# Write a prompt to classify customer feedback into 'Bug Report', 'Feature Request', \n",
    "# or 'General Inquiry'. Include examples for the first two items in feedback_list \n",
    "# and ask the model to classify the third item.\n",
    "# Example Format:\n",
    "# Classify the following customer feedback:\n",
    "# Feedback: '[Feedback 1 Text]' -> Category: [Correct Category 1]\n",
    "# Feedback: '[Feedback 2 Text]' -> Category: [Correct Category 2]\n",
    "# Feedback: '[Feedback 3 Text]' -> Category: \n",
    "\"\"\"\n",
    "# generated_classification = llm_generate(prompt_classify, max_tokens=10)\n",
    "# print(f\"Feedback to Classify: '{feedback_list[2]}' \")\n",
    "# print(f\"Generated Classification:\")\n",
    "# print(generated_classification)\n",
    "\n",
    "print(\"\\nAssignment section complete. Make sure you have written prompts and uncommented the function calls.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** \n",
    "Prompt engineering is becoming a crucial skill. By providing clear instructions, context, and sometimes examples (few-shot prompting), users can elicit complex behaviors from LLMs without traditional coding or model training. The effectiveness depends heavily on the LLM's capabilities and the quality of the prompt.\n",
    "\n",
    "**Discussion Questions:** \n",
    "- What makes a 'good' prompt? What are some common pitfalls in prompt design?\n",
    "- How does prompting compare to fine-tuning in terms of cost, effort, performance, and control over the output?\n",
    "- What are the ethical considerations when using LLMs for tasks like text generation or automated decision-making?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "This notebook provided an overview of key Natural Language Processing concepts and techniques, from fundamental text preprocessing to advanced Transformer models and prompting.\n",
    "\n",
    "We explored:\n",
    "* **Core NLP Tasks:** Tokenization, preprocessing, vectorization (TF-IDF), Named Entity Recognition, Topic Modeling, Summarization, and Translation.\n",
    "* **Sentiment Analysis Case Study:** A practical walkthrough of building, evaluating, and applying a sentiment classifier for business insights, including a hands-on challenge.\n",
    "* **Modern NLP:** Introduction to Transformer architectures (like BERT) and their capabilities, the concept of fine-tuning pre-trained models, and the power of prompting Large Language Models.\n",
    "\n",
    "Throughout the notebook, we emphasized the **business applications** of these techniques, demonstrating how NLP can drive value by extracting insights from text, automating tasks, and improving customer experiences.\n",
    "\n",
    "The field of NLP is rapidly evolving, particularly with the advent of large language models. Understanding these fundamental concepts and practical implementations provides a strong foundation for leveraging NLP in various business contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
