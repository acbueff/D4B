{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Sentiment Analysis for Product Reviews\n",
       "\n",
       "This notebook demonstrates a practical implementation of sentiment analysis for product reviews, inspired by Amazon's approach. We'll build a simple sentiment classifier that can categorize reviews as positive, negative, or neutral.\n",
       "\n",
       "## Business Context\n",
       "\n",
       "Amazon processes millions of customer reviews daily. Automatically analyzing the sentiment of these reviews allows them to:\n",
       "- Identify products with quality issues\n",
       "- Highlight highly-rated products\n",
       "- Track customer satisfaction trends\n",
       "- Feed data into recommendation systems\n",
       "\n",
       "By the end of this notebook, you'll understand how sentiment analysis works and how to implement a basic version yourself."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup and Data Loading\n",
       "\n",
       "First, let's import the necessary libraries and load a sample dataset of product reviews."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Import libraries\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
       "from sklearn.naive_bayes import MultinomialNB\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "from sklearn.pipeline import Pipeline\n",
       "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
       "\n",
       "import nltk\n",
       "from nltk.corpus import stopwords\n",
       "from nltk.stem import WordNetLemmatizer\n",
       "\n",
       "# Download required NLTK data\n",
       "nltk.download('stopwords')\n",
       "nltk.download('wordnet')\n",
       "nltk.download('punkt')\n",
       "\n",
       "# Set plot style\n",
       "sns.set_style(\"whitegrid\")\n",
       "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Load sample dataset (in practice, this would be your own data)\n",
       "# For this demo, we'll use a subset of Amazon product reviews\n",
       "# The dataset format should have at least 'review_text' and 'rating' columns\n",
       "\n",
       "# Sample code to load data:\n",
       "# df = pd.read_csv('amazon_reviews_sample.csv')\n",
       "\n",
       "# For demonstration, we'll create a small sample dataset\n",
       "data = {\n",
       "    'review_text': [\n",
       "        \"This product exceeded my expectations. The quality is outstanding and it works perfectly.\",\n",
       "        \"Absolutely terrible. Broke after two days and customer service was unhelpful.\",\n",
       "        \"Good value for the price, but the instructions were confusing.\",\n",
       "        \"Love this product! Would definitely buy again and recommend to friends.\",\n",
       "        \"Mediocre at best. Does the job but nothing special.\",\n",
       "        \"Waste of money. Don't buy this product.\",\n",
       "        \"Decent quality but shipping took too long.\",\n",
       "        \"The perfect solution to my problem. Extremely satisfied with this purchase.\",\n",
       "        \"Received a damaged item. Very disappointed.\",\n",
       "        \"Not what I expected based on the description, but it's okay.\"\n",
       "    ],\n",
       "    'rating': [5, 1, 3, 5, 3, 1, 3, 5, 1, 2]\n",
       "}\n",
       "\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "# Display the first few rows\n",
       "df.head()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Data Preparation\n",
       "\n",
       "Now, let's prepare our data for sentiment analysis. We'll:\n",
       "1. Convert ratings to sentiment categories\n",
       "2. Create a text preprocessing function\n",
       "3. Split the data into training and testing sets"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Convert ratings to sentiment categories\n",
       "# This is a simplified approach - in real applications, you might use the original ratings\n",
       "def convert_rating_to_sentiment(rating):\n",
       "    if rating >= 4:  # 4-5 stars\n",
       "        return 'positive'\n",
       "    elif rating <= 2:  # 1-2 stars\n",
       "        return 'negative'\n",
       "    else:  # 3 stars\n",
       "        return 'neutral'\n",
       "\n",
       "df['sentiment'] = df['rating'].apply(convert_rating_to_sentiment)\n",
       "\n",
       "# Display the sentiment distribution\n",
       "print(\"Sentiment distribution:\")\n",
       "print(df['sentiment'].value_counts())\n",
       "\n",
       "# Plot the sentiment distribution\n",
       "plt.figure(figsize=(8, 6))\n",
       "df['sentiment'].value_counts().plot(kind='bar', color=['green', 'gray', 'red'])\n",
       "plt.title('Sentiment Distribution')\n",
       "plt.ylabel('Count')\n",
       "plt.xlabel('Sentiment')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Create a text preprocessing function\n",
       "def preprocess_text(text):\n",
       "    \"\"\"Function to preprocess text by lowercasing, removing punctuation, and lemmatizing\"\"\"\n",
       "    # Initialize lemmatizer\n",
       "    lemmatizer = WordNetLemmatizer()\n",
       "    \n",
       "    # Get English stopwords\n",
       "    stop_words = set(stopwords.words('english'))\n",
       "    \n",
       "    # Tokenize, lowercase, and remove punctuation\n",
       "    tokens = nltk.word_tokenize(text.lower())\n",
       "    tokens = [token for token in tokens if token.isalpha()]\n",
       "    \n",
       "    # Remove stopwords and lemmatize\n",
       "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
       "    \n",
       "    # Join tokens back into a string\n",
       "    return ' '.join(tokens)\n",
       "\n",
       "# Apply preprocessing to the review text\n",
       "df['processed_text'] = df['review_text'].apply(preprocess_text)\n",
       "\n",
       "# Display original and processed text for comparison\n",
       "comparison = pd.DataFrame({\n",
       "    'Original Text': df['review_text'],\n",
       "    'Processed Text': df['processed_text'],\n",
       "    'Sentiment': df['sentiment']\n",
       "})\n",
       "comparison.head(3)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Split the data into training and testing sets\n",
       "X_train, X_test, y_train, y_test = train_test_split(\n",
       "    df['processed_text'], \n",
       "    df['sentiment'], \n",
       "    test_size=0.3,  # 30% for testing\n",
       "    random_state=42,  # For reproducibility\n",
       "    stratify=df['sentiment']  # Maintain sentiment distribution\n",
       ")\n",
       "\n",
       "print(f\"Training data size: {X_train.shape[0]} reviews\")\n",
       "print(f\"Testing data size: {X_test.shape[0]} reviews\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Feature Engineering\n",
       "\n",
       "Now we'll convert our text data into numerical features that can be used by machine learning algorithms. We'll use two popular approaches:\n",
       "1. Bag of Words (CountVectorizer)\n",
       "2. TF-IDF (Term Frequency-Inverse Document Frequency)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Bag of Words approach\n",
       "count_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Include both unigrams and bigrams\n",
       "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
       "\n",
       "# Show the vocabulary size\n",
       "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
       "\n",
       "# Show the feature names (words) in the vocabulary\n",
       "print(\"\\nFirst 20 features in the vocabulary:\")\n",
       "print(list(count_vectorizer.vocabulary_.keys())[:20])\n",
       "\n",
       "# Show the sparse matrix shape\n",
       "print(f\"\\nFeature matrix shape: {X_train_counts.shape}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# TF-IDF approach\n",
       "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
       "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
       "\n",
       "# Convert to DataFrame for better visualization\n",
       "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
       "df_tfidf = pd.DataFrame(X_train_tfidf.toarray(), columns=feature_names)\n",
       "\n",
       "# Show a sample of the TF-IDF matrix\n",
       "print(\"Sample of TF-IDF features for the first review:\")\n",
       "# Get non-zero features for the first document\n",
       "non_zero_cols = df_tfidf.iloc[0].loc[df_tfidf.iloc[0] > 0].sort_values(ascending=False)\n",
       "print(non_zero_cols.head(10))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Building the Sentiment Analysis Model\n",
       "\n",
       "We'll build and compare two models using scikit-learn's Pipeline:\n",
       "1. Naive Bayes with Count Vectorizer\n",
       "2. Logistic Regression with TF-IDF"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Model 1: Naive Bayes with Count Vectorizer\n",
       "nb_pipeline = Pipeline([\n",
       "    ('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
       "    ('classifier', MultinomialNB())\n",
       "])\n",
       "\n",
       "# Train the model\n",
       "nb_pipeline.fit(X_train, y_train)\n",
       "\n",
       "# Make predictions on the test set\n",
       "y_pred_nb = nb_pipeline.predict(X_test)\n",
       "\n",
       "# Evaluate the model\n",
       "print(\"Naive Bayes Model Performance:\")\n",
       "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
       "print(\"\\nClassification Report:\")\n",
       "print(classification_report(y_test, y_pred_nb))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Model 2: Logistic Regression with TF-IDF\n",
       "lr_pipeline = Pipeline([\n",
       "    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),\n",
       "    ('classifier', LogisticRegression(max_iter=1000))\n",
       "])\n",
       "\n",
       "# Train the model\n",
       "lr_pipeline.fit(X_train, y_train)\n",
       "\n",
       "# Make predictions on the test set\n",
       "y_pred_lr = lr_pipeline.predict(X_test)\n",
       "\n",
       "# Evaluate the model\n",
       "print(\"Logistic Regression Model Performance:\")\n",
       "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
       "print(\"\\nClassification Report:\")\n",
       "print(classification_report(y_test, y_pred_lr))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Visualize confusion matrices for both models\n",
       "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
       "\n",
       "# Naive Bayes confusion matrix\n",
       "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
       "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', xticklabels=nb_pipeline.classes_, \n",
       "            yticklabels=nb_pipeline.classes_, ax=axes[0])\n",
       "axes[0].set_ylabel('True Sentiment')\n",
       "axes[0].set_xlabel('Predicted Sentiment')\n",
       "axes[0].set_title('Naive Bayes Model')\n",
       "\n",
       "# Logistic Regression confusion matrix\n",
       "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
       "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=lr_pipeline.classes_, \n",
       "            yticklabels=lr_pipeline.classes_, ax=axes[1])\n",
       "axes[1].set_ylabel('True Sentiment')\n",
       "axes[1].set_xlabel('Predicted Sentiment')\n",
       "axes[1].set_title('Logistic Regression Model')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Feature Importance Analysis\n",
       "\n",
       "Let's examine which words are most indicative of positive and negative sentiment:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Extract coefficients from logistic regression model\n",
       "vectorizer = lr_pipeline.named_steps['vectorizer']\n",
       "classifier = lr_pipeline.named_steps['classifier']\n",
       "\n",
       "# Get feature names\n",
       "feature_names = vectorizer.get_feature_names_out()\n",
       "\n",
       "# For multiclass, we need to extract coefficients for each class\n",
       "coefs = classifier.coef_\n",
       "\n",
       "# Create a DataFrame with coefficients\n",
       "coefficients_df = pd.DataFrame()\n",
       "\n",
       "for i, sentiment in enumerate(classifier.classes_):\n",
       "    coefficients_df[f'{sentiment}_coef'] = coefs[i]\n",
       "\n",
       "coefficients_df['feature'] = feature_names\n",
       "\n",
       "# Sort features by their importance for each sentiment\n",
       "top_features = {}\n",
       "for sentiment in classifier.classes_:\n",
       "    # Get top positive coefficients\n",
       "    top_positive = coefficients_df.sort_values(f'{sentiment}_coef', ascending=False)\n",
       "    top_features[f'top_{sentiment}'] = top_positive[['feature', f'{sentiment}_coef']].head(10)\n",
       "\n",
       "# Display top features for each sentiment\n",
       "for sentiment, features_df in top_features.items():\n",
       "    print(f\"\\n{sentiment.replace('top_', 'Top 10 Features for ')} Sentiment:\")\n",
       "    print(features_df)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Visualize top words for positive and negative sentiment\n",
       "fig, axes = plt.subplots(1, len(classifier.classes_), figsize=(18, 6))\n",
       "\n",
       "for i, sentiment in enumerate(classifier.classes_):\n",
       "    # Get the top features DataFrame\n",
       "    top_df = top_features[f'top_{sentiment}']\n",
       "    \n",
       "    # Create horizontal bar chart\n",
       "    ax = axes[i]\n",
       "    ax.barh(top_df['feature'], top_df[f'{sentiment}_coef'])\n",
       "    ax.set_title(f'Top 10 Features for {sentiment.capitalize()} Sentiment')\n",
       "    ax.set_xlabel('Coefficient Value')\n",
       "    \n",
       "    # Invert y-axis to have highest coefficient at the top\n",
       "    ax.invert_yaxis()\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Making Predictions on New Reviews\n",
       "\n",
       "Now let's use our model to analyze sentiment in new, unseen reviews:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Define a function to predict sentiment with probability\n",
       "def predict_sentiment(review_text, model=lr_pipeline):\n",
       "    \"\"\"Predict sentiment of a review with probability scores\"\"\"\n",
       "    # Get the prediction\n",
       "    sentiment = model.predict([review_text])[0]\n",
       "    \n",
       "    # Get probability scores for each class\n",
       "    proba = model.predict_proba([review_text])[0]\n",
       "    proba_dict = dict(zip(model.classes_, proba))\n",
       "    \n",
       "    return {\n",
       "        'text': review_text,\n",
       "        'predicted_sentiment': sentiment,\n",
       "        'confidence': proba_dict[sentiment],\n",
       "        'probability_scores': proba_dict\n",
       "    }\n",
       "\n",
       "# Test on new reviews\n",
       "new_reviews = [\n",
       "    \"This product is amazing! It works exactly as described and the quality is excellent.\",\n",
       "    \"I'm very disappointed with this purchase. It broke after just a week of use.\",\n",
       "    \"The product is okay. Not great, not terrible. It does what it's supposed to do.\",\n",
       "    \"While the shipping was slow, the product itself is pretty good.\",\n",
       "    \"This is the worst purchase I've ever made. Avoid at all costs!\"\n",
       "]\n",
       "\n",
       "# Analyze each review\n",
       "for review in new_reviews:\n",
       "    result = predict_sentiment(review)\n",
       "    print(f\"\\nReview: {result['text']}\")\n",
       "    print(f\"Predicted sentiment: {result['predicted_sentiment']} (Confidence: {result['confidence']:.2f})\")\n",
       "    print(\"Probability scores:\")\n",
       "    for sentiment, score in result['probability_scores'].items():\n",
       "        print(f\"  {sentiment}: {score:.2f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Business Application Demonstration\n",
       "\n",
       "Let's simulate how Amazon might use sentiment analysis in their business processes:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Simulate a batch of new product reviews\n",
       "product_reviews = [\n",
       "    {\"product_id\": \"B001\", \"product_name\": \"Wireless Headphones\", \"review\": \"Excellent sound quality and battery life. Very comfortable to wear.\"},\n",
       "    {\"product_id\": \"B001\", \"product_name\": \"Wireless Headphones\", \"review\": \"Decent headphones but they hurt my ears after an hour.\"},\n",
       "    {\"product_id\": \"B001\", \"product_name\": \"Wireless Headphones\", \"review\": \"Battery drains too quickly. Not worth the money.\"},\n",
       "    {\"product_id\": \"B002\", \"product_name\": \"Smart Watch\", \"review\": \"Love this watch! The fitness tracking is accurate and the battery lasts for days.\"},\n",
       "    {\"product_id\": \"B002\", \"product_name\": \"Smart Watch\", \"review\": \"Screen cracked after just one week. Poor quality.\"},\n",
       "    {\"product_id\": \"B002\", \"product_name\": \"Smart Watch\", \"review\": \"Good features but the app is buggy.\"},\n",
       "    {\"product_id\": \"B003\", \"product_name\": \"Bluetooth Speaker\", \"review\": \"Amazing sound for such a small speaker. Highly recommend!\"},\n",
       "    {\"product_id\": \"B003\", \"product_name\": \"Bluetooth Speaker\", \"review\": \"Decent speaker but the connection keeps dropping.\"},\n",
       "    {\"product_id\": \"B003\", \"product_name\": \"Bluetooth Speaker\", \"review\": \"Terrible build quality. It stopped working after a month.\"}\n",
       "]\n",
       "\n",
       "# Create a DataFrame\n",
       "reviews_df = pd.DataFrame(product_reviews)\n",
       "\n",
       "# Add sentiment predictions\n",
       "reviews_df['sentiment'] = reviews_df['review'].apply(lambda x: predict_sentiment(x)['predicted_sentiment'])\n",
       "reviews_df['confidence'] = reviews_df['review'].apply(lambda x: predict_sentiment(x)['confidence'])\n",
       "\n",
       "# Display the results\n",
       "print(\"Product Reviews with Sentiment Analysis:\")\n",
       "reviews_df"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Analyze sentiment by product\n",
       "product_sentiment = reviews_df.groupby(['product_id', 'product_name', 'sentiment']).size().unstack(fill_value=0)\n",
       "\n",
       "# Calculate sentiment percentages\n",
       "product_sentiment_pct = product_sentiment.div(product_sentiment.sum(axis=1), axis=0) * 100\n",
       "\n",
       "# Create a sentiment score (-100 to +100 scale)\n",
       "def calculate_sentiment_score(row):\n",
       "    # If columns don't exist, use 0\n",
       "    positive = row.get('positive', 0)\n",
       "    negative = row.get('negative', 0)\n",
       "    neutral = row.get('neutral', 0)\n",
       "    \n",
       "    # Calculate weighted score\n",
       "    total = positive + negative + neutral\n",
       "    if total == 0:\n",
       "        return 0\n",
       "    return ((positive * 100) - (negative * 100)) / total\n",
       "\n",
       "# Add sentiment score\n",
       "product_sentiment['sentiment_score'] = product_sentiment.apply(calculate_sentiment_score, axis=1)\n",
       "\n",
       "# Display product sentiment analysis\n",
       "print(\"Product Sentiment Analysis:\")\n",
       "print(product_sentiment)\n",
       "print(\"\\nProduct Sentiment Percentages:\")\n",
       "print(product_sentiment_pct)\n",
       "\n",
       "# Visualize the sentiment distribution by product\n",
       "plt.figure(figsize=(12, 6))\n",
       "product_sentiment_pct.drop(columns=['sentiment_score'] if 'sentiment_score' in product_sentiment_pct.columns else []).plot(\n",
       "    kind='bar', stacked=True, \n",
       "    color=['green', 'gray', 'red'] if all(col in product_sentiment_pct.columns for col in ['positive', 'neutral', 'negative']) \n",
       "    else None\n",
       ")\n",
       "plt.title('Sentiment Distribution by Product')\n",
       "plt.xlabel('Product')\n",
       "plt.ylabel('Percentage')\n",
       "plt.xticks(rotation=45)\n",
       "plt.legend(title='Sentiment')\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Identify common themes in negative reviews (simplified version of aspect-based sentiment analysis)\n",
       "negative_reviews = reviews_df[reviews_df['sentiment'] == 'negative']\n",
       "\n",
       "print(\"Negative Reviews Analysis:\")\n",
       "for _, row in negative_reviews.iterrows():\n",
       "    print(f\"\\nProduct: {row['product_name']} (ID: {row['product_id']})\")\n",
       "    print(f\"Review: {row['review']}\")\n",
       "    \n",
       "    # In a real system, this would use more sophisticated techniques\n",
       "    # For simplicity, we'll just check for common issue keywords\n",
       "    issues = []\n",
       "    if any(word in row['review'].lower() for word in ['battery', 'drain', 'charge', 'power']):\n",
       "        issues.append('Battery Issues')\n",
       "    if any(word in row['review'].lower() for word in ['broke', 'broken', 'crack', 'damage', 'quality']):\n",
       "        issues.append('Quality/Durability Issues')\n",
       "    if any(word in row['review'].lower() for word in ['app', 'software', 'bug', 'connection', 'bluetooth']):\n",
       "        issues.append('Software/Connectivity Issues')\n",
       "    if any(word in row['review'].lower() for word in ['uncomfortable', 'pain', 'hurt', 'fit']):\n",
       "        issues.append('Comfort Issues')\n",
       "        \n",
       "    if issues:\n",
       "        print(f\"Potential Issues: {', '.join(issues)}\")\n",
       "    else:\n",
       "        print(\"No specific issues identified.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Learning Challenge\n",
       "\n",
       "Now it's your turn to apply what you've learned! Try to complete the following tasks:\n",
       "\n",
       "1. Modify the preprocessing function to improve performance (e.g., handle emoticons, exclamation marks)\n",
       "2. Try a different classifier (e.g., SVM, Random Forest)\n",
       "3. Implement a function to extract the specific aspect being discussed in a review (aspect-based sentiment analysis)\n",
       "\n",
       "### Challenge Code Template"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# 1. Enhanced preprocessing function\n",
       "def enhanced_preprocess_text(text):\n",
       "    \"\"\"Enhanced preprocessing with emoticon handling and exclamation preservation\"\"\"\n",
       "    # Your implementation here\n",
       "    \n",
       "    return processed_text\n",
       "\n",
       "# 2. Try a different classifier\n",
       "# Your implementation here\n",
       "\n",
       "# 3. Aspect-based sentiment analysis\n",
       "def extract_aspects(review_text):\n",
       "    \"\"\"Extract aspects (product features) from review text and their sentiment\"\"\"\n",
       "    # Your implementation here\n",
       "    \n",
       "    return aspects"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9. Business Implications\n",
       "\n",
       "Let's discuss how sentiment analysis creates business value:\n",
       "\n",
       "1. **Product Development**: Identifying product issues from negative reviews helps prioritize improvements\n",
       "2. **Customer Service**: Automatically flagging strongly negative reviews for response\n",
       "3. **Marketing**: Extracting positive feedback for testimonials and marketing materials\n",
       "4. **Competitive Analysis**: Comparing sentiment across competing products\n",
       "5. **Recommendation Systems**: Incorporating sentiment to improve product recommendations\n",
       "\n",
       "For Amazon, sentiment analysis is not just a technical exercise but a core business capability that drives decisions across the organization. By automating the understanding of millions of customer opinions, they can identify trends, address issues, and improve the customer experience at scale."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "In this notebook, we've built a basic sentiment analysis system for product reviews. We've seen how to:\n",
       "\n",
       "1. Preprocess text data for sentiment analysis\n",
       "2. Convert text to numerical features using vectorization\n",
       "3. Train machine learning models to classify sentiment\n",
       "4. Evaluate model performance\n",
       "5. Apply the model to analyze new reviews\n",
       "6. Extract business insights from sentiment analysis\n",
       "\n",
       "While our simple model demonstrates the core concepts, production systems at companies like Amazon would include more advanced techniques such as:\n",
       "- Deep learning models (BERT, RoBERTa)\n",
       "- Aspect-based sentiment analysis\n",
       "- Multilingual support\n",
       "- Real-time processing pipelines\n",
       "\n",
       "However, the fundamental principles remain the same, and you now understand how sentiment analysis works and how it creates business value."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }