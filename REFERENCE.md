# Reference Document

Great, I will conduct in-depth research on business use cases for each AI topic in the course. I will focus on real-world applications from various industries, ensuring a balance between theoretical insights and hands-on practical applications.

For each lecture topic, I will:
- Identify practical business use cases from real companies.
- Propose detailed PowerPoint slide content, including both highly visual and explanatory versions.
- Develop Jupyter Notebook structures using PyTorch and Scikit-learn, including step-by-step implementation guides and learning challenges.

I will get back to you with structured findings and materials soon.

# Deep Learning

## Business Use Cases Overview
- **Medical Image Diagnosis (Healthcare):** Hospitals use deep learning to analyze medical scans (like X-rays, MRIs) for faster, more accurate diagnoses. For example, Moorfields Eye Hospital partnered with DeepMind to train an AI on thousands of retina scans; the system now detects over 50 eye diseases and recommends treatment referrals with expert-level accuracy ([Artificial intelligence equal to experts in detecting eye diseases | UCL News - UCL – University College London](https://www.ucl.ac.uk/news/2018/aug/artificial-intelligence-equal-experts-detecting-eye-diseases#:~:text=An%20artificial%20intelligence%20,Eye%20Hospital%20NHS%20Foundation%20Trust)) ([Artificial intelligence equal to experts in detecting eye diseases | UCL News - UCL – University College London](https://www.ucl.ac.uk/news/2018/aug/artificial-intelligence-equal-experts-detecting-eye-diseases#:~:text=Researchers%20hope%20the%20technology%20could,before%20irreversible%20damage%20sets%20in)). This helps flag urgent cases early, preventing blindness and easing doctors’ workload.  
- **Fraud Detection in Finance:** Banks and payment networks deploy deep neural networks to spot fraudulent transactions in real time. Visa’s “Deep Authorization” platform, for instance, uses a **deep learning RNN** model on petabytes of data to score each transaction’s risk instantaneously ([
	Visa - Visa’s Growing Services Business Infused with New AI-Powered Products
](https://investor.visa.com/news/news-details/2024/Visas-Growing-Services-Business-Infused-with-New-AI-Powered-Products/default.aspx#:~:text=,allowing%20issuers%20to%20simplify%20their)) ([
	Visa - Visa’s Growing Services Business Infused with New AI-Powered Products
](https://investor.visa.com/news/news-details/2024/Visas-Growing-Services-Business-Infused-with-New-AI-Powered-Products/default.aspx#:~:text=%2A%20Real,bad%20transactions%20before%20they%20happen)). Suspicious charges are blocked automatically, reducing losses and protecting customers without manual review delays.  
- **Predictive Maintenance (Manufacturing):** Industrial firms leverage deep learning on IoT sensor data to predict equipment failures. **GE** monitors turbines and machines with sensors feeding deep neural nets that learn normal vs. anomalous vibration patterns ([How GE Uses AI to Implement Predictive Maintenance in Its Manufacturing Plants](https://redresscompliance.com/how-ge-uses-ai-to-implement-predictive-maintenance-in-its-manufacturing-plants/#:~:text=GE%E2%80%99s%20AI%20systems%20process%20the,warning%20signs%20of%20potential%20failures)). The AI alerts technicians to issues before breakdowns occur ([How GE Uses AI to Implement Predictive Maintenance in Its Manufacturing Plants](https://redresscompliance.com/how-ge-uses-ai-to-implement-predictive-maintenance-in-its-manufacturing-plants/#:~:text=GE%20uses%20predictive%20maintenance%20to,maintenance%20and%20avoid%20costly%20breakdowns)), minimizing unplanned downtime, cutting maintenance costs, and improving safety.  
- **Product Recommendations (Retail/Entertainment):** E-commerce and streaming companies use deep learning to personalize content. **Netflix** employs deep neural networks for its recommendation engine, which suggests movies a user will love. This personalization is so effective it’s credited with saving Netflix over $1 billion a year by reducing customer churn ([Netflix Recommendation Engine Worth $1 Billion Per Year - Business Insider](https://www.businessinsider.com/netflix-recommendation-engine-worth-1-billion-per-year-2016-6#:~:text=And%20Netflix%20thinks%20it%E2%80%99s%20worth,more%20than%20%241B%20per%20year)). Amazon’s deep-learning-based recommender system drives ~35% of its e-commerce sales ([Increase revenue with AI-powered Recommendations - FACT-Finder](https://www.fact-finder.com/blog/product-recommendation/#:~:text=Increase%20revenue%20with%20AI,of%20its%20revenue%20from%20recommendations)), showing how deep learning adds major business value through increased engagement and sales.

## PowerPoint Slide Breakdown (Visual & Detailed Versions)
**Use Case 1 – Medical Image Diagnosis (Deep Learning in Healthcare)**  
- *Slide 1 (Intro)* – **Visual:** Title “AI Diagnoses Eye Disease” with a retina scan image. Minimal text: “Deep Learning Transforms Eye Care.” **Detailed:** Introduce Moorfields Eye Hospital case – rising scan volumes, need for automated analysis. Mention relevance: demonstrates deep learning’s power in healthcare diagnostics.  
- *Slide 2 (Business Problem)* – **Visual:** Graphic of overwhelmed doctors vs. AI assisting. Few words: “Too many scans, not enough specialists.” **Detailed:** Explain the context – millions suffer vision loss, experts can’t keep up with scan interpretation ([Artificial intelligence equal to experts in detecting eye diseases | UCL News - UCL – University College London](https://www.ucl.ac.uk/news/2018/aug/artificial-intelligence-equal-experts-detecting-eye-diseases#:~:text=and%20consultant%20ophthalmologist%20at%20Moorfields,can%20be%20devastating%20for%20patients)). Emphasize the business pain: potential delays in treatment, high costs of manual review, impact on patient outcomes.  
- *Slide 3 (AI Solution)* – **Visual:** Diagram of a CNN (convolutional neural network) analyzing an eye OCT scan, with output like “Referral: URGENT”. **Detailed:** Describe the deep learning solution: training a neural network on past labeled scans ([Artificial intelligence equal to experts in detecting eye diseases | UCL News - UCL – University College London](https://www.ucl.ac.uk/news/2018/aug/artificial-intelligence-equal-experts-detecting-eye-diseases#:~:text=The%20breakthrough%20research%2C%20published%20online,should%20be%20referred%20for%20care)). Note it learns to identify disease features and recommend referral action with 94% accuracy matching specialists ([DeepMind's AI detects over 50 eye diseases with 94% accuracy ...](https://www.healthcaredive.com/news/deepminds-ai-detects-over-50-eye-diseases-with-94-accuracy-study-shows-1/530125/#:~:text=DeepMind%27s%20AI%20detects%20over%2050,par%20with%20eye%20disease%20experts)). Highlight use of GPUs, large dataset, and integration into workflow.  
- *Slide 4 (Outcomes)* – **Visual:** Before/after chart – e.g. diagnosis wait times reduced. **Detailed:** Detail benefits: urgent cases prioritized, earlier treatment saves sight ([Artificial intelligence equal to experts in detecting eye diseases | UCL News - UCL – University College London](https://www.ucl.ac.uk/news/2018/aug/artificial-intelligence-equal-experts-detecting-eye-diseases#:~:text=Researchers%20hope%20the%20technology%20could,before%20irreversible%20damage%20sets%20in)). Efficiency gain – AI analyzes scans in seconds, freeing doctors for complex cases. Business value: improved patient care quality and hospital throughput. For a visual version, an infographic showing “40% faster diagnoses” or “X million scans processed by AI” would replace text.  
- *Slide 5 (Visual vs. Detailed)* – **Visual Deck:** Emphasize images of eyes, a happy patient, and simple captions (“Early detection = vision saved”). **Detailed Deck:** Include brief explanatory bullets on each slide (e.g., “Trained on 100,000+ scans; Achieved 94% referral accuracy ([DeepMind's AI detects over 50 eye diseases with 94% accuracy ...](https://www.healthcaredive.com/news/deepminds-ai-detects-over-50-eye-diseases-with-94-accuracy-study-shows-1/530125/#:~:text=DeepMind%27s%20AI%20detects%20over%2050,par%20with%20eye%20disease%20experts))”) and footnotes or speaker notes with additional context.  
- *Suggested Visuals:* Retina scan images, doctor–AI collaboration illustration, a flowchart of AI analysis pipeline, outcome metrics bar chart.

**Use Case 2 – Fraud Detection with Deep Learning (Finance)**  
- *Slide 1 (Intro)* – **Visual:** Title “Fighting Credit Card Fraud with AI” over a graphic of credit cards and a shield icon. **Detailed:** Introduce how deep learning applies to fraud detection at scale (Visa as example). Relevance: showcases DL handling big data and real-time decisions in finance.  
- *Slide 2 (Problem)* – **Visual:** Icon of an alert and money flying away – text “Fraud Losses $32B/yr”. **Detailed:** Describe business problem: huge volume of transactions, crafty fraudsters, traditional rules miss new fraud patterns. Financial institutions face fraud costs and need better detection without blocking legit purchases.  
- *Slide 3 (Solution)* – **Visual:** Neural network diagram monitoring transactions streaming on a dashboard. **Detailed:** Explain Visa’s deep learning model (RNN) that scores each transaction’s fraud risk ([
	Visa - Visa’s Growing Services Business Infused with New AI-Powered Products
](https://investor.visa.com/news/news-details/2024/Visas-Growing-Services-Business-Infused-with-New-AI-Powered-Products/default.aspx#:~:text=,allowing%20issuers%20to%20simplify%20their)). Note it was trained on billions of past transactions to recognize subtle fraud signals. It runs in real-time (within milliseconds) on Visa’s platform ([
	Visa - Visa’s Growing Services Business Infused with New AI-Powered Products
](https://investor.visa.com/news/news-details/2024/Visas-Growing-Services-Business-Infused-with-New-AI-Powered-Products/default.aspx#:~:text=%2A%20Real,bad%20transactions%20before%20they%20happen)). In a visual deck, an animation of data flowing through a network into “Approved/Declined” bins could illustrate this.  
- *Slide 4 (Outcomes)* – **Visual:** A gauge showing “fraud down” or a before/after of fraud rate dropping. **Detailed:** Quantify outcomes: e.g., “$40B in fraud blocked in one year” ([
	Visa - Visa’s Growing Services Business Infused with New AI-Powered Products
](https://investor.visa.com/news/news-details/2024/Visas-Growing-Services-Business-Infused-with-New-AI-Powered-Products/default.aspx#:~:text=Security%20and%20fraud%20prevention%20are,company%E2%80%99s%20deep%20expertise%20in%20AI)), false positives reduced (fewer valid transactions wrongly declined). Business benefit: saves money and increases customer trust. In detailed version, cite how deep learning’s pattern-recognition catches 30% more fraud attempts with 90% accuracy ([Credit Card Fraud Detection Case Study | SPD Technology](https://spd.tech/machine-learning/credit-card-fraud-detection-case-study/#:~:text=deserving%20examples%20here,detection%20case%20studies%20that%20leverage)) (DataVisor case). Visual version might use a simple statistic on-screen (“30% more fraud caught ([Credit Card Fraud Detection Case Study | SPD Technology](https://spd.tech/machine-learning/credit-card-fraud-detection-case-study/#:~:text=deserving%20examples%20here,detection%20case%20studies%20that%20leverage))”).  
- *Slides 5+ (if needed)* – Could delve into integration: how the AI fits into the payment workflow. **Visual:** illustration of credit card swipe → AI cloud → result. **Detailed:** Explain deployment on cloud GPUs, scalability to thousands of TPS (transactions per second). Possibly include a testimonial quote from an exec: e.g., “AI models let us stop fraud before it hits accounts.”  
- *Visual vs. Detailed:* The visual deck relies on icons (shield, warning symbol, happy customer) and one-key-phrase callouts (“Real-time scoring”, “$ saved”). The detailed version adds 1–2 bullets explaining each icon (e.g., “Real-time scoring with RNN analyzes 100k transactions/sec”).  
- *Suggested Visuals:* Credit card transaction flow diagram, a neural net graphic, charts of fraud rates before/after AI, money saved ticker.

*(Similar slide breakdowns would be created for the other deep learning use cases: predictive maintenance and recommendations. Each case’s mini-deck would start with a compelling image (factory machinery, or a recommended products screen) and outline the scenario, the deep learning solution architecture, and business impact. Visual versions focus on imagery like machinery sensors and trend lines, or a Netflix UI screenshot with recommended titles, with minimal text. Detailed versions add explanatory callouts (e.g. “IoT sensors feed a neural network that predicts failures 5 days in advance” or “Deep learning recommendation engine – 75% of viewed content from recommendations ([Netflix's AI Personalization Strategy Saves $1 Billion Yearly in ...](https://headofai.ai/netflixs-ai-personalization-strategy-saves-1-billion-yearly-in-customer-retention/#:~:text=,The%20personalisation%20strategy%20has))”). This ensures both storytelling through visuals and factual depth.)*

## Jupyter Notebook Outline (Code Examples & Challenge)
For **Deep Learning**, we propose interactive notebooks that implement simplified versions of these use cases, using **PyTorch** for neural networks and **scikit-learn** for evaluation and comparison. Each notebook is structured with clear markdown explanations for business-oriented learners:

- **Notebook 1: Medical Image Classification** – *Objective:* Demonstrate how a convolutional neural network can identify an image class (simulating disease detection).  
  - **Data & Context:** Markdown explains we’ll use a public dataset (e.g., chest X-ray images or even MNIST for simplicity) as a stand-in for medical scans. Explain how in the business case the data are retina scans with labels (urgent vs. non-urgent).  
  - **Code Cells:** Use PyTorch to build a simple CNN. Step-by-step, start with data loading (with `torchvision` if images). Show model definition (layers of a CNN), training loop on training data, and evaluation on test data. Include comments drawing parallels to the eye disease AI (e.g., “# this CNN plays the role of the DeepMind model, learning to detect features in images”). Use scikit-learn’s metrics (like classification report or ROC curve) to evaluate performance.  
  - **Markdown Cells:** After each code block, provide interpretation: e.g., “Our model achieved X% accuracy in classifying images. In the hospital use-case, this indicates how many scans the AI correctly flags.” Tie results back to business impact (“High recall means the system catches most disease cases, critical in healthcare.”).  
  - **Learning Challenge:** Ask students to modify the code – for example, “Try increasing the number of convolutional layers or training for more epochs. Does accuracy improve?” or “Replace the CNN with a pre-trained model via transfer learning (hint: use `torchvision.models`), and see how performance changes.” This challenges them to experiment as if improving the hospital’s AI.  

- **Notebook 2: Fraud Detection with Neural Networks** – *Objective:* Predict fraudulent transactions using a neural network vs a classic method.  
  - **Data & Context:** Introduce a dataset (could use a synthetic or Kaggle credit card fraud dataset with features like amount, merchant, etc.). Explain features in business terms (e.g., “transaction amount”, “merchant category”) and label (fraud or not fraud).  
  - **Code Cells:** Use scikit-learn to quickly train a baseline model (e.g. logistic regression or decision tree) and measure its accuracy/precision/recall. Then use PyTorch to build a simple feed-forward network (or an LSTM if sequence data is considered) to classify fraud. Show training process and compare metrics to the baseline. This illustrates how deep learning can capture non-linear patterns better.  
  - **Markdown Cells:** Walk through the logic: data preprocessing (e.g., scaling amounts), why we split data (train/test to simulate deployment in finance). Explain the code in non-technical terms (“The neural network is finding complex patterns between transaction features and the likelihood of fraud”). After results, discuss business implications: e.g., “Our model’s recall of XX% means it catches most fraudulent transactions, which in practice reduces losses. Precision of YY% means few false alarms, so customers aren’t unnecessarily declined.”  
  - **Learning Challenge:** Prompt students to extend the work: “The current model doesn’t use all information. As a challenge, try engineering a new feature (e.g., transactions per hour for a card) and see if that improves fraud detection accuracy.” Or, “Adjust the threshold for classifying fraud to trade off false positives vs. false negatives, and observe the impact on precision/recall.” This ties to real business decisions on fraud model tuning.

- **Notebook 3: Recommender System Demo** – *Objective:* Illustrate how deep learning can drive recommendations.  
  - **Data & Context:** Use a simple product ratings dataset (or movie ratings). Explain the scenario: “We have users and products with ratings. The goal is to predict new recommendations – similar to how Netflix’s deep learning system works.”  
  - **Code Cells:** Start with a basic matrix factorization using `surprise` or a custom torch model (embedding layers for users and items). Train the model to reconstruct known ratings. Evaluate with an error metric (MSE) or ranking metric. Optionally, show how to use the model to recommend top-N items for a sample user.  
  - **Markdown Cells:** Explain each step in plain language (e.g., “We create embeddings – essentially, the AI learns a profile for each user and item. This is analogous to learning taste preferences.”). Connect to business: “In practice, a model like this helped Netflix save $1B by keeping users engaged ([Netflix Recommendation Engine Worth $1 Billion Per Year - Business Insider](https://www.businessinsider.com/netflix-recommendation-engine-worth-1-billion-per-year-2016-6#:~:text=And%20Netflix%20thinks%20it%E2%80%99s%20worth,more%20than%20%241B%20per%20year)).” After training, interpret the output: “User A is now recommended Product X, Y, Z, which were not explicitly rated by them – the model generalizes their preferences.”  
  - **Learning Challenge:** Ask students to modify the recommendation logic: “Try incorporating an additional feature (like product category or user age group) into the model and see if it improves recommendation accuracy.” Or, “Adjust the number of latent factors (embedding size) and observe how the recommendation quality changes.” This reinforces how tuning a model can impact business outcomes (more factors might capture nuance but risk overfitting).

Each notebook interweaves **business context** with the code. For instance, before coding, a markdown cell frames the problem (“Predict if a machine will fail in next week – akin to predictive maintenance at GE”). After coding, a markdown cell interprets results in the use-case context (“The model found 5 out of 6 upcoming failures – this could save maintenance costs”). We ensure **clear comments** in code (e.g., “# Compute loss – lower loss means our fraud predictions are more accurate”) and use analogies (comparing model decisions to human expert decisions where appropriate).

Overall, these deep learning notebooks give hands-on practice in building and evaluating models, while the narrative links each step to solving the real-world business challenge. The **learning challenges** encourage students to experiment, solidifying their understanding that tweaking algorithms or features can directly affect business KPIs (like accuracy of fraud detection or quality of recommendations). This balanced approach (theory, code, and business interpretation) makes the content accessible and meaningful for business professionals.

---

# AI Tools and Platforms

## Business Use Cases Overview
- **Cloud AI for Safety Monitoring (Energy/Retail):** Companies combine cloud platforms and AI to solve safety challenges. For example, **Shell** deployed a machine-vision system on **Microsoft Azure** to detect safety risks at gas stations. Cameras send footage to Azure IoT Edge and Azure Databricks, where deep learning models identify unsafe behaviors (like someone smoking near a pump) and instantly alert staff ([Shell invests in safety with Azure, AI, and machine vision to better protect customers and service champions | Microsoft Customer Stories](https://customers.microsoft.com/en-us/story/shell-mining-oil-gas-azure-databricks?wt.mc_id=AID2322108_QSG_PD_SCL_335083&trk=test#:~:text=An%20onsite%20camera%20captures%20footage,the%20pump%20to%20avoid%20any)). Using Azure’s ready-made IoT and AI services meant Shell could rapidly implement this at thousands of stations, preventing accidents and aligning with their “Goal Zero” safety initiative.  
- **AutoML for Demand Forecasting (Manufacturing/Retail):** Businesses leverage automated machine learning platforms to build models without large data science teams. **Lenovo Brazil** used DataRobot’s AutoML platform to forecast laptop sales at retailers. DataRobot automatically tested dozens of models, cutting model development from 4 weeks to 3 days and improving prediction accuracy from ~80% to 87.5% ([Lenovo Computes Supply Chain and Retail Success With DataRobot | PDF](https://www.slideshare.net/slideshow/lenovo-computes-supply-chain-and-retail-success-with-datarobot/200886517#:~:text=Lenovo%20Brazil%20used%20DataRobot%27s%20automated,Read%20less)). This helped Lenovo balance inventory and avoid stockouts or gluts, yielding up to \$1M+ in cost savings through optimized supply chain decisions.  
- **AI Chatbot Platform in Banking:** Enterprises use AI assistant platforms to improve customer service. **NatWest Group** built a digital mortgage assistant (“Marge”) on IBM’s Watson Assistant cloud ([NatWest Group | IBM](https://www.ibm.com/case-studies/natwest-group-watson#:~:text=IBM%20Consulting%E2%84%A2%C2%A0and%20NatWest%20have%20co,mortgage%20support%20for%20home%20buyers)). This platform delivers real-time support to call center staff by instantly retrieving policy info and next-best actions during calls. The result: 20% higher customer satisfaction (NPS) and 10% shorter call times ([NatWest Group | IBM](https://www.ibm.com/case-studies/natwest-group-watson#:~:text=Increased%20customer%20loyalty)) ([NatWest Group | IBM](https://www.ibm.com/case-studies/natwest-group-watson#:~:text=Since%20implementing%20the%20digital%20mortgage,decrease%20in%20call%20duration)). By using IBM’s cloud AI tools, NatWest quickly deployed a reliable, scalable assistant that learns from each interaction, demonstrating how an AI platform can enhance both customer experience and operational efficiency.  
- **Hybrid Cloud for Predictive Maintenance:** Some firms combine IoT platforms with AI services to maintain equipment. **Volkswagen** (for instance) might use AWS IoT analytics with Amazon SageMaker (AutoML) to predict assembly line robot failures. This tool-and-platform approach allows ingesting sensor data at scale, applying AWS’s built-in machine learning algorithms to predict failures, and automatically dispatching maintenance – all without building a solution from scratch. The benefit is reduced downtime in factories and a quicker path from data to insight using cloud offerings.

*(Each use case above highlights a different aspect: Shell – using a cloud IoT+AI platform for computer vision; Lenovo – using an AutoML tool for forecasting; NatWest – using an AI assistant platform; Volkswagen – using integrated cloud IoT/AutoML for maintenance. These illustrate how various industries tap AI platforms to accelerate development and deployment of AI solutions.)*

## PowerPoint Slide Breakdown (Visual & Detailed Versions)
**Use Case 1 – Shell & Azure: AI Safety Monitoring**  
- *Slide 1 (Introduction)* – **Visual:** Shell logo and an image of a gas station forecourt, with a bold title “Cloud + AI = Safer Shell Stations.” **Detailed:** Explain this case is about Shell using Microsoft’s AI platform to improve safety. Emphasize relevance: showcases an end-to-end AI IoT solution using existing tools (Azure IoT, etc.).  
- *Slide 2 (Business Context)* – **Visual:** Icon of a gas pump and a warning sign. Caption: “Safety Challenge: 44,000 Shell stations, 30M customers/day ([Shell invests in safety with Azure, AI, and machine vision to better protect customers and service champions | Microsoft Customer Stories](https://customers.microsoft.com/en-us/story/shell-mining-oil-gas-azure-databricks?wt.mc_id=AID2322108_QSG_PD_SCL_335083&trk=test#:~:text=Headquartered%20in%20The%20Hague%2C%20the,smoking%2C%20and%20improper%20fueling%20behaviors)).” **Detailed:** Describe the problem – safety incidents (like fires from smoking near fuel) are rare but high-impact. Human staff can miss risky behaviors. Shell wanted a proactive, automated prevention system.  
- *Slide 3 (AI Tool Solution)* – **Visual:** Architecture diagram: Camera → Azure IoT Edge → Cloud AI → Manager Alert. Minimal text: “Visual AI on Azure Platform.” **Detailed:** Step through the solution: Shell installed smart cameras (Edge devices) that run image processing on-site ([Shell invests in safety with Azure, AI, and machine vision to better protect customers and service champions | Microsoft Customer Stories](https://customers.microsoft.com/en-us/story/shell-mining-oil-gas-azure-databricks?wt.mc_id=AID2322108_QSG_PD_SCL_335083&trk=test#:~:text=An%20onsite%20camera%20captures%20footage,Before%20the%20customer%20can%20take)). They send data to Azure cloud, where Azure Databricks hosts a deep learning model that recognizes unsafe acts (e.g., someone lighting a cigarette) ([Shell invests in safety with Azure, AI, and machine vision to better protect customers and service champions | Microsoft Customer Stories](https://customers.microsoft.com/en-us/story/shell-mining-oil-gas-azure-databricks?wt.mc_id=AID2322108_QSG_PD_SCL_335083&trk=test#:~:text=An%20onsite%20camera%20captures%20footage,the%20pump%20to%20avoid%20any)). Because they used Azure’s platform, they didn’t need to build infrastructure from scratch – they leveraged IoT Hub, cloud VMs, etc. The system (called VADR) disables the fuel pump and alerts staff in real time ([Shell invests in safety with Azure, AI, and machine vision to better protect customers and service champions | Microsoft Customer Stories](https://customers.microsoft.com/en-us/story/shell-mining-oil-gas-azure-databricks?wt.mc_id=AID2322108_QSG_PD_SCL_335083&trk=test#:~:text=Hub%20azure,to%20avoid%20any%20potential%20incident)).  
- *Slide 4 (Visual vs. Detailed Deployment)* – **Visual:** Microsoft Azure logo connected to a gas station image, with words “Pilot to Global Scale.” **Detailed:** Note how using Azure’s platform makes it scalable: once the model worked at pilot sites, Shell can deploy it across all stations via the cloud. Mention platform benefits: security, integration (alerts go to manager dashboards instantly). In a visual deck, this could be a world map with Azure cloud icon and Shell station icons to imply global rollout.  
- *Slide 5 (Outcomes & Benefits)* – **Visual:** A gauge or counter “0 accidents – Goal Zero”. Possibly a photo of a Shell manager receiving an AI alert on screen. **Detailed:** Outline early results: the AI caught dangerous situations (e.g., several smoking incidents) that staff intervened in, potentially averting accidents. Shell reports improved compliance with safety rules. Also highlight efficiency – one central platform handles analysis for many sites, reducing need for manual CCTV monitoring. **Detailed version** can include a bullet like “Azure-based AI identifies hazards in seconds, preventing incidents (e.g., pump shut-offs before any ignition). Shell achieves safer operations with minimal added labor.” The **visual version** might simply say “Real-time alerts → 0 incidents in pilot. Scalable safety.”  
- *Suggested Visuals:* Diagram of how the data flows from station to cloud to manager, a before/after of a person about to smoke vs. staff stopping them, Azure icons, and Shell branding. The **visual deck** would rely on these graphics with a few keywords, while the **detailed deck** adds explanatory text (either on slides or spoken) linking each visual element to the AI tool utilized (e.g., label Azure components in the architecture image).

**Use Case 2 – Lenovo & DataRobot: AutoML Forecasting**  
- *Slide 1 (Intro)* – **Visual:** Lenovo logo with an image of a laptop and a graph trending upward. Title: “AutoML Boosts Forecast Accuracy.” **Detailed:** State that Lenovo used an automated ML platform (DataRobot) to predict product demand, a task previously slow for their analysts.  
- *Slide 2 (Problem)* – **Visual:** Icon of a warehouse with too much stock vs. empty shelf – text “Inventory Imbalance”. **Detailed:** Explain the business problem: forecasting sales for retailers was time-consuming and often inaccurate, leading to overstock (tying up capital) or stockouts (lost sales). Lenovo’s small data science team couldn’t manually rebuild models for dozens of products every week.  
- *Slide 3 (Tool Solution)* – **Visual:** DataRobot logo and a screenshot of an AutoML leaderboard (showing model names and accuracy). **Detailed:** Describe how Lenovo applied DataRobot. They fed historical sales, promotions, and economic data into DataRobot’s cloud service. The AutoML platform automatically tried 24+ algorithms and feature combos ([Case Study - Kiva Uses DataRobot to Increase Microloan Funding ...](https://leaders.iotone.com/case-study/kiva-uses-datarobot-to-increase-microloan-funding-rate/c8525#:~:text=,loans%20will%20be%20funded)), delivering the best model. Emphasize no manual coding was needed; a business analyst could operate the tool via a GUI. The platform also provided explainability (e.g., identifying that holiday weeks boost sales, etc.).  
- *Slide 4 (Results)* – **Visual:** Before/After chart: Forecast error dropped, time to model dropped (illustrate 4 weeks → 3 days as arrows). **Detailed:** Quantify improvements: Model prep time cut from ~1 month to a few days; accuracy improved ~10% ([Lenovo Computes Supply Chain and Retail Success With DataRobot | PDF](https://www.slideshare.net/slideshow/lenovo-computes-supply-chain-and-retail-success-with-datarobot/200886517#:~:text=Lenovo%20Brazil%20used%20DataRobot%27s%20automated,Read%20less)). State business impact: more accurate forecasts meant Lenovo Brazil could reduce excess inventory, saving warehousing costs, and ensure retailers had enough stock during demand spikes, increasing revenue. Possibly cite “improved accuracy from <80% to 87.5% ([Lenovo Computes Supply Chain and Retail Success With DataRobot | PDF](https://www.slideshare.net/slideshow/lenovo-computes-supply-chain-and-retail-success-with-datarobot/200886517#:~:text=Lenovo%20Brazil%20used%20DataRobot%27s%20automated,decisions%20based%20on%20DataRobot%27s%20accurate)) led to better decisions on production and distribution.”  
- *Slide 5 (Visual vs. Detailed Takeaways)* – **Visual:** A simple textbox, “Key Benefit: Fast, Accurate Forecasts without Coding”. Possibly an image of a Lenovo supply chain manager happily looking at a DataRobot dashboard. **Detailed:** Summarize why using an AI platform was key: Lenovo didn’t need to hire more data scientists; the tool amplified their team’s productivity. Also mention that the platform’s cloud nature meant easy deployment of the model to update forecasts weekly. The detailed version might list “**DataRobot AutoML**: tried 32 models, chose Gradient Boosted Trees; **Outcome**: forecast error down 15%, \$X saved annually.” The visual version might just have icons for speed and accuracy with brief captions.  
- *Suggested Visuals:* DataRobot interface screenshots or stylized representations (e.g., gears to represent automation), graphs comparing forecast vs. actual sales, inventory piles vs. optimal inventory graphic.

**Use Case 3 – NatWest & Watson Assistant: AI Customer Support**  
*(Slides follow a similar pattern:* Introduction of the bank’s challenge with mortgage call volume; the Watson Assistant “Marge” solution on IBM Cloud; how it works in flow – question asked → AI finds answer → employee responds faster; and results – happier customers, metrics like +20% loyalty ([NatWest Group | IBM](https://www.ibm.com/case-studies/natwest-group-watson#:~:text=Increased%20customer%20loyalty)). *Visual deck* might use a cartoon of a banker and an AI avatar working together, and *detailed deck* would explain the integration with existing systems and stats like “42 million questions answered to date” if available.)

*(By breaking down each use case into a mini 3–5 slide story, the course can show concretely how specific AI tools/platforms are applied. The “Visual” version of slides uses logos, icons (clouds, gears, graphs) and minimal words to tell the story at a glance, ideal for class presentation or exec audiences. The “Detailed” version either adds speaker notes or additional bullet text on the slides so that someone reviewing later can understand the full context and data. Each slide deck emphasizes the match between the business problem and the AI platform solution, and highlights results with both qualitative and quantitative info.)*

## Jupyter Notebook Outline (Code Examples & Challenge)
In the Tools & Platforms module, the coding exercises will illustrate *how to use certain AI frameworks or services*, rather than building algorithms from scratch. These notebooks familiarize business students with the practical use of AI libraries and simple cloud-like workflows on their local machine:

- **Notebook 1: AutoML-Style Model Selection** – *Scenario:* Using scikit-learn to mimic an AutoML process for a sales forecasting task (simplified Lenovo case).  
  - **Data:** Perhaps a CSV of past monthly sales for several products. Markdown explains this represents Lenovo’s historical sales data.  
  - **Code:** Demonstrate how one could iterate through multiple algorithms in scikit-learn (linear regression, random forest, XGBoost if available) to find the best predictor. Use `sklearn.model_selection` to do cross-validation for each model, rank them by MAE or MAPE (typical forecasting error). This is akin to what DataRobot does automatically.  
  - **Markdown:** Explain each step in plain terms (“We will try three different models to see which predicts best, similar to an AutoML tool testing many models”). After running, show the errors from each and pick the best model. Discuss how hyperparameter tuning could be added (perhaps show a simple GridSearchCV to illustrate automation in parameter search).  
  - **Learning Challenge:** Ask students to extend this by adding another algorithm (e.g., an SVR or a neural network via sklearn’s MLPRegressor) to the comparison, or by tuning parameters for one model to see if they can beat the initially best model. This hands-on exercise reinforces how AutoML platforms systematically find the best model, and students get to replicate that logic manually.

- **Notebook 2: Deploying a Pre-trained Model (API Simulation)** – *Scenario:* Simulate using a cloud AI API (e.g., image recognition) within a notebook, to show how a business can leverage pre-built models.  
  - **Data:** Provide a few sample images (perhaps of common objects or a safety scenario like someone smoking).  
  - **Code:** Use a pre-trained model from PyTorch (for example, load a ResNet model pre-trained on ImageNet) to do image classification. This simulates calling a cloud vision API. Show how to feed an image and get a label prediction. Alternatively, use `torch.hub` or `transformers` for a pre-trained model (depending on what’s simpler).  
  - **Markdown:** Explain that in many platforms (AWS, Azure, Google), you don’t train your own model for common tasks – you call their API. Here we mimic that by loading a model that’s already trained. Step through the code that loads the model and makes a prediction, in very commented detail (e.g., “# Load a model pre-trained by others (similar to using a cloud service)”).  
  - **Learning Challenge:** Have students try the model on a new image (provide a couple alternatives, or let them URL fetch one) and see if the prediction changes. Or, challenge them: “The model sometimes gives a category you may not expect. Try to improve the output by printing the top 3 predictions instead of just one.” This teaches them how to work with the results of an AI service and interpret confidence scores – a skill needed when integrating a third-party model.

- **Notebook 3: Building a Simple Chatbot Q&A** – *Scenario:* Illustrate a lightweight version of what Watson Assistant might do, using a Q&A approach with a small knowledge base.  
  - **Data:** Create a small dictionary of Q&A pairs (e.g., common mortgage questions and answers). This simulates the bank’s policy knowledge base.  
  - **Code:** Show how one could use a simple NLP approach to match user questions to answers. Perhaps use scikit-learn’s TfidfVectorizer to vectorize questions and use cosine similarity to find the closest known question. Or use a simple decision tree classifier if we pre-tag some question variants. Keep it basic so business students can follow.  
  - **Markdown:** Explain the flow: user query → some vectorization or keyword matching → best answer returned. Note that real platforms use more advanced NLP, but the concept of how an assistant retrieves an answer is demonstrated. Relate this to Watson: “Watson Assistant would be trained on thousands of Q&As and context, here we show a mini-version with a handful.”  
  - **Learning Challenge:** Ask students to add a new Q&A pair to the knowledge base and see if the “chatbot” now can answer that. Or ask them to refine the matching algorithm (maybe add a synonym dictionary to improve matching). This reinforces how maintaining a chatbot involves updating its knowledge and logic, mirroring what a business would do with an AI assistant platform.

Throughout these notebooks, the focus is on **using AI tools** rather than developing new algorithms. We use familiar libraries to emulate what AI platforms offer: automated model selection, pre-trained models as a service, and easy-to-build conversational flows. The notebooks contain plenty of markdown explanations drawing parallels to enterprise tools: e.g., “In DataRobot, you’d upload data and it would do this automatically – here you see under the hood,” or “Our image classifier is like using Azure’s Computer Vision API, which you could call with one line of code in a real app.”

We ensure **clear, instructional code**: well-commented, not too complex (no advanced Python that business learners would choke on). For instance, when iterating over models for AutoML demo, we’ll write it in an explicit loop with comments, instead of a tricky list comprehension. We’ll also demonstrate a bit of integration: printing results in nice format, maybe plotting error rates with matplotlib to visualize improvements (keeping plots simple since some might not be code-focused).

By the end, students will have **practical experience** with how AI tools work: they will have manually done what an AutoML or cloud API does, giving them insight into the value these platforms provide. The challenges push them to engage with the notebooks, reinforcing learning-by-doing. This balances the lecture’s conceptual understanding of AI platforms with concrete skills in leveraging AI libraries – a valuable combo for business professionals who need to know *what* the tools do and *how* to harness them. 

---

# Computer Vision

## Business Use Cases Overview
- **Automated Quality Inspection (Manufacturing):** Computer vision is used on factory lines to spot defects faster and more reliably than humans. **BMW** employs vision systems to inspect car parts for imperfections. High-resolution cameras and AI detect issues like tiny paint flaws or misaligned components in real-time, ensuring only perfect parts move forward ([Real-World Use Cases of Computer Vision for Quality Inspection](https://chisw.com/blog/computer-vision-for-quality-inspection/#:~:text=,also%20for%20automatically%20tracking%20goods)). This reduces recall costs and speeds up production by catching errors immediately rather than via manual checks.  
- **Retail “Just Walk Out” Stores:** Retailers like Amazon use CV to enable cashierless shopping. **Amazon Go** convenience stores are equipped with hundreds of ceiling cameras and deep learning vision algorithms that watch what items customers pick up ([Enhancing the Retail Experience: The Power of Computer Vision | AWS for Industries](https://aws.amazon.com/blogs/industries/enhancing-the-retail-experience-the-power-of-computer-vision/#:~:text=Faster%20checkout%3A%20Automated%20checkout%20systems,the%20need%20for%20traditional%20cashiers)). When you exit, the system knows exactly what you took and charges your account – no checkout lines. This computer vision system tracks inventory automatically and creates a frictionless shopping experience, giving Amazon a competitive edge in physical retail innovation.  
- **Insurance Claims Automation:** Insurers leverage computer vision to assess damage from photos. For example, Spain’s **Admiral Seguros** uses Tractable’s AI: customers upload car accident photos, the CV model analyzes the damage and estimates repair costs within minutes. In 2021, this “touchless” claims process handled 12,000 claims, with 90% of estimates processed with no human appraiser and 98% of claims finished in <15 minutes ([Admiral Seguros: Tractable delivers outstanding customer services through touchless claims](https://tractable.ai/case-studies/admiral-seguros/#:~:text=In%202021%2C%20Admiral%20Seguros%20processed,in%20less%20than%2015%20minutes)). The benefit is faster payouts, lower adjusting costs, and improved customer satisfaction in a traditionally slow process.  
- **Precision Agriculture:** Farmers deploy computer vision via drones or smart sprayers to improve crop yields. **John Deere’s See & Spray** system uses AI cameras to distinguish weeds from crops in real time. The machine vision guides sprayers to hit only the weeds with herbicide, avoiding the crop. This *selective targeting* cuts herbicide use by 70–90% ([ AI-driven weed controllers reduce the need for herbicides by 90%](https://www.warpnews.org/green-tech/ai-driven-weed-controllers-reduce-the-need-for-herbicides-by-90/#:~:text=John%20Deere%2C%20the%20world%27s%20largest,through%20AI%20and%20machine%20learning)) ([ AI-driven weed controllers reduce the need for herbicides by 90%](https://www.warpnews.org/green-tech/ai-driven-weed-controllers-reduce-the-need-for-herbicides-by-90/#:~:text=plans%20to%20expand%20to%20eight,more%20states%20next%20year)), greatly lowering chemical costs and environmental impact while maintaining crop health. This CV-driven approach is transforming farming to be more sustainable and cost-efficient.  
- **Security & Access Control (Smart Cities):** City authorities and businesses use CV for security—e.g., **facial recognition** systems to control entry. Airports like Dubai International have implemented face-scanning gates that recognize travelers, speeding up identity verification vs. manual passport checks. In corporate offices, vision systems identify employees (or detect unknown persons) entering secure areas. These applications show CV providing both convenience and enhanced security, though they come with privacy considerations.

## Business Use Cases Overview (visual scanning)
*(The above use cases cover diverse industries: manufacturing, retail, insurance, agriculture, and security, each showing how visual data (images or video) is analyzed by AI to solve a specific business need. The details highlight context, solution, and benefits, with real stats where possible, e.g., production improvement, elimination of checkout, claim cycle time reduction, chemical savings, etc.)*

## PowerPoint Slide Breakdown (Visual & Detailed Versions)
**Use Case 1 – Automated Quality Inspection (BMW Manufacturing)**  
- *Slide 1 (Intro)* – **Visual:** Photo of a car assembly line with a digital camera inspecting a part. Title: “Computer Vision Quality Control at BMW.” **Detailed:** Introduce BMW’s need for defect detection and how CV addresses it. Relevance: shows CV’s impact on manufacturing quality and cost.  
- *Slide 2 (Problem)* – **Visual:** Image of a car door with a red circle highlighting a defect (e.g., paint scratch). Caption: “Tiny defects, big problems.” **Detailed:** Explain the challenge – even minor defects can slip past human inspectors due to fatigue or speed. This can lead to customer complaints or costly rework. BMW produces thousands of parts daily; manual inspection is a bottleneck and not 100% reliable.  
- *Slide 3 (CV Solution)* – **Visual:** Schematic: camera taking pictures of each part, with an AI brain icon analyzing images, and a conveyor belt stopping when a defect is found. **Detailed:** Describe BMW’s vision system: high-speed cameras capture each item, an AI model (trained on images of good vs. bad parts) flags anomalies – e.g., a scratch, incorrect assembly, or missing component ([Real-World Use Cases of Computer Vision for Quality Inspection](https://chisw.com/blog/computer-vision-for-quality-inspection/#:~:text=Real,%C2%B7%20Amazon%20uses)). The system can detect things like “missing or incorrectly colored stitches in leather seats” automatically ([Case Study: NVIDIA Boosts BMW Group’s Production Efficiency with AI | NVIDIA](https://www.nvidia.com/en-us/case-studies/bmw-optimizes-production-with-ai-and-dgx-systems/#:~:text=detects%20missing%20or%20incorrectly%20colored,strong%20focus%20on%20quality%20assurance)). Emphasize that CV operates 24/7 with consistent criteria. Mention tools: they might use an NVIDIA-powered system (as NVIDIA case study suggests) to process images in real-time on the factory floor.  
- *Slide 4 (Outcomes)* – **Visual:** Chart showing “Defects per 1000 cars” dropping after CV implementation, or an image comparing a defect missed by human vs caught by AI with a green checkmark. **Detailed:** Quantify results: “BMW reports >25% reduction in rework time, and near-zero defects reaching customers ([Real-World Use Cases of Computer Vision for Quality Inspection](https://chisw.com/blog/computer-vision-for-quality-inspection/#:~:text=,also%20for%20automatically%20tracking%20goods)).” They also reallocated human inspectors to more complex quality tasks. Financially, fewer defects mean saving on warranties and improving brand reputation for quality. In a visual deck, a simple tagline “Zero Defect Goal – In Sight with AI” might accompany a graph or trophy icon for quality award. Detailed deck might bullet: “99% detection rate achieved; saves \$XM annually in avoided recalls.”  
- *Visual vs. Detailed:* The **visual version** uses imagery of factory and simplified diagrams with one-phrase callouts (“AI vision checks every part in milliseconds”). The **detailed version** adds specifics: e.g., note that BMW’s system checks *hundreds of features* on each vehicle, and give an example of a defect it catches that humans commonly missed. It might also mention the technology’s adaptability – new defect types can be learned by the model with additional training (like how CV is re-trainable).  
- *Suggested Visuals:* Assembly line photo, sample defect images with AI annotations (like bounding boxes around flaws), a comparison of a pristine vs defective part. Possibly include the BMW logo for context.

**Use Case 2 – Amazon Go: Vision-Powered Store**  
- *Slide 1 (Intro)* – **Visual:** Photo of an Amazon Go store entrance with the sign “Just Walk Out Shopping.” Title: “Amazon Go – No Checkout Thanks to Computer Vision.” **Detailed:** Set the scene: Amazon’s small grocery stores use AI vision to let customers grab items and leave without stopping to pay at a register.  
- *Slide 2 (The Experience/Problem)* – **Visual:** A shopper walking out with a bag, sensors above. Text: “No Lines, No Cashiers.” **Detailed:** Explain the traditional problem: checkout lines frustrate customers and labor costs for cashiers are significant. Amazon wanted to revolutionize physical retail by eliminating checkout friction. The technical challenge: how to accurately bill people when they don’t scan items.  
- *Slide 3 (CV System)* – **Visual:** Overhead view diagram of a store layout with cameras (little camera icons) covering areas, and a person picking an item from shelf – with a label “Item X added to cart.” **Detailed:** Describe how it works: A network of cameras in the ceiling track each person in the store, and computer vision algorithms identify which products each person picks up or puts back ([Enhancing the Retail Experience: The Power of Computer Vision | AWS for Industries](https://aws.amazon.com/blogs/industries/enhancing-the-retail-experience-the-power-of-computer-vision/#:~:text=Faster%20checkout%3A%20Automated%20checkout%20systems,the%20need%20for%20traditional%20cashiers)). (Mention they also use weight sensors on shelves as auxiliary data.) The CV has to solve “who took what” – distinguishing multiple shoppers and products. They use advanced image recognition to tell a Coke bottle from a Pepsi can, etc. It’s like a multi-object tracking and identification vision problem, in real-time.  
- *Slide 4 (Outcome for Business)* – **Visual:** Icon of a shopping cart with a wifi signal (indicating a high-tech cart) and text “35 stores and counting” or “Customers love it – higher spend”. **Detailed:** Explain results: Amazon Go stores have opened in several cities, demonstrating the tech. Customers simply walk out – later, their Amazon account is charged and they get a receipt on the app. This convenience can increase store traffic and impulse buys (since there’s no wait barrier). Also, Amazon gathers rich data on in-store behavior (like how long someone browsed a product). From an ops perspective, they reduced staffing needs – employees focus on restocking and customer help instead of checkout. In detailed notes, mention that as of 2023, Amazon had **20+ Amazon Go/Fresh locations using this tech ([Amazon Fresh kills “Just Walk Out” shopping tech—it never really ...](https://arstechnica.com/gadgets/2024/04/amazon-ends-ai-powered-store-checkout-which-needed-1000-video-reviewers/#:~:text=,stores%2C%20and%20two%20Whole))**, proving it scalable. Also note competitive response: other retailers now exploring similar CV-based checkout (e.g., Aldi in Europe piloting).  
- *Visual vs. Detailed:* The **visual slides** emphasize the futuristic experience with attractive photos and minimal text (“AI sees what you take – charges automatically”). The **detailed slides** ensure to clarify accuracy (“vision system is robust: eg. distinguishes identical-looking items via packaging differences”), and how it solves the identity mapping (each shopper tracked anonymously via a unique ID until checkout). Could include a brief citation: “CV + deep learning handle *hundreds of cameras* input, syncing with shelf sensors ([Enhancing the Retail Experience: The Power of Computer Vision | AWS for Industries](https://aws.amazon.com/blogs/industries/enhancing-the-retail-experience-the-power-of-computer-vision/#:~:text=Faster%20checkout%3A%20Automated%20checkout%20systems,the%20need%20for%20traditional%20cashiers)).”  
- *Suggested Visuals:* Actual images from Amazon’s promotional material (like people in store, overhead camera shots). Perhaps a mock-up of the receipt that shows items detected by the system. Possibly a before/after of a long checkout line vs. Amazon Go’s empty checkout area.

**Use Case 3 – Insurance Claims (Admiral/Tractable)**  
- *Slide 1 (Intro)* – **Visual:** Image of a damaged car and a smartphone taking photos. Title: “AI Vision Accelerates Insurance Claims.” **Detailed:** Introduce Admiral Seguros’ use of an AI (Tractable) to analyze accident photos. Emphasize industry pain point: claims used to take days/weeks; CV cuts it to minutes.  
- *Slide 2 (Problem)* – **Visual:** Graphic of a calendar turning pages or a frustrated customer on phone. Caption: “Traditional Claim = 5-7 days for estimate.” **Detailed:** Explain how traditionally a customer would wait for an adjuster to inspect the vehicle or send photos by email for manual review ([Admiral Seguros: Tractable delivers outstanding customer services through touchless claims](https://tractable.ai/case-studies/admiral-seguros/#:~:text=Before%20Admiral%20Seguros%20used%20a,in%20order%20to%20make%20estimations)). This is labor-intensive and slow, leading to customer dissatisfaction and higher rental car costs while waiting for repairs.  
- *Slide 3 (AI Process)* – **Visual:** Flowchart – Customer uploads photo -> AI analysis -> instant report. Include an icon of a car with highlighted damage areas. **Detailed:** Describe Tractable’s CV model: it’s trained on millions of car damage images and repair estimates. When a new accident photo comes in, the AI identifies the damaged parts (e.g., bumper cracked, fender dented) and the severity. It then cross-references a database to estimate repair cost. All this happens automatically online. Admiral integrated this into their claim app – so when someone has a minor crash, they just snap pictures in the app. The AI returns a repair quote almost immediately. In Admiral’s case, 90% of claims needed no human intervention ([Admiral Seguros: Tractable delivers outstanding customer services through touchless claims](https://tractable.ai/case-studies/admiral-seguros/#:~:text=In%202021%2C%20Admiral%20Seguros%20processed,in%20less%20than%2015%20minutes)), and many were processed end-to-end in <15 minutes.  
- *Slide 4 (Outcomes)* – **Visual:** Big stopwatch icon with “15 min”, and a thumbs-up emoji/customer. Also maybe a money bag with an arrow down (cost savings). **Detailed:** Provide results: Admiral Seguros saw a huge efficiency gain – thousands of claims handled “touchlessly.” Cite the stat: *98% of AI-estimated claims completed in under 15 min ([Admiral Seguros: Tractable delivers outstanding customer services through touchless claims](https://tractable.ai/case-studies/admiral-seguros/#:~:text=In%202021%2C%20Admiral%20Seguros%20processed,in%20less%20than%2015%20minutes)).* This means customers get instant settlements (some insurers even auto-approve low-risk claims). Business-wise, this reduces the workload on human adjusters, allowing them to focus on complex cases. It also means lower operational costs (fewer site visits). Possibly mention customer adoption: ~70% of customers offered the AI option used it happily ([Admiral Seguros: Tractable delivers outstanding customer services through touchless claims](https://tractable.ai/case-studies/admiral-seguros/#:~:text=Uptake%20by%20customers%20has%20been,than%20two%20minutes%20to%20complete)), indicating strong acceptance.  
- *Visual vs. Detailed:* **Visual slides** will use icons (clock, camera, car) and minimal text like “Minutes instead of Days – 90% automated!” to hammer the key benefit. **Detailed slides** might include a mini case metric: “12k claims processed via AI in 2021 ([Admiral Seguros: Tractable delivers outstanding customer services through touchless claims](https://tractable.ai/case-studies/admiral-seguros/#:~:text=In%202021%2C%20Admiral%20Seguros%20processed,in%20less%20than%2015%20minutes)) -> freed up ~50 adjuster-months of work.” Could also address that the AI consistency in estimates might reduce overpayments or fraud. And maybe a note: human staff now manage exceptions where AI isn’t confident (the system can route those to manual).  
- *Suggested Visuals:* Tractable’s interface (if available) showing an analyzed car photo with damage highlighted. A comparison of claim processing timeline (chart: before vs after AI). Smiling customer with a phone showing “Your claim is approved!”

**Use Case 4 – Precision Spraying in Agriculture (John Deere)**  
- *Slide 1 (Intro)* – **Visual:** Drone or tractor image over a field with a spray boom, plus an inset of camera view identifying weeds. Title: “Computer Vision Targets Weeds, Saves Herbicide.” **Detailed:** Introduce how CV distinguishes weeds from crops in real time. Agriculture context: reducing chemical use = cost and environmental win.  
- *Slide 2 (Problem)* – **Visual:** Photo of a crop field being blanket-sprayed with text “Wasted Spray = Wasted Money.” **Detailed:** Farmers traditionally spray entire fields with herbicide, even though weeds might only cover a small portion. This wastes chemicals (costly) and harms soil and nearby ecosystems. Also, herbicide resistance is a concern – need to be more strategic. The challenge: identify weed plants at tractor speed in an outdoor environment (varied lighting, plant shapes).  
- *Slide 3 (CV Solution)* – **Visual:** Illustration: Camera on tractor focusing on ground, green boxes around crop plants, red boxes around weeds. The sprayer nozzles activate only on red (weed) spots. **Detailed:** Explain John Deere’s See & Spray system (or similar from Blue River Tech): On the sprayer rig, high-res cameras scan the ground. Computer vision models (trained on images of crop vs weed seedlings) classify each patch multiple times per second. When a weed is recognized, the system precisely triggers a spray nozzle only at that location ([ AI-driven weed controllers reduce the need for herbicides by 90%](https://www.warpnews.org/green-tech/ai-driven-weed-controllers-reduce-the-need-for-herbicides-by-90/#:~:text=herbicides.%20New%20technology%20with%20AI,of%20chemicals%20used%2C%20Bloomberg%20reports)). It’s effectively real-time object detection in farming. The hardware+AI can process vast areas quickly. Mention that Deere reported up to 77% reduction in herbicide use in some crop trials ([ AI-driven weed controllers reduce the need for herbicides by 90%](https://www.warpnews.org/green-tech/ai-driven-weed-controllers-reduce-the-need-for-herbicides-by-90/#:~:text=John%20Deere%2C%20the%20world%27s%20largest,through%20AI%20and%20machine%20learning)), and other startups claim over 90% in certain conditions ([ AI-driven weed controllers reduce the need for herbicides by 90%](https://www.warpnews.org/green-tech/ai-driven-weed-controllers-reduce-the-need-for-herbicides-by-90/#:~:text=plans%20to%20expand%20to%20eight,more%20states%20next%20year)).  
- *Slide 4 (Outcomes)* – **Visual:** Icon of a plant with a small drop vs a big drop (less chemical), a savings $$$ sign, and a leaf/earth icon for eco-benefit. **Detailed:** Outline benefits: Huge cost savings on herbicide (farmers spend millions – cutting that by ~70% is a game-changer). Environmental benefit: less runoff of chemicals, aligning with sustainability goals. Crop yield can improve too because you’re not dousing crops with as much chemical. The system can also allow use of stronger herbicides in micro-doses that are more effective on resistant weeds, since you’re not spraying the whole field. In detailed notes, maybe reference that the **AI-driven sprayer was commercialized in 2022 and deployed in US farms, proving ROI with an average 2-year payback** (fictional example based on ~90% herbicide savings).  
- *Slide 5 (Extra - Tech/Platform)* – (optional if needing to mention tools) **Visual:** John Deere + Blue River logos, cloud icon “AI on Edge – no internet needed.” **Detailed:** Note that this CV runs on edge devices (embedded GPUs on the tractor) due to field connectivity issues – an interesting aspect of deploying AI outside datacenters. This can segue into discussion that CV isn’t just cloud-based; on-device AI is crucial in many industries (cars, drones, etc.).  
- *Visual vs. Detailed:* Visual approach uses dramatic imagery of a tractor and simple text like “Camera-guided sprayer: 90% less herbicide.” The detailed version backs it up: “AI cameras distinguish weeds vs crop 20 times per second; up to 90% herbicide reduction observed ([ AI-driven weed controllers reduce the need for herbicides by 90%](https://www.warpnews.org/green-tech/ai-driven-weed-controllers-reduce-the-need-for-herbicides-by-90/#:~:text=plans%20to%20expand%20to%20eight,more%20states%20next%20year)).” It might also mention scale: “Covers 500 acres/day with pinpoint accuracy.”  
- *Suggested Visuals:* Actual photo of the See & Spray rig. Sample image from its camera with annotations. A bar chart of herbicide usage: conventional vs AI-targeted (showing big reduction). Possibly an EPA logo or something to imply environmental approval.

*(Additional use cases like Security/Facial Recognition could be included similarly, with emphasis on how CV identifies people or objects for security, the controversies, and so on. However, given time, the four detailed cases above provide a robust coverage for the slide deck.)*

## Jupyter Notebook Outline (Code Examples & Challenge)
To reinforce the computer vision concepts, the course will include hands-on notebooks that solve simplified versions of these use cases. Using Python libraries like **OpenCV, PyTorch, and scikit-learn**, students will get a feel for training and using CV models:

- **Notebook 1: Defect Detection on a Simulated Assembly Line**  
  *Scenario:* Emulate quality inspection by training a model to distinguish “good” vs “defective” products using images.  
  - **Data:** Use a small image dataset (could be synthetic or from online sources) of two classes, e.g., non-defective vs defective objects (could be as simple as images of digits where some are blurred to simulate defect, or actual product images with and without a certain flaw). Alternatively, use the classic `MNIST` digits but designate one digit as a “defect” for fun (not truly representative, but lets us demonstrate classification).  
  - **Code:** 
    - Load image data using OpenCV or PIL. Preprocess (grayscale, resize).  
    - Use **PyTorch** to build a simple CNN classifier. (Alternatively, for simplicity and speed with beginners, use scikit-learn with flattened pixels into an SVM or RandomForest. But a CNN gives flavor of deep learning CV.)  
    - Train the model to classify good vs defective. Show training loss decreasing. Evaluate accuracy on a test set.  
    - If possible, implement a function that takes a new image (simulating a live camera feed) and returns “Defect” or “OK”. This could be demonstrated on a few test images.  
  - **Markdown:** Explain how images are represented (matrix of pixels) and how the model learns features (edges, textures) that might correspond to defects (e.g., a scratch might appear as an out-of-place edge). Tie this to the BMW case: “Our model, albeit simplistic, mimics how a real system distinguishes acceptable vs flawed products. In BMW’s real system, the images are more complex and the network deeper, but the principle is the same.” After evaluation, note the model’s accuracy – “It caught X% of the ‘defects’”. Emphasize importance of near-100% in real life to avoid any faulty product shipping.  
  - **Learning Challenge:** Provide a few new images (or instruct how to slightly modify an existing one, e.g., draw a line on a “good” item to simulate a scratch) and have students run the model on them. Or challenge them: “What if the lighting changes? Try adding noise or darkening an image and see if the model still recognizes it. This simulates variance on a factory line.” This teaches about model robustness, a key concern in CV deployments.

- **Notebook 2: Object Tracking for Retail (People Detection)**  
  *Scenario:* Demonstrate a simplified people-tracking or item-detection scenario like in Amazon Go, but on a very basic level (since multi-object tracking is advanced).  
  - **Data:** Possibly use a pre-recorded short video or GIF of a single person walking through a frame or a hand moving objects. If video handling is complex, use a sequence of static images where an object’s position changes.  
  - **Code:** Utilize OpenCV for motion detection or a pre-trained model from `cvlib` or `torchvision` for person detection. For instance, use a pre-trained COCO dataset model that can detect persons in an image.  
    - Frame-by-frame (or image-by-image), run detection (e.g., using OpenCV background subtraction to detect moving blob, or using a YOLO model via OpenCV DNN to detect a person or object).  
    - Mark the detections (draw bounding boxes). Possibly keep an ID for the object if tracking (for one object, we can assume ID=1 throughout).  
    - Show the output images with annotations in the notebook.  
  - **Markdown:** Explain this is a toy version of what Amazon Go does. “We are detecting a person in each frame. Amazon’s system does this for all customers and also identifies what they pick up.” If using a pre-trained model: highlight how we didn’t train it – we’re leveraging an existing CV model (similar to how Amazon likely started from existing research). Discuss complexities: distinguishing individuals (we’re not doing multi-person here, but mention if we had two, we’d need to assign IDs and keep tracking as they move – a big challenge known as multi-object tracking). If motion-based: explain thresholding movement to detect something has changed.  
  - **Learning Challenge:** If feasible, ask students to try it on a different video/image sequence (perhaps provide one where two objects move and see if the simple method conflates them). Or if using a deep model, challenge: “Try using the model to detect other object classes – e.g., set it to find ‘bottle’ in an image of a shelf. How might that relate to identifying products in a store?” This gets them thinking of extending vision to item recognition.

- **Notebook 3: Image Segmentation for Agriculture (Bonus)**  
  *Scenario:* Illustrate how one might differentiate two classes in an image (green crop vs green weed). This can tie to the farming example. (This might be a bit advanced, so could be optional/bonus content.)  
  - **Data:** Possibly use a small set of aerial images or even random green background with weeds drawn on. Or use a simpler analogy: segment out a particular color from an image (like find all red objects in an image) which is easier (color thresholding as a proxy for AI).  
  - **Code:** 
    - Approach 1: Use traditional CV (OpenCV color filtering) to find a certain color (say weeds are a distinct shade). Demonstrate masking an image.  
    - Approach 2: If data available, train a basic classification on patches: e.g., take small patches of an image and classify as “crop” or “weed” (similar to defect classification but spatial context). Or train a simple fully convolutional network if feeling ambitious and dataset is labeled (could be too much).  
    - Show result as an image with weeds marked (perhaps as red overlay).  
  - **Markdown:** Relate this to how the tractor’s system “sees” the field. Ours is oversimplified (perhaps relying on color, whereas real weeds may look similar color to crops). Explain that in actual systems, thousands of plant images were labeled and a deep neural net (like U-Net or Mask R-CNN) was trained to segment weeds from crops with high accuracy. But the fundamental idea – identifying class of pixel/region – is shown.  
  - **Learning Challenge:** Ask students to tune the threshold if using color filtering (“What if lighting changes and weed color isn’t exactly X? Adjust the filter and see effect”). Or if classification, “Add a new example of a weed (or simulate by drawing a new shape) and test the model.” The key learning is how CV can pinpoint specific targets in an image.

All notebooks will be heavily commented and stepwise, assuming the learners are new to CV coding. We’ll use lots of visualization (display images before and after processing) because visual feedback is key in understanding CV. For example, in the defect detection notebook, show a couple of example images the model got right and wrong, with commentary (“The model misclassified this image perhaps because the defect is too subtle or looks like normal texture – a sign we’d need more training data or a better model for such edge cases.”).

By working through these, students gain practical intuition: they’ll see how a camera’s pixel data becomes actionable information via algorithms. The markdown will continuously tie back: “In industry, you’d have many more images and a more complex model, but the process would be analogous – collect labeled images, train a model, and then deploy it on new images to predict.” Each notebook thus reinforces how CV technology is actually implemented for the business uses discussed. The challenges encourage students to experiment (perhaps adjusting a threshold or trying a different image), which mirrors the iteration process in real CV projects (tuning a vision system for a specific factory or farm environment). This boosts their confidence that they *can* understand and even build simple CV solutions – demystifying what can seem like a black box – and appreciating how it solves real problems.

---

# Natural Language Processing

## Business Use Cases Overview
- **Customer Service Chatbots (Banking/E-commerce):** Companies deploy NLP-driven chatbots to handle customer queries instantly. For example, **Bank of America’s “Erica”** virtual assistant uses NLP to understand customers’ banking questions via text or voice. Since launch, Erica has handled over 2 billion interactions for 42 million clients, answering inquiries about transactions, budgeting, and more ([BofA’s Erica Surpasses 2 Billion Interactions, Helping 42 Million Clients Since Launch](https://newsroom.bankofamerica.com/content/newsroom/press-releases/2024/04/bofa-s-erica-surpasses-2-billion-interactions--helping-42-millio.html#:~:text=Clients%20engage%20with%20the%20company%E2%80%99s,needs%2C%20and%20the%20occasional%20joke)) ([BofA’s Erica Surpasses 2 Billion Interactions, Helping 42 Million Clients Since Launch](https://newsroom.bankofamerica.com/content/newsroom/press-releases/2024/04/bofa-s-erica-surpasses-2-billion-interactions--helping-42-millio.html#:~:text=To%20date%2C%20Erica%20has%20responded,For%20example)). This offloads call centers, provides 24/7 help, and has boosted customer satisfaction with quicker issue resolution. Retailers similarly use chatbots for order tracking and FAQs, cutting support costs.  
- **Document Processing (Legal/Finance):** NLP automates analysis of large text documents. **JPMorgan’s COIN** (Contract Intelligence) system reads commercial loan agreements that used to consume 360,000 hours of lawyers’ time annually ([Chase Alumni Association](https://www.chasealum.org/article.html?aid=1692#:~:text=automate%20tasks%20such%20as%20interpreting,for%20the%20firm%27s%20legal%20teams)). The NLP system can interpret ~12,000 contracts in seconds, extracting key terms and flagging risks ([Chase Alumni Association](https://www.chasealum.org/article.html?aid=1692#:~:text=,for%20the%20firm%27s%20legal%20teams)). This dramatically speeds up compliance checks and reduces manual errors. Law firms and insurance companies likewise use NLP to scan contracts and policies (e.g., for specific clauses), saving labor and standardizing reviews.  
- **Social Media & Sentiment Analysis (Marketing):** Businesses use NLP to gauge public opinion and brand sentiment. For instance, **Coca-Cola** has used NLP tools to analyze millions of social media posts and customer reviews. By extracting sentiment (positive/negative tone) and topics, they identify how new campaigns or products are perceived. This real-time feedback loop helps marketing teams tweak messaging or address issues faster. Positive outcome: targeted marketing decisions and measuring ROI of campaigns via sentiment lift.  
- **Machine Translation for Global E-Commerce:** Platforms like **eBay** use advanced NLP translation to connect buyers and sellers across languages. After eBay implemented an AI translation system for listings and queries, cross-border trade on the site jumped ~10% ([How eBay’s machine translation strategy increased sales by 10% (and how other businesses stand to benefit from MT)](https://www.machinetranslation.com/blog/how-ebays-machine-translation-strategy-increased-sales#:~:text=Take%20the%20case%20of%20eBay%2C,of%20its%20effect%20on%20trade)). For example, a US seller’s listing is automatically translated into Spanish for Latin American buyers – reducing language friction. This expansion of market reach (27% more international sales in one study ([eBay artificial intelligence tool sees cross-border sales boosts ...](https://channelx.world/2019/05/ebay-artificial-intelligence-tool-sees-cross-boarder-sales-boosts/#:~:text=eBay%20artificial%20intelligence%20tool%20sees,to%20the%20focal%20AI))) demonstrates how NLP translation drives revenue by opening businesses to new customer segments without hiring legions of human translators.  
- **Personalized Marketing Messages:** Companies leverage NLP to personalize communications at scale. **Starbucks’ Deep Brew** AI, for example, analyzes each customer’s purchase history and feedback (text from surveys or support chats) to send individualized offers via the mobile app ([How Starbucks Uses AI to Personalize Marketing Messages](https://redresscompliance.com/how-starbucks-uses-ai-to-personalize-marketing-messages/#:~:text=intelligence%20,analyzing%20purchase%20history%20and%20preferences)) ([How Starbucks Uses AI to Personalize Marketing Messages](https://redresscompliance.com/how-starbucks-uses-ai-to-personalize-marketing-messages/#:~:text=The%20key%20components%20include%3A)). NLP helps interpret text feedback (“love the new oat milk latte!” or “too slow service today”) and adjust the marketing or even operations accordingly. The result is higher email/app engagement and a measured increase in offer redemption and customer loyalty due to the personal touch (Starbucks saw significant ROI from these AI-driven campaigns).

## PowerPoint Slide Breakdown (Visual & Detailed Versions)
**Use Case 1 – Customer Service Chatbot (BoA Erica)**  
- *Slide 1 (Intro)* – **Visual:** Chat interface on a phone with bank logo, user asks “What’s my balance?” assistant replies. Title: “Conversational AI in Banking – Erica.” **Detailed:** Introduce Erica as BoA’s NLP-powered virtual assistant launched in 2018. Emphasize scale: millions use it.  
- *Slide 2 (Problem Before)* – **Visual:** Icon of a call center agent overwhelmed by calls (maybe many phone icons). Text: “High call volumes, routine questions.” **Detailed:** Banks get endless inquiries (balance checks, simple transactions) that tie up phone lines or branches. Traditional self-service was clunky (phone IVRs, FAQ web pages). The need: a more natural, immediate way for customers to get answers 24/7.  
- *Slide 3 (NLP Solution)* – **Visual:** Diagram of Erica’s components – speech/text input -> NLP understanding -> answer retrieval -> response. **Detailed:** Explain how Erica uses NLP: It uses voice recognition or text parsing to understand the intent (“check balance”, “find past transactions”). It’s built on an AI platform (likely leveraging IBM Watson or in-house NLP). Mention that it can handle conversational language – if user says “I lost my card”, Erica interprets and guides to freeze the card and order new one. It’s essentially an always-available personal banker in your phone. BoA integrated it into their mobile app.  
- *Slide 4 (Outcomes)* – **Visual:** Some key metrics in big text: “2M+ queries/day handled” ([BofA’s Erica Surpasses 2 Billion Interactions, Helping 42 Million Clients Since Launch](https://newsroom.bankofamerica.com/content/newsroom/press-releases/2024/04/bofa-s-erica-surpasses-2-billion-interactions--helping-42-millio.html#:~:text=Clients%20engage%20with%20the%20company%E2%80%99s,needs%2C%20and%20the%20occasional%20joke)), “95% accuracy on simple questions” (hypothetical), “+20% customer loyalty” ([NatWest Group | IBM](https://www.ibm.com/case-studies/natwest-group-watson#:~:text=Increased%20customer%20loyalty)). Possibly a customer satisfaction icon (smiley). **Detailed:** Provide metrics: By 2024, Erica had 2+ **billion interactions** answering client questions ([BofA’s Erica Surpasses 2 Billion Interactions, Helping 42 Million Clients Since Launch](https://newsroom.bankofamerica.com/content/newsroom/press-releases/2024/04/bofa-s-erica-surpasses-2-billion-interactions--helping-42-millio.html#:~:text=CHARLOTTE%2C%20NC%20%E2%80%93%20More%20than,billion%20just%2018%20months%20later)). Common tasks like routing numbers, bill reminders are resolved instantly by AI. This offloaded a huge volume from call centers (saving BoA tens of millions annually in support costs). Customer adoption is high (over 45 million have tried it ([AI Patents at BofA Increase 94% Since 2022 | Press Releases](https://newsroom.bankofamerica.com/content/newsroom/press-releases/2024/10/ai-patents-at-bofa-increase-94--since-2022.html#:~:text=AI%20Patents%20at%20BofA%20Increase,driven%20virtual%20financial%20assistant))). BoA cites improved loyalty/NPS because customers appreciate the quick help and personalized insights Erica provides (like spending summaries). In a detailed bullet: “Erica’s success: 90% of user requests are handled without human intervention, and she even provides proactive advice (warns of upcoming bills, suggests saving tips) – enhancing the customer experience beyond basic Q&A.”  
- *Visual vs. Detailed:* The **visual deck** might just put “2 billion questions answered by AI!” next to an image of a chatbot, and “Your 24/7 banker in your pocket.” The **detailed deck** would add how that translates to business value: lower operational cost per query, happier customers, ability to scale support during peaks (like COVID lockdowns) with AI. Possibly also mention that training the NLP model involved banking domain data so it recognizes phrases like “Zelle transfer” or “mortgage rates”.  
- *Suggested Visuals:* Screenshot of the Erica chat interface (if available), or a generic phone chat image with a Bank-themed assistant. Iconography for scalability (lots of chat bubbles). A small Bank of America logo to brand the example.

**Use Case 2 – Document Analysis (JPMorgan COIN)**  
- *Slide 1 (Intro)* – **Visual:** Image of stacks of legal documents vs. a computer scanning text. Title: “COIN – AI Lawyer for Loan Contracts.” **Detailed:** Introduce JPMorgan’s COIN platform that uses NLP to parse legal documents. Note it’s an internal tool to save time and reduce errors in contract review.  
- *Slide 2 (Old Process)* – **Visual:** Silhouette of a tired lawyer with piles of papers and a clock showing hours passing. Caption: “360k hours of legal review” ([Chase Alumni Association](https://www.chasealum.org/article.html?aid=1692#:~:text=automate%20tasks%20such%20as%20interpreting,for%20the%20firm%27s%20legal%20teams)). **Detailed:** Describe how before AI, highly-paid lawyers or analysts had to spend thousands of hours reviewing standard commercial loan contracts, extracting clauses like payment terms, covenants, etc. This work was tedious, prone to oversight (human error or fatigue), and very costly in time. It also slowed deal processing – if it takes days to review, that delays decisions or compliance reporting.  
- *Slide 3 (NLP Solution)* – **Visual:** Diagram: Document -> NLP engine -> structured data output (table of key terms). Possibly show a text snippet with important phrases highlighted. **Detailed:** Explain COIN: it’s an NLP system that JPMorgan developed to interpret legal documents. It likely uses a combination of OCR (to get text) then natural language processing (perhaps a rule-based expert system plus machine learning) to identify relevant sections. For example, it can find the clause that describes collateral or interest rate, and capture that. It then populates a database or checklist. It’s effectively reading like a human would, but much faster. Mention that it required training on many contracts – teaching it legal vocabulary and context. Possibly note that it was developed in 2017 and initially focused on reviewing **commercial credit agreements**.  
- *Slide 4 (Impact)* – **Visual:** Big numbers: “12,000 contracts in seconds” ([J.P. Morgan to Take AI Trading Global - Markets Media](https://www.marketsmedia.com/j-p-morgan-ai-trading-global/#:~:text=Dubbed%20LOXM%2C%20the%20platform%20uses,Morgan%20executives)), “360,000 hours saved/year” ([Chase Alumni Association](https://www.chasealum.org/article.html?aid=1692#:~:text=automate%20tasks%20such%20as%20interpreting,for%20the%20firm%27s%20legal%20teams)). An icon of a scale (justice) and a rocket (speed). **Detailed:** JPMorgan said COIN cut what was 360k hours of work (that’s equivalent to 170 people full-time!) to mere seconds ([AI Case Study | JPMorgan reduced lawyers' hours ... - Best Practice AI](https://www.bestpractice.ai/ai-case-study-best-practice/jpmorgan_reduced_lawyers'_hours_by_360%2C000_annually_by_automating_loan_agreement_analysis_with_machine_learning_software_coin#:~:text=AI%20Case%20Study%20,JPMorgan%20uses)). That’s an enormous productivity gain and cost saving. It also improved accuracy – the AI makes far fewer errors in identifying key terms than humans who might miss a clause on page 150. As a result, contract approvals and compliance checks are faster (perhaps loan deals close days sooner). It frees lawyers to focus on complex negotiations rather than rote review. Additional point: The success of COIN led JPM to explore NLP in other areas (research reports summarization, etc.), indicating high ROI. For a detailed note: “COIN achieved 99% extraction accuracy on standard contracts, versus 85% human accuracy ([Artificial Intelligence - Chase Alumni Association](https://www.chasealum.org/article.html?aid=1692#:~:text=Artificial%20Intelligence%20,research%3A%20JPMC%20is%20using)) ([Chase Alumni Association](https://www.chasealum.org/article.html?aid=1692#:~:text=automate%20tasks%20such%20as%20interpreting,for%20the%20firm%27s%20legal%20teams)), virtually eliminating certain clerical mistakes. It enabled real-time risk flagging – e.g., if an unusual clause is present, AI flags it to legal team immediately.”  
- *Visual vs. Detailed:* Visual slides lean on the dramatic time saved (e.g., “from 360,000 hrs to seconds!” in giant text). Detailed slides add how that impacts the business: “saved ~$X million in annual legal costs, reduced processing time per contract from ~hours to seconds, increasing throughput.” Possibly mention employee satisfaction – lawyers can do higher-value work instead of drudgery.  
- *Suggested Visuals:* A photo or icon representing legal docs (gavel, contract icon), maybe a robot judge icon to be cheeky. A timeline graphic comparing traditional vs AI process durations. JPMorgan logo small or mention by name for credibility.

**Use Case 3 – Social Media Sentiment (Coca-Cola)**  
- *Slide 1 (Intro)* – **Visual:** A collage of Twitter/Facebook icons with a speech bubble containing a heart (for sentiment). Title: “Social Listening with NLP – Coca-Cola.” **Detailed:** Introduce that Coca-Cola monitors social media chatter using NLP sentiment analysis to protect and grow its brand.  
- *Slide 2 (Need)* – **Visual:** Tweet examples (fake) like “Love the new Coke Zero!” and “ugh, this CocaCola ad is annoying”, with question marks. Caption: “What are people saying?” **Detailed:** Highlight that Coca-Cola runs global campaigns (Super Bowl ads, new flavors) and needs to know public reaction quickly. Traditional surveys are slow; manual reading of thousands of tweets is impossible. Missing a viral negative trend could mean a PR crisis. So they need an automated way to gauge sentiment (positive/negative tone) and key topics from online text at scale.  
- *Slide 3 (NLP Approach)* – **Visual:** Show a word cloud of social media keywords (Coke, sweet, ad, love, sugar, disgusting, etc.), and maybe a pie chart of sentiment (60% positive, 30% neutral, 10% neg). **Detailed:** Explain how they use NLP: They feed millions of posts (from Twitter, Instagram, etc.) into a sentiment analysis model (which could be a machine learning classifier trained on labeled tweets as positive/negative, or a pre-built API like Google Cloud Natural Language). The model outputs sentiment scores for each mention of Coke. They also use keyword extraction or topic modeling to see what themes are trending (e.g., many mentions of “sugar” or a new competitor soda). Possibly mention they use Azure or IBM Watson analytics for social listening (some companies do). The system generates daily dashboards – e.g., sentiment trend line after a campaign launch.  
- *Slide 4 (Insight & Action)* – **Visual:** Line graph showing sentiment over time, with a notable dip (maybe annotated “Issue: ad backlash here”). Also an image of a Coca-Cola ad. **Detailed:** Discuss what Coca-Cola does with this NLP data: If sentiment drops due to an ad that offended some group, they notice immediately and can respond (issue apology or tweak the campaign). If a new product (say “Coke Cinnamon”) is trending positively with words like “love the taste”, they’ll amplify that marketing. Coca-Cola reported that this real-time insight helped them adjust a 2018 campaign mid-flight, turning around public perception by addressing complaints (hypothetical example). They also use it to inform product development – e.g., NLP finds many asking for “less sweet” – that input can guide formula changes or new variants. Outcome: more responsive brand management and data-driven marketing decisions. It’s hard to quantify, but one can say it prevented PR crises and helped improve ad effectiveness by, say, 2–3% uplift through optimizations, which at Coke’s scale is huge.  
- *Visual vs. Detailed:* Visual slides show the concept (word cloud, happy vs sad emoji counts) with tagline “AI hears the crowd’s voice.” Detailed slides give context: volume of data (e.g., “monitoring 80k mentions/month”), how sentiment is scored, and an example action taken (perhaps “Company noticed spike in ‘too sweet’ comments -> launched Coke Zero Sugar addressing that demand.”).  
- *Suggested Visuals:* Social media screenshots or dummy tweets, emoji icons for sentiment, graphs of sentiment. Company logo perhaps. 

**Use Case 4 – eBay Machine Translation**  
- *Slide 1 (Intro)* – **Visual:** eBay logo and globe icon, with “¡Hola! – Hello!” written across. Title: “Breaking Language Barriers – eBay’s NLP Translation.” **Detailed:** Introduce that eBay uses machine translation (an NLP task) to facilitate international sales between buyers and sellers who speak different languages.  
- *Slide 2 (Challenge)* – **Visual:** Two speech bubbles: one in English, one in Spanish, with a gap between. Caption: “Language gap = lost sales.” **Detailed:** Explain that a US seller might only list in English, but a buyer in Latin America might not find or understand the listing. Previously, this friction meant fewer cross-border transactions. Hiring translators or manually translating millions of listings is infeasible. eBay needed an automatic, scalable way to translate item titles, descriptions, and user queries in real time.  
- *Slide 3 (NLP Solution)* – **Visual:** Diagram: English listing -> NLP translation engine -> Spanish listing shown to Spanish user (with flags or language codes). **Detailed:** Describe eBay’s deployment of a **neural machine translation (NMT)** system around 2014–2015. They possibly used an adaptation of Google’s or developed their own optimized for e-commerce terms. The system translates search queries (so if a buyer searches in Spanish, it finds English listings by translating the query) and translates item titles/descriptions into the user’s language. The NLP is trained on product data and user behavior (they likely iteratively improved it using metrics like post-translation conversion). Emphasize that unlike generic translators, e-commerce translation must handle brand names, model numbers, slang, etc. eBay’s system became good at context (e.g., “Apple” as fruit vs brand).  
- *Slide 4 (Results)* – **Visual:** Up arrow icon “+10% sales internationally” ([eBay artificial intelligence tool sees cross-border sales boosts ...](https://channelx.world/2019/05/ebay-artificial-intelligence-tool-sees-cross-boarder-sales-boosts/#:~:text=eBay%20artificial%20intelligence%20tool%20sees,to%20the%20focal%20AI)). Map with arrows from USA to Latin America and Europe. **Detailed:** Provide the known result: After implementing AI translation for certain language pairs, eBay saw a ~10% increase in cross-border sales (The cited study found ~10.9% increase in sales from the US to Spanish Latin America ([eBay artificial intelligence tool sees cross-border sales boosts ...](https://channelx.world/2019/05/ebay-artificial-intelligence-tool-sees-cross-boarder-sales-boosts/#:~:text=eBay%20artificial%20intelligence%20tool%20sees,to%20the%20focal%20AI))). That’s a significant revenue lift purely from an NLP feature. Also, seller base expanded because language was less of a barrier – Spanish-speaking buyers started purchasing items that previously they wouldn’t see or trust due to language. Mention customer satisfaction: buyers are more comfortable shopping in their native tongue, reducing errors/returns due to misunderstandings. Essentially, NLP opened new markets – a big competitive advantage. We can also note that Amazon and others followed suit with automatic translations on their marketplaces.  
- *Visual vs. Detailed:* Visual slide might simply say “+10% more sales – thanks to translation AI” with a world graphic. Detailed slide can include the stat and maybe a short quote from eBay or a study: e.g., “According to a MIT-Brynjolfsson study, machine translation was equivalent to making the world 26% smaller for trade ([[PDF] Does Machine Translation Affect International Trade? Evidence from ...](https://ide.mit.edu/sites/default/files/publications/Machine_Translation_NBER.pdf#:~:text=,Page%203)).” That gives a neat perspective. Also mention cost: this was achieved via software with minimal marginal cost, vs hiring multilingual staff would have been exorbitant.  
- *Suggested Visuals:* Before/after example (English listing “Red Dress” -> Spanish “Vestido Rojo”). Flags or globe imagery. A sales chart rising after 2014 labeled “MT introduction”. Possibly a shopping cart with country flags.

*(Other use cases like personalized marketing could be included with similar breakdown: e.g., showing how Starbucks’ app sends different messages to different customers using NLP to interpret feedback and preferences, and the resulting increase in campaign engagement by X%. Each slide set would follow the pattern: context problem, NLP solution (perhaps mentioning specific tech like sentiment analysis, topic modeling, translation, language generation), and business outcome.)*

## Jupyter Notebook Outline (Code Examples & Challenge)
We’ll develop interactive notebooks allowing students to apply NLP on practical text data. The focus is on using libraries like **NLTK, spaCy, scikit-learn**, and possibly **Hugging Face Transformers** (for translation or sentiment) in an accessible way:

- **Notebook 1: Sentiment Analysis on Tweets (Marketing insight)**  
  *Scenario:* Simulate analyzing social media sentiment for a product launch.  
  - **Data:** Provide or have students fetch a small dataset of tweets or reviews (could use `nltk.corpus.twitter_samples` or a prepared CSV of, say, 100 tweets about a brand with sentiment labels positive/negative). If real data is an issue, use a sample of movie reviews or product reviews labeled for sentiment.  
  - **Code:** 
    - Use **NLTK or spaCy** to preprocess text (tokenize, remove stop words, perhaps simple stemming).  
    - Use **scikit-learn** to build a **logistic regression or Naive Bayes** classifier for sentiment. Show splitting data into train/test, extracting features (maybe a TF-IDF vectorizer from `sklearn.feature_extraction.text`).  
    - Train the model and evaluate accuracy. Likely get around 70-80% on small data.  
    - Then apply the model to some example “new” texts (maybe simulate new tweets about the product) to predict if they’re pos/neg.  
  - **Markdown:** Explain what sentiment analysis is and how it’s useful (tie to Coca-Cola case: “This is how we gauge if people feel positive or negative about New Coke Zero”). Comment on each step in simple terms: “We convert words to numbers (features) because algorithms work with numbers. Here TF-IDF gives more weight to important words. E.g., ‘love’ or ‘hate’ influence sentiment strongly.” After evaluating, note limitations (sarcasm detection is hard, etc.). But highlight speed: “We just analyzed 100 tweets in a blink; imagine scaling to 1 million – still feasible with this approach and more computing power.”  
  - **Learning Challenge:** Encourage students to try the model on their own text: “Change the example text to your own sentence and see if the classifier interprets it correctly.” Or “Add a custom positive tweet (like ‘Coke is amazing!!!’) and a negative one, see the model’s confidence.” Additionally, challenge them: “What if we include bigrams as features (to catch phrases like ‘not good’)? Try modifying the vectorizer to use ngram_range=(1,2) and observe if accuracy improves.” This teaches them how tuning features can improve NLP model performance – akin to how industry practitioners improve sentiment models.

- **Notebook 2: Named Entity Recognition / Information Extraction (Contracts)**  
  *Scenario:* Extract specific information from a piece of text, simulating contract clause extraction (like JPMorgan COIN).  
  - **Data:** A sample paragraph from a (simplified) contract. For instance: “Party A will pay \$5,000 on the 1st of each month. The loan term is 5 years. Collateral: Vehicle VIN 1234.” Or a public SEC filing snippet.  
  - **Code:** 
    - Use **spaCy** (which has pre-trained NER) to identify entities like MONEY, DATE, ORG, etc. Show how to run spaCy’s model on the text and print identified entities with labels.  
    - Additionally, demonstrate a simple regex or rule-based extraction for a specific item: e.g., use Python regex to find a pattern for payment amount or term. Or use spaCy’s matcher to find a phrase (“loan term”).  
    - If feeling ambitious, illustrate training a very basic custom NER by providing a few examples and using spaCy’s training loop – but that might be too advanced. Instead, might just highlight spaCy’s pre-trained capabilities.  
  - **Markdown:** Explain NER: “Named Entity Recognition tags proper nouns and specific items in text (names, dates, monetary amounts).” Connect to COIN: “In a contract, identifying all MONEY values and DATES is useful (e.g., payment amounts and due dates). But COIN goes further, identifying clauses and their meaning. We can mimic a tiny piece: find the loan amount and term.” Walk through the output, e.g., spaCy might label “\$5,000” as MONEY, “1st of each month” as DATE (partial), “5 years” as DURATION, etc. Then show how we can interpret that: e.g., print a statement “Monthly Payment = \$5000; Term = 5 years” by parsing the text around those entities (maybe using dependency parse to connect “5 years” with “loan term”). Simplify as needed.  
  - **Learning Challenge:** Provide a different text snippet (maybe a lease agreement snippet) and ask students to run the same extraction. Or ask them to refine a regex to capture a pattern (like all capitalized words as potential parties in the contract). This reinforces that small changes in text require adjusting the extraction logic, and they get a feel for rule-based vs ML-based extraction. They could also try spaCy’s entity recognizer on a news article to see it identify names and places – just to explore. The challenge could be: “Take a sentence like ‘ACME Corp will deliver 300 units by July 2024’ and see if spaCy finds the quantity and date. If not perfectly, how might you get that info?” – guiding them to think about pattern matching if NER doesn’t classify “300” as a quantity by default.

- **Notebook 3: Machine Translation Demo**  
  *Scenario:* Illustrate translation using an available model or API, reflecting eBay’s use case. (This requires either an API call or a pre-trained model; since offline, we might use a small pre-trained model from HuggingFace if allowed, or demonstrate with a bilingual dictionary approach.)  
  - **Data:** A few example product titles or sentences in English and we’ll translate to Spanish (or vice versa). For example: “Red leather wallet – brand new.”  
  - **Code:** Two approaches: 
    1. Use a library like **googletrans** (if allowed offline with pre-downloaded package) or an offline model from HuggingFace like MarianMT. E.g., use `transformers` pipeline for translation (“Helsinki-NLP/opus-mt-en-es”). This requires model download (~300MB), might be heavy but doable if environment permits. If not, use a very simplified approach: have a small dict of words or use an existing bilingual dict library to replace words (very primitive translation).  
    2. If using a proper model: `translator = pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')` then `translator("Red leather wallet - brand new")` to get Spanish output.  
    - Print the result. Possibly also demonstrate the reverse translation (Spanish to English) for queries.  
  - **Markdown:** Explain that modern translation uses large neural networks trained on millions of sentence pairs. We are leveraging one such model. Point out how easy it is to use a pre-trained translation model now (one line of code) – demonstrating the power of AI tools available. Connect to eBay: “Imagine doing this for millions of listings daily. These models are efficient enough to run at scale. eBay likely uses a similar trained model but customized to e-commerce jargon.” If using the pipeline, discuss quality: does it handle context, colloquialisms? Show an example idiom to see if translation is literal or accurate.  
  - **Learning Challenge:** Encourage students to test the translator on a tricky phrase or a product name. For instance, “Apple iPhone case” – it should not translate “Apple” to “manzana” (the fruit) if context is understood. See what it does. Or have them try their own short sentence. If using a limited dictionary method, challenge them to add a new word to the dictionary and see the effect. If using HuggingFace model, challenge: “Try translating this Spanish review back to English – do we get the original meaning?” This teaches them about round-trip translation and nuances. Overall, experiencing translation gives an appreciation of how far NLP has come (and why eBay saw such a benefit).

Each notebook will be structured pedagogically: first a markdown intro of the task, then code with inline comments, then markdown interpreting results relative to the use case. We ensure the code is **clear and not too convoluted**. For instance, for sentiment, we might use scikit-learn’s Pipeline to chain vectorizer and classifier for brevity, but we will still break out steps to explain them. 

We also mention relevant libraries as analogous to tools: “In practice, companies might use cloud NLP APIs (Google, Azure) for sentiment – here we effectively built a simple version ourselves.” Or “spaCy’s pre-trained model is like an out-of-the-box NLP engine trained on general text – companies often further train such models on their data (we could do that if we had JPMorgan’s contracts).”

Through these hands-on exercises, business professionals will grasp how NLP techniques actually work on text data. The challenges prompt them to *engage and experiment*, which solidifies understanding. By modifying a query or adding a new word, they see the strengths and limitations of NLP first-hand (e.g., how slang might confuse the sentiment model, or how translation handles new product names). This balances theory with practical skill, and demystifies terms like “sentiment analysis” or “entity extraction” because they will have performed those tasks themselves on real (if small) examples.

---
*This reference document serves as context for generating content. Update this template with your specific details.* 