{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "## Welcome to Deep Learning for Business Applications!\n",
    "\n",
    "In this section, we introduce the concept of deep learning and its significance in modern AI. Deep learning leverages neural networks (including Convolutional Neural Networks and Deep Generative Models) to solve complex problems.\n",
    "\n",
    "Key Topics:\n",
    "\n",
    "- Neural Networks\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "- Generative Models (e.g., Diffusion Models)\n",
    "Question for Reflection:\n",
    "\n",
    "- How do you think deep learning can create a competitive edge in business analytics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Learning?\n",
    "\n",
    "### Understanding Deep Learning\n",
    "\n",
    "Deep learning is a subset of machine learning that uses artificial neural networks to automatically extract features from large amounts of data. It has become a cornerstone for advancements in computer vision, natural language processing, and beyond.\n",
    "\n",
    "Question for Reflection:\n",
    "\n",
    "- How does automatic feature extraction differ from traditional manual feature engineering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network with one hidden layer\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=5):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Create the model instance\n",
    "model = SimpleNN(input_size=5)\n",
    "print(\"Simple Neural Network Architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "The printed model architecture shows how a simple neural network is built in PyTorch using linear layers and activation functions. Notice how the network structure clearly defines the forward pass.\n",
    "\n",
    "Question:\n",
    "\n",
    "- Why might a business prefer a deep learning approach over traditional machine learning when dealing with complex datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "### The Perceptron – The Building Block of Neural Networks\n",
    "\n",
    "The perceptron is the simplest type of artificial neuron used for binary classification. It computes a weighted sum of inputs and passes the result through an activation function.\n",
    "\n",
    "Question for Reflection:\n",
    "\n",
    "- Can you think of a simple business decision that could be modeled as a binary classification problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        # Initialize weights with small random values, +1 for the bias term\n",
    "        self.weights = np.random.randn(input_size + 1) * 0.01\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Add bias input of 1\n",
    "        x = np.insert(x, 0, 1)\n",
    "        # Compute the weighted sum\n",
    "        weighted_sum = np.dot(x, self.weights)\n",
    "        # Apply the step activation function\n",
    "        return step_function(weighted_sum)\n",
    "\n",
    "    def train(self, X, y, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                xi_bias = np.insert(xi, 0, 1)\n",
    "                prediction = self.predict(xi)\n",
    "                error = target - prediction\n",
    "                self.weights += self.learning_rate * error * xi_bias\n",
    "\n",
    "# Example data: OR logic gate (simple binary classification)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([0, 1, 1, 1])\n",
    "\n",
    "# Train the perceptron\n",
    "perceptron = Perceptron(input_size=2)\n",
    "perceptron.train(X, y, epochs=10)\n",
    "\n",
    "# Test the perceptron\n",
    "print(\"Perceptron predictions for OR gate:\")\n",
    "for xi in X:\n",
    "    print(f\"Input {xi} -> Predicted: {perceptron.predict(xi)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "This simple perceptron model learned the OR gate logic. Even though it’s basic, it forms the foundation for more complex deep learning architectures.\n",
    "\n",
    "Question:\n",
    "\n",
    "- How would you extend this model to handle multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Activation Functions\n",
    "\n",
    "### Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions introduce non-linearity to the network, allowing it to learn complex patterns. Common functions include:\n",
    "\n",
    "- Sigmoid\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Tanh\n",
    "- Softmax\n",
    "\n",
    "Question for Reflection:\n",
    "\n",
    "- How might the choice of activation function impact model performance and training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "# Create an array of values\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Compute the functions\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_relu = relu(x)\n",
    "y_tanh = tanh(x)\n",
    "\n",
    "# For softmax, demonstrate on a 2-element vector for each x value (this is a bit contrived)\n",
    "# Here, we compute softmax on two values: x and 0.\n",
    "y_softmax = np.array([softmax([val, 0]) for val in x])\n",
    "y_softmax_first_component = y_softmax[:, 0]\n",
    "\n",
    "# Plot the activation functions\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, y_sigmoid, label='Sigmoid')\n",
    "plt.title('Sigmoid Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, y_relu, label='ReLU', color='orange')\n",
    "plt.title('ReLU Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x, y_tanh, label='Tanh', color='green')\n",
    "plt.title('Tanh Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x, y_softmax_first_component, label='Softmax (Component 1)', color='red')\n",
    "plt.title('Softmax Activation (First Component)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "The plots illustrate how each activation function behaves over a range of inputs. Notice how ReLU simply thresholds at 0, while Sigmoid and Tanh smoothly saturate at their extremes.\n",
    "\n",
    "Question:\n",
    "\n",
    "- Based on these plots, when might you prefer using ReLU over Sigmoid in a deep learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Networks with Perceptrons\n",
    "\n",
    "### Stacking Perceptrons to Build Neural Networks\n",
    "\n",
    "By combining multiple perceptrons, we can create multi-layer neural networks that capture more intricate patterns in the data. This is the basic idea behind deep architectures.\n",
    "\n",
    "Question for Reflection:\n",
    "\n",
    "- What advantages do deep (multi-layer) architectures offer compared to single-layer networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple MLP for binary classification\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 16)\n",
    "        self.layer2 = nn.Linear(16, 8)\n",
    "        self.layer3 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Create and print the model\n",
    "model_mlp = SimpleMLP()\n",
    "print(\"Simple MLP Architecture:\")\n",
    "print(model_mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "This MLP model shows how stacking layers (each a collection of perceptrons) allows the network to learn complex, non-linear relationships. In business applications, such models are used for tasks such as customer segmentation and fraud detection.\n",
    "\n",
    "Question:\n",
    "\n",
    "- How might increasing the number of layers or neurons affect the model’s ability to generalize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Loss\n",
    "\n",
    "### Measuring Model Performance with Loss Functions\n",
    "\n",
    "Loss functions quantify how well a neural network’s predictions match the actual targets. For regression, we often use Mean Squared Error (MSE), and for classification, Cross-Entropy Loss is popular.\n",
    "\n",
    "Question for Reflection:\n",
    "\n",
    "- Why is minimizing the loss function critical for successful model training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dummy data: true values and predictions\n",
    "y_true = np.array([3.0, -0.5, 2.0, 7.0])\n",
    "y_pred = np.array([2.5, 0.0, 2.1, 7.8])\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = np.mean((y_true - y_pred) ** 2)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "The Mean Squared Error (MSE) gives us a measure of the average squared difference between our predictions and the actual values. A lower MSE indicates a better performing model.\n",
    "\n",
    "Question:\n",
    "\n",
    "- What might be some limitations of using MSE in certain business applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks\n",
    "\n",
    "### Training Neural Networks: From Backpropagation to Gradient Descent\n",
    "\n",
    "Training involves adjusting the model’s weights to minimize the loss function. Techniques like backpropagation compute the gradients, and gradient descent updates the weights.\n",
    "\n",
    "Question for Reflection:\n",
    "\n",
    "- What challenges might you face when training a deep network on real-world business data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and prepare the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target.astype(np.int64)  # CrossEntropyLoss expects class indices\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define a simple network for Iris classification\n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)  # 3 output classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # raw logits; CrossEntropyLoss applies softmax internally\n",
    "        return x\n",
    "\n",
    "model_train = IrisNet()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_train.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model_train.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_train(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on test data\n",
    "model_train.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model_train(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "The model trains on the Iris dataset using PyTorch. We monitor the training process by adjusting the model’s parameters to minimize the loss. This exercise demonstrates the core idea behind model optimization.\n",
    "\n",
    "Question:\n",
    "\n",
    "- How could you apply similar training techniques to a customer churn prediction model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "### CNNs for Image Processing\n",
    "\n",
    "Convolutional Neural Networks are designed to extract spatial features from images through convolution and pooling operations. They have transformed industries such as healthcare, security, and autonomous driving.\n",
    "\n",
    "Question for Reflection:\n",
    "\n",
    "- Why are convolutional layers particularly effective for visual data analysis in a business context (e.g., quality control in manufacturing)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert to tensor and scale to [0,1]\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # standard normalization for MNIST\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define a simple CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)  # from 1 to 32 channels\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        # Calculate the size of the flattened feature map after conv and pooling\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model_cnn = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Train the CNN for a few epochs (for demonstration)\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model_cnn.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cnn(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "# Evaluate the CNN on test data\n",
    "model_cnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model_cnn(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy on MNIST: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "This CNN model demonstrates how convolutional and pooling layers work together to extract features from images. Such techniques are widely used in business for image-based quality control and visual inspection.\n",
    "\n",
    "Question:\n",
    "\n",
    "- How might you modify this model if you were to apply it to higher-resolution business images (e.g., satellite imagery for crop monitoring)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Models\n",
    "\n",
    "### Diffusion Models in Generative AI\n",
    "\n",
    "Diffusion models generate high-quality synthetic images (and other data types) by iteratively refining noisy inputs. Although the full models are complex, we can simulate the idea of iterative noise reduction.\n",
    "\n",
    "Question for Reflection:\n",
    "\n",
    "- In what ways could generative models transform industries such as marketing or product design?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Create a simple grayscale image (e.g., a centered square)\n",
    "image = np.zeros((64, 64))\n",
    "image[20:44, 20:44] = 1  # a white square on a black background\n",
    "\n",
    "# Function to add noise\n",
    "def add_noise(img, noise_level=0.3):\n",
    "    noise = noise_level * np.random.randn(*img.shape)\n",
    "    return np.clip(img + noise, 0, 1)\n",
    "\n",
    "# Simulate the diffusion process: iteratively denoise the image\n",
    "noisy_image = add_noise(image, noise_level=0.5)\n",
    "iterations = 10\n",
    "denoised_images = [noisy_image]\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Apply a Gaussian filter to simulate denoising (a stand-in for diffusion model steps)\n",
    "    denoised = gaussian_filter(denoised_images[-1], sigma=1)\n",
    "    denoised_images.append(denoised)\n",
    "\n",
    "# Plot the progression\n",
    "fig, axs = plt.subplots(2, 6, figsize=(15, 5))\n",
    "axs = axs.ravel()\n",
    "for i, img in enumerate(denoised_images):\n",
    "    axs[i].imshow(img, cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(f\"Step {i}\")\n",
    "plt.suptitle(\"Iterative Denoising (Diffusion-like Process)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "This simulation shows how an initially noisy image can be gradually refined through repeated processing steps. In real diffusion models, this process is learned and used to generate entirely new images.\n",
    "\n",
    "Question:\n",
    "\n",
    "- How do you think the iterative refinement process in diffusion models could be applied to generate synthetic data for business simulations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Limitations\n",
    "\n",
    "### Limitations and Challenges in Deep Learning\n",
    "\n",
    "Despite their power, neural networks have limitations including:\n",
    "\n",
    "The need for large labeled datasets\n",
    "High computational costs\n",
    "Interpretability issues and the risk of overfitting\n",
    "Question for Reflection:\n",
    "\n",
    "What strategies could a business implement to overcome these challenges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the underlying function: y = x^2 with a bit of noise\n",
    "def generate_data(x):\n",
    "    noise = 0.05 * np.random.randn(*x.shape)\n",
    "    return x**2 + noise\n",
    "\n",
    "# Create the training data: 20 samples from a uniform distribution in [-1, 1]\n",
    "n_train = 20\n",
    "X_train = np.random.uniform(-1, 1, (n_train, 1)).astype(np.float32)\n",
    "y_train = generate_data(X_train)\n",
    "\n",
    "# Create validation data:\n",
    "# 30 samples (about 30%) similar to training: uniform(-1, 1)\n",
    "n_val_similar = 30\n",
    "X_val_similar = np.random.uniform(-1, 1, (n_val_similar, 1)).astype(np.float32)\n",
    "y_val_similar = generate_data(X_val_similar)\n",
    "\n",
    "# 70 samples (about 70%) out-of-distribution: uniform(2, 3)\n",
    "n_val_out = 70\n",
    "X_val_out = np.random.uniform(2, 3, (n_val_out, 1)).astype(np.float32)\n",
    "y_val_out = generate_data(X_val_out)\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors with explicit float32 type\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_similar_tensor = torch.tensor(X_val_similar, dtype=torch.float32)\n",
    "y_val_similar_tensor = torch.tensor(y_val_similar, dtype=torch.float32)\n",
    "X_val_out_tensor = torch.tensor(X_val_out, dtype=torch.float32)\n",
    "y_val_out_tensor = torch.tensor(y_val_out, dtype=torch.float32)\n",
    "\n",
    "# Define an over-parameterized neural network to encourage overfitting\n",
    "class OverfitNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OverfitNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = OverfitNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model for many epochs to force overfitting\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "val_similar_losses = []\n",
    "val_out_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs_train = model(X_train_tensor)\n",
    "    loss_train = criterion(outputs_train, y_train_tensor)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss_train.item())\n",
    "    \n",
    "    # Validation phase: evaluate on both similar and out-of-distribution data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Similar validation data\n",
    "        outputs_val_similar = model(X_val_similar_tensor)\n",
    "        loss_val_similar = criterion(outputs_val_similar, y_val_similar_tensor)\n",
    "        val_similar_losses.append(loss_val_similar.item())\n",
    "        \n",
    "        # Out-of-distribution validation data\n",
    "        outputs_val_out = model(X_val_out_tensor)\n",
    "        loss_val_out = criterion(outputs_val_out, y_val_out_tensor)\n",
    "        val_out_losses.append(loss_val_out.item())\n",
    "\n",
    "# Plot training and both validation losses over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_similar_losses, label='Validation Loss (Similar)')\n",
    "plt.plot(val_out_losses, label='Validation Loss (Out-of-Distribution)')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Overfitting Demonstration with Out-of-Distribution Data\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "The loss curves indicate that while the model continues to reduce training error, the validation error starts to rise—an indication of overfitting. In business applications, overfitting can lead to models that perform well in the lab but fail in real-world situations.\n",
    "\n",
    "Question:\n",
    "\n",
    "- What regularization techniques or data strategies could be used to prevent overfitting in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Customer Subscription Prediction using PyTorch\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this assignment, you will build a binary classification model using PyTorch. Your model will predict whether a customer will purchase a subscription service based on a small, synthetic dataset representing customer engagement and spending behavior. The goal is to apply deep learning to a business use case, and your final output will be the test accuracy of your model. For this assignment, your model should achieve a test accuracy of at least 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions\n",
    "### 1. Dataset Generation (20 minutes)\n",
    "- Generate a synthetic dataset with 100 samples.\n",
    "Each sample should include 4 features representing customer data:\n",
    "\n",
    "    - logins_per_month: Integer (simulate values between 0 and 30)\n",
    "    - monthly_spending: Float (simulate values between 0 and 1000)\n",
    "    - support_tickets: Integer (simulate values between 0 and 10)\n",
    "    - customer_satisfaction: Float (simulate values between 1 and 5)\n",
    "- Create a target variable purchase (binary: 1 for purchase, 0 for no purchase) using a simple rule. For example, you might compute a “score” as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0.05 * logins_per_month + 0.001 * monthly_spending + 0.1 * support_tickets + 0.2 * customer_satisfaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, if score > 1.5 (you can adjust the threshold if needed to create a roughly balanced dataset), set purchase to 1; otherwise, set it to 0.\n",
    "\n",
    "- Split the dataset into:\n",
    "\n",
    "    - Training set: 70 samples\n",
    "    - Test set: 30 samples\n",
    "    \n",
    "Hint: Use random splitting (set a random seed for reproducibility)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Building (20 minutes)\n",
    "- Build a multi-layer perceptron (MLP) using PyTorch:\n",
    "    - Your model should have at least one hidden layer.\n",
    "    - Use a non-linear activation function (e.g., ReLU) in the hidden layer(s).\n",
    "    - The output layer should be set up for binary classification. (Hint: Use one output neuron with a sigmoid activation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training (20 minutes)\n",
    "- Train your model using:\n",
    "    - Binary Cross-Entropy Loss as your loss function.\n",
    "    - An optimizer such as Adam.\n",
    "    - Train for at least 100 epochs.\n",
    "- Monitor the training loss. You can print the loss every 10 or 20 epochs to check that the loss is decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation (10–20 minutes)\n",
    "- Evaluate your model on the test set:\n",
    "\n",
    "    - Convert the model’s predicted probabilities to binary predictions (Hint: Use torch.round).\n",
    "    - Calculate the test accuracy (the fraction of correct predictions).\n",
    "- Print the test accuracy.\n",
    "Your final output should be similar to:\n",
    "\n",
    "Note: If your model’s test accuracy is below 80% (i.e., less than 0.80), then your solution is considered incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test Accuracy: 0.85\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Submission\n",
    "- Submit your completed Jupyter Notebook that contains all the code and outputs.\n",
    "- Ensure your code runs without errors and prints out the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Criteria\n",
    "Your assignment solution will be considered correct if:\n",
    "\n",
    "- The synthetic dataset is generated with 100 samples and 4 features, and the target variable is computed as described.\n",
    "- The data is split correctly into training (70 samples) and test (30 samples) sets.\n",
    "- A PyTorch neural network (MLP) with at least one hidden layer is built and trained for at least 100 epochs.\n",
    "- The model uses Binary Cross-Entropy Loss and an optimizer (e.g., Adam).\n",
    "- The final printed test accuracy is at least 80%.\n",
    "- Your Notebook runs without errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints and Tips\n",
    "- Data Conversion: Make sure you convert your NumPy arrays to PyTorch tensors and set the appropriate data type (e.g., torch.float32).\n",
    "- Activation Function: In the output layer, use torch.sigmoid to obtain probabilities.\n",
    "- Prediction: Use torch.round to convert probabilities into binary predictions (0 or 1).\n",
    "- Reproducibility: Set random seeds for both NumPy and PyTorch to ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Example: converting a NumPy array to a float32 tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Outline (Skeleton Code)\n",
    "Below is a rough outline of the code structure. You do not need to follow this exactly, but it may help you organize your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Dataset Generation\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate features\n",
    "n_samples = 100\n",
    "logins = np.random.randint(0, 31, (n_samples, 1))\n",
    "spending = np.random.uniform(0, 1000, (n_samples, 1))\n",
    "tickets = np.random.randint(0, 11, (n_samples, 1))\n",
    "satisfaction = np.random.uniform(1, 5, (n_samples, 1))\n",
    "\n",
    "# Combine features into a single dataset\n",
    "X = np.hstack([logins, spending, tickets, satisfaction]).astype(np.float32)\n",
    "\n",
    "# Compute target variable\n",
    "score = 0.05 * logins + 0.001 * spending + 0.1 * tickets + 0.2 * satisfaction\n",
    "threshold = 1.5  # adjust as needed for a balanced dataset\n",
    "y = (score > threshold).astype(np.float32)\n",
    "\n",
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Step 2: Model Building\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomerNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "model = CustomerNet()\n",
    "\n",
    "# Step 3: Training\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor).squeeze()\n",
    "    loss = criterion(outputs, y_train_tensor.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Step 4: Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor).squeeze()\n",
    "    predicted_labels = torch.round(predictions)\n",
    "    correct = (predicted_labels == y_test_tensor.squeeze()).float().sum()\n",
    "    accuracy = correct / y_test_tensor.shape[0]\n",
    "    print(f\"Test Accuracy: {accuracy.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "- Why might deep learning be useful in predicting customer behavior in a business context?\n",
    "-What are some potential challenges when using a small synthetic dataset to model real-world business problems?\n",
    "\n",
    "By following these instructions and verifying that your final printed test accuracy is at least 80%, you will have successfully completed the assignment. Good luck, and enjoy applying deep learning to a real business scenario!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
